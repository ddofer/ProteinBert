{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dan changes - writing code blindly\n",
    "    0. Add more evaluation metrics. \n",
    "    1. use early stopping (based on validatio nset)\n",
    "    2. For longer sequences than max_len - should truncate rather than dropping...\n",
    "        * COmplications - for problems with local labelling we can't truncate, as we would lose the label! \n",
    "        For local (per position) labeling - we will extract a sliding window / chunks. For other problems (whole seqence), we'll simply truncate (start+end) I think!\n",
    "    * put MAX_GLOBAL_SEQ_LEN as parameter instead of magic number\n",
    "    \n",
    "    \n",
    "    * NOTE: In `build_model` - I believe input datatypes chould be changed to int16, instead of int32 and float 32. (Would save on memory per batch) - it's just the input encoding and input sequences encoding?. \n",
    "    \n",
    "    \n",
    "* The \"target processing\" code could be cleaned up a LOT by refactoring to expect a TUPLE :\n",
    "    * `(\"name\", whole sequence/local\", \"binary/multiclass/regression\"). \n",
    "    \n",
    "    \n",
    "    \n",
    "    * Nadav's transformer code - \n",
    "         * https://github.com/nadavbra/keras-seq-vec-transformer/blob/master/keras_seq_vec_transformer.py\n",
    "         * (Huji CSE server - chaperone-02): `/cs/phd/nadavb/github_projects/keras_seq_vec_transformer/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "# from tensorflow import keras \n",
    "from tensorflow import keras as keras\n",
    "from IPython.display import display\n",
    "\n",
    "# from pwas.shared_utils.util import log\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, r2_score, f1_score, precision_score, recall_score, balanced_accuracy_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import classification_report, roc_auc_score, log_loss # DAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from keras.callbacks import EarlyStopping \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "KERAS_SEQ_VEC_SCRIPT_PATH = \"keras_seq_vec_transformer.py\"\n",
    "# \"\"/cs/phd/nadavb/github_projects/keras_seq_vec_transformer/keras_seq_vec_transformer.py\" # ORIGINAL path in CSE server\n",
    "with open(KERAS_SEQ_VEC_SCRIPT_PATH, 'r') as f:\n",
    "    '''\n",
    "    Some horrible hacks. To make keras_seq_vec_transformer work with tf.keras instead of just keras we run the following instead of\n",
    "    just exec(f.read()).\n",
    "    Note that also above we run from tensorflow import keras instead of just import keras\n",
    "    '''\n",
    "    import tensorflow.keras.backend as K \n",
    "    from tensorflow.keras.layers import LayerNormalization\n",
    "    exec('\\n'.join([line for line in f.read().splitlines() if not line.startswith('import keras') and not \\\n",
    "            line.startswith('from keras_layer_normalization')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_AAS = 'ACDEFGHIKLMNPQRSTUVWXY'\n",
    "N_ANNOTATIONS = 8943\n",
    "\n",
    "# BENCHMARKS_DIR = '/cs/phd/nadavb/cafa_project/data/proteomic_benchmarks'\n",
    "BENCHMARKS_DIR = '../data'\n",
    "\n",
    "# PRETRAINED_MODEL_WEIGHTS_FILE_PATH = '/cs/phd/nadavb/cafa_project/data/model_weights/whole-donkey-13/epoch-48020.pkl'\n",
    "# PRETRAINED_MODEL_WEIGHTS_FILE_PATH = '../models/old_model_weights_epoch_89930.pkl'\n",
    "PRETRAINED_MODEL_WEIGHTS_FILE_PATH = '../models/epoch_28530_sample_91400000.pkl'\n",
    "\n",
    "BENCHMARKS = [\n",
    "#     'signalP_binary.dataset',\n",
    "#     'scop.dataset',\n",
    "#     'remote_homology',\n",
    "#     'fluorescence', \n",
    "    'secondary_structure',\n",
    "    \n",
    "#     'disorder_secondary_structure',\n",
    "## these were missing before - DAN :\n",
    "#     \"PhosphositePTM.dataset\",  ### 9 min per epoch\n",
    "#     \"stability\" \n",
    "             ]\n",
    "  #### \"phosphoserine.dataset\",   ## sequence texts and label lengths are different - dataset needs to be changed in advance or dropped\n",
    "\n",
    "MAX_GLOBAL_SEQ_LEN = 450 # 600 # set it here to make it easier to change. should be length used in training.  # DAN\n",
    "\n",
    "MAX_EPOCHS=50 # max train epochs,\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "FAST_RUN = True # False#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FAST_RUN:\n",
    "    MAX_EPOCHS = 1\n",
    "    BATCH_SIZE = BATCH_SIZE//2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## duplicate of code from \"utils.py\"\n",
    "def sliding_truncate_df_seqs_lengthwise(row,max_length:int = MAX_GLOBAL_SEQ_LEN):\n",
    "    \"without sliding, just truncate  - based on: https://stackoverflow.com/questions/50962722/how-to-split-pandas-dataframe-single-row-into-two-rows\"\n",
    "    r_len = len(row)\n",
    "    max_length -= 2\n",
    "    if r_len > max_length:\n",
    "        return(row[:max_length//2] + row[-max_length//2:]) # take first and last segments up to max length total\n",
    "    return row\n",
    "\n",
    "\n",
    "## https://stackoverflow.com/questions/18854620/whats-the-best-way-to-split-a-string-into-fixed-length-chunks-and-work-with-the\n",
    "def chunkstring(string:str, length:int=MAX_GLOBAL_SEQ_LEN):\n",
    "    length -= 2\n",
    "    try: return list(string[0+i:length+i ] for i in range(0, len(string), length))\n",
    "    except: \n",
    "        print(\"exception - string\",string)\n",
    "        print(type(string))\n",
    "\n",
    "def chunk_df(df,length:int=MAX_GLOBAL_SEQ_LEN,cols=[\"text\",\"labels\"]):\n",
    "    \"\"\"\n",
    "    Splits a df of strings and sequence labels into chunks of max length length. \n",
    "    Returns a new df, with instances that were \"too long\" now split into multiple rows.\n",
    "    Should be used  for sequence labelling problems (i.e where we must keep all positions). \n",
    "    \n",
    "    This function can also be applied (with modifications) to make \"sliding windows\" for LM/LANGUAGE MODEL training.\n",
    "    \n",
    "    Function rewritten due to error with duplicate indexes - now only support (for now) 2 columns\n",
    "    \"\"\"\n",
    "    length -= 2\n",
    "    df = pd.DataFrame({\"text\":df[\"text\"].apply(chunkstring).explode().values,\n",
    "                      \"labels\":df[\"labels\"].astype(str).apply(chunkstring).explode().values},columns=cols)    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pd.read_csv(\"../data/TAPE_benchmarks/disorder_secondary_structure.valid.csv\",nrows=2)\n",
    "# pd.read_csv(\"../data/stability.train.csv\",nrows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows 43416\n",
      "max len text 35213.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    43410.000000\n",
       "mean       606.431537\n",
       "std        671.138013\n",
       "min          2.000000\n",
       "25%        280.000000\n",
       "50%        449.000000\n",
       "75%        733.000000\n",
       "max      35213.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43411</th>\n",
       "      <td>MDKEKDGIPISSLREITLLLRLRHPNIVELKEVVVGNHLESIFLVM...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43412</th>\n",
       "      <td>MYCLLAAPLCLLSLLLSPLSPAAPISPSEPIGQAYSLALYMQKNTS...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43413</th>\n",
       "      <td>MDLEAARNGTARRLDGDFELGSISNQSREKKKKVNLIGPLTLFRYS...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43414</th>\n",
       "      <td>MSLVLLSLAALCRSAVPREPTVQCGSETGPSPEWMLQHDLIPGDLR...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43415</th>\n",
       "      <td>MSVHYTLNLRVFWPLVTGLCTALVCLYHVLRGSGGARAEPADGVDG...</td>\n",
       "      <td>0000000000000000000000000000000000000000000000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "43411  MDKEKDGIPISSLREITLLLRLRHPNIVELKEVVVGNHLESIFLVM...   \n",
       "43412  MYCLLAAPLCLLSLLLSPLSPAAPISPSEPIGQAYSLALYMQKNTS...   \n",
       "43413  MDLEAARNGTARRLDGDFELGSISNQSREKKKKVNLIGPLTLFRYS...   \n",
       "43414  MSLVLLSLAALCRSAVPREPTVQCGSETGPSPEWMLQHDLIPGDLR...   \n",
       "43415  MSVHYTLNLRVFWPLVTGLCTALVCLYHVLRGSGGARAEPADGVDG...   \n",
       "\n",
       "                                                  labels  \n",
       "43411  0000000000000000000000000000000000000000000000...  \n",
       "43412  0000000000000000000000000000000000000000000000...  \n",
       "43413  0000000000000000000000000000000000000000000000...  \n",
       "43414  0000000000000000000000000000000000000000000000...  \n",
       "43415  0000000000000000000000000000000000000000000000...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv(\"../data/neuropred.dataset.test.csv\")\n",
    "# df = pd.read_csv(\"../data/TAPE_benchmarks/disorder_secondary_structure.valid.csv\")\n",
    "\n",
    "df = pd.read_csv(\"../data/PhosphositePTM.dataset.train.csv\")#,nrows=3)\n",
    "# df = pd.read_csv(\"../data/phosphoserine.dataset.train.csv\") # has different lengths  of text and labels\n",
    "\n",
    "print(\"# rows\",df.shape[0])\n",
    "print(\"max len text\",df[\"text\"].str.len().max())\n",
    "display(df[\"text\"].str.len().describe())\n",
    "\n",
    "diff_len = df[\"text\"].str.len() - df[\"labels\"].str.len()\n",
    "assert max(diff_len)==0  # all rows should have same length of text and labels\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_AAS = 'ACDEFGHIKLMNPQRSTUVWXY'\n",
    "ADDITIONAL_TOKENS = ['<OTHER>', '<START>', '<END>', '<PAD>']\n",
    "\n",
    "# Each sequence is added <START> and <END> tokens\n",
    "ADDED_TOKENS_PER_SEQ = 2\n",
    "\n",
    "n_aas = len(ALL_AAS)\n",
    "aa_to_token_index = {aa: i for i, aa in enumerate(ALL_AAS)}\n",
    "additional_token_to_index = {token: i + len(ALL_AAS) for i, token in enumerate(ADDITIONAL_TOKENS)}\n",
    "token_to_index = {**aa_to_token_index, **additional_token_to_index}\n",
    "index_to_token = {index: token for token, index in token_to_index.items()}\n",
    "n_tokens = len(token_to_index)\n",
    "\n",
    "def tokenize_seq(seq):\n",
    "    other_token_index = additional_token_to_index['<OTHER>']\n",
    "    return [additional_token_to_index['<START>']] + [aa_to_token_index.get(aa, other_token_index) for aa in seq] + \\\n",
    "            [additional_token_to_index['<END>']]\n",
    "\n",
    "def tokenize_seqs(seqs,max_seq_len=MAX_GLOBAL_SEQ_LEN):\n",
    "    \n",
    "    tokenized_seqs = additional_token_to_index['<PAD>'] * np.ones((len(seqs), max_seq_len))\n",
    "    \n",
    "    for i, seq in enumerate(seqs):\n",
    "        tokenized_seq = tokenize_seq(seq)\n",
    "        assert len(tokenized_seq) <= max_seq_len\n",
    "        tokenized_seqs[i, :len(tokenized_seq)] = tokenized_seq\n",
    "        \n",
    "    return tokenized_seqs\n",
    "\n",
    "def create_model(max_seq_len):\n",
    "    \n",
    "    input_seq_layer = keras.layers.Input(shape = (max_seq_len,), dtype = np.int32, name = 'input-seq')\n",
    "    input_annoatations_layer = keras.layers.Input(shape = (N_ANNOTATIONS,), dtype = np.float32, name = 'input-annotations')\n",
    "    output_seq_layer, output_annoatations_layer = TransformerAutoEncoder(vocab_size = n_tokens, d_vec = N_ANNOTATIONS, \\\n",
    "            output_vec_activation = 'sigmoid', name = 'auto-encoder')([input_seq_layer, input_annoatations_layer])\n",
    "\n",
    "    # An ugly hack meant to achieve nothing than renaming those layers.\n",
    "    output_seq_layer = keras.layers.Reshape(output_seq_layer.shape[1:], name = 'output_seq_layer')(output_seq_layer)\n",
    "    output_annoatations_layer = keras.layers.Reshape(output_annoatations_layer.shape[1:], name = 'output_annoatations_layer')\\\n",
    "            (output_annoatations_layer)\n",
    "\n",
    "    return keras.models.Model(inputs = [input_seq_layer, input_annoatations_layer], outputs = [output_seq_layer, output_annoatations_layer])\n",
    "\n",
    "def load_model_weights(model, path):\n",
    "    with open(path, 'rb') as f:\n",
    "        model_weights, optimizer_weights = pickle.load(f)\n",
    "        model.set_weights(model_weights)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The same processing needs to be applied to the validation set , as the train set.\n",
    "* A few datasets lack a validation set. We can set them to have one in advance, or just randomly sample the train set and split it here. \n",
    "\n",
    "* NOTE - sequence length truncation is applied to labels as well, for per position data. This may be problematic for evaluation!!  - We may need to split into more segments for sake of evaluation..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X, raw_y_true):     \n",
    "    Y_pred = model.predict(X) # array of probabilities\n",
    "    Y_pred_classes = Y_pred.argmax(axis = -1) # try to get classes, not probability per class\n",
    "#     print(\"y_pred\",Y_pred)\n",
    "#     print(\"Y_pred_classes\",Y_pred_classes)\n",
    "\n",
    "    ### some sklearn metrics will fail, since we are doing multiclass and regression etc'. But i'll just \"Try\" each # DAN\n",
    "#     print(\"trying sklearn metrics\")\n",
    "    try:\n",
    "        print(\"classification Report\\n\")\n",
    "        print(classification_report(raw_y_true,Y_pred_classes))\n",
    "    except:pass    \n",
    "    try:\n",
    "        print(\"MCC %.4f%\" % matthews_corrcoef(raw_y_true,Y_pred_classes))\n",
    "    except:pass\n",
    "    try:\n",
    "        print(\"F1 - macro avg %.4f%\" % f1_score(raw_y_true,Y_pred_classes, average='macro'))\n",
    "        print(\"precision - micro avg %.2f%%\" % (100 * precision_score(raw_y_true,Y_pred_classes, average='micro')))\n",
    "        print(\"Recall - macro avg %.2f%%\" % (100 * recall_score(raw_y_true,Y_pred_classes, average='micro')))\n",
    "              \n",
    "        print(\"balanced_accuracy_score %.4f%\" % balanced_accuracy_score(raw_y_true,Y_pred_classes))\n",
    "    except:pass\n",
    "    \n",
    "    try:\n",
    "        print(\"r2 %.4f%\" % r2_score(raw_y_true,Y_pred.flatten())) # doesn't work? DAN\n",
    "        print(\"mean_absolute_error %.4f%\" % mean_absolute_error(raw_y_true,Y_pred.flatten()))\n",
    "    except: pass\n",
    "    \n",
    "    try: #get probability if supported, then calc logloss, auc\n",
    "#         Y_pred_proba = model.predict(test_X)[:,1] # this function is deprecated..\n",
    "        print(\"roc_auc_score %.4f%\" % roc_auc_score(raw_y_true,Y_pred[:,1]))\n",
    "        print(\"log_loss %.4f%\" % log_loss(raw_y_true,Y_pred[:,1]))\n",
    "    except:pass\n",
    "    \n",
    "    if is_y_numeric:\n",
    "        results = pd.DataFrame({'true': raw_y_true, 'pred': Y_pred.flatten()})\n",
    "        print(\"spearman's rho (correlation)\",results.corr(method=\"spearman\"))\n",
    "        print('R^2 score: %.2g' % r2_score(results['true'], results['pred']))\n",
    "        \n",
    "        print(\"mean absolute error score %.4g\" % mean_absolute_error(results['true'], results['pred']))      \n",
    "              \n",
    "    else:\n",
    "        if is_y_seq:\n",
    "            results_true = []\n",
    "            results_pred = []\n",
    "\n",
    "            for true_seq, pred_seq in zip(raw_y_true, Y_pred.argmax(axis = -1)):\n",
    "                for true_token, pred_token_index in zip(true_seq, pred_seq):\n",
    "                    results_true.append(true_token)\n",
    "                    results_pred.append('<PAD>' if pred_token_index == n_labels else unique_labels[pred_token_index])\n",
    "\n",
    "            results = pd.DataFrame({'true': results_true, 'pred': results_pred})\n",
    "        else:\n",
    "            predicted_labels = [unique_labels[i] for i in Y_pred.argmax(axis = -1)]\n",
    "            results = pd.DataFrame({'true': raw_y_true, 'pred': predicted_labels})\n",
    "\n",
    "        confusion_matrix = results.groupby(['true', 'pred']).size().unstack().fillna(0)\n",
    "        #### display confusion matrix\n",
    "        if len(set(unique_labels))<20:\n",
    "            print('Confusion matrix:')\n",
    "            display(confusion_matrix)\n",
    "\n",
    "        accuracy = (results['true'] == results['pred']).mean()\n",
    "        imbalance = (results['true'].value_counts().max() / len(results))\n",
    "        print('Accuracy: %.2f%%' % (100 * accuracy))\n",
    "        print('Imbalance (most common label): %.2f%%' % (100 * imbalance))\n",
    "\n",
    "#         if set(unique_labels) == {0, 1}:\n",
    "        if len(set(unique_labels)) == 2:\n",
    "            y_true = results['true'].astype(float)\n",
    "            y_pred = results['pred'].astype(float)\n",
    "            print('MCC: %.2f%%' % (100 * matthews_corrcoef(y_true, y_pred)))\n",
    "            \n",
    "            print(\"F1 - macro avg %.2f%%\" % (100 * f1_score(y_true, y_pred, average='macro')))       \n",
    "            print(\"precision - micro avg %.2f%%\" % (100 * precision_score(y_true, y_pred, average='micro')))\n",
    "            print(\"Recall - macro avg %.2f%%\" % (100 * recall_score(y_true, y_pred, average='micro')))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Code comment: It would be great to __refactor__ this - it is currently cumbersome and not productionable. \n",
    "    * It should be rewritten to expect a train/valid/test set and Ys, (+- the target type, or a function for each type of target) and the Y preprocessing to be a seperate, user defined step, _not trying to infer it here as part of the loop_. \n",
    "    \n",
    "    \n",
    "    \n",
    "Data augmentation idea/experiment: \n",
    "    * Duplicate train subset, with sequence reversed (and also reverse y/labels if target is sequence_label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== secondary_structure ==========\n",
      "\n",
      "\n",
      "868 training-set records, 22 valid-set records, 4 test-set records\n",
      "is_y_seq True\n",
      "Sequence output with 3 tokens.\n",
      "chunked train set shape: (954, 2)\n",
      "chunked test_set size: 4\n",
      "Max seq len: 450\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder/kernel:0', 'auto-encoder/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder/kernel:0', 'auto-encoder/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder/kernel:0', 'auto-encoder/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder/kernel:0', 'auto-encoder/bias:0'] when minimizing the loss.\n",
      "120/120 - 17s - loss: 1.3766 - val_loss: 1.3657 - lr: 1.0000e-04\n",
      "\n",
      "*** Training-set performance: ***\n",
      "classification Report\n",
      "\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>&lt;PAD&gt;</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28819</td>\n",
       "      <td>17949</td>\n",
       "      <td>867</td>\n",
       "      <td>30229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20633</td>\n",
       "      <td>8697</td>\n",
       "      <td>772</td>\n",
       "      <td>14918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38193</td>\n",
       "      <td>18507</td>\n",
       "      <td>1657</td>\n",
       "      <td>34107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred      0      1     2  <PAD>\n",
       "true                           \n",
       "0     28819  17949   867  30229\n",
       "1     20633   8697   772  14918\n",
       "2     38193  18507  1657  34107"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 18.19%\n",
      "Imbalance (most common label): 42.94%\n",
      "*** validation performance: ***\n",
      "classification Report\n",
      "\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>&lt;PAD&gt;</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>822</td>\n",
       "      <td>490</td>\n",
       "      <td>12</td>\n",
       "      <td>819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>447</td>\n",
       "      <td>207</td>\n",
       "      <td>13</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1009</td>\n",
       "      <td>490</td>\n",
       "      <td>47</td>\n",
       "      <td>927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred     0    1   2  <PAD>\n",
       "true                      \n",
       "0      822  490  12    819\n",
       "1      447  207  13    328\n",
       "2     1009  490  47    927"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 19.18%\n",
      "Imbalance (most common label): 44.07%\n",
      "*** Test-set performance: ***\n",
      "classification Report\n",
      "\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>&lt;PAD&gt;</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156</td>\n",
       "      <td>88</td>\n",
       "      <td>13</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred    0   1   2  <PAD>\n",
       "true                    \n",
       "0      86  56   2     74\n",
       "1      82  46   7     92\n",
       "2     156  88  13    161"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 16.80%\n",
      "Imbalance (most common label): 48.44%\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for benchmark_name in BENCHMARKS:\n",
    "    print('========== %s ==========' % benchmark_name)\n",
    "    \n",
    "    text_file_path = os.path.join(BENCHMARKS_DIR, '%s.benchmark.txt' % benchmark_name)\n",
    "    \n",
    "    train_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.train.csv' % benchmark_name)\n",
    "    valid_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.valid.csv' % benchmark_name)\n",
    "    test_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.test.csv' % benchmark_name)\n",
    "    \n",
    "    ### printout of text from benchmarks text files/descriptions: \n",
    "    \"\"\"\n",
    "#     if os.path.exists(text_file_path):\n",
    "#         with open(text_file_path, 'r') as f:\n",
    "#             for line in f:\n",
    "#                 print('[$$$] ' + line, end = '')\n",
    "    \"\"\"\n",
    "    \n",
    "    print('\\n')\n",
    "    ##### As originally used, the \"Test\" set was actually the validation set and vice versa. I leave the names unchanged to avoid bugs, but it is confusing!! Dan\n",
    "    train_set = pd.read_csv(train_set_file_path).dropna().drop_duplicates()\n",
    "\n",
    "    is_y_numeric = np.issubdtype(train_set['labels'].dtype, np.floating)   \n",
    "    \n",
    "    try: \n",
    "        valid_set = pd.read_csv(valid_set_file_path).dropna().drop_duplicates()\n",
    "    except:\n",
    "#     if valid_set is None: \n",
    "        print(f\"validation set {valid_set_file_path} missing\")\n",
    "        print(\"splitting train to train and random validation set\")\n",
    "        ## we could do stratified sampling here, but requires checking if problem is seq based or not\n",
    "        try: ## could also try based on checking is_y_numeric & categoricals\n",
    "            train_set, valid_set = train_test_split(train_set,\n",
    "                                                    stratify=train_set['labels'],test_size=0.1,random_state=42)\n",
    "            print(\"Stratified sampling of validation set\")\n",
    "        except:\n",
    "            print(\"randomly sampling validation set\")\n",
    "            train_set, valid_set = train_test_split(train_set,test_size=0.1,random_state=42)\n",
    "    \n",
    "    try: \n",
    "        test_set = pd.read_csv(test_set_file_path).dropna().drop_duplicates()\n",
    "    except:\n",
    "        print(f\"test set {test_set} missing\")\n",
    "    \n",
    "    if FAST_RUN:\n",
    "        if \"remote_homology\" in benchmark_name:\n",
    "#            \"remote_homology special fast sampling\" - over 1K classes\n",
    "#             train_set = train_set.sample(frac=0.8,random_state=42) # a small sample will give errors in remote homology/massively multiclass\n",
    "            train_set = pd.concat([train_set.sample(frac=0.1,random_state=42),train_set.drop_duplicates('labels')]).drop_duplicates()\n",
    "        else:\n",
    "            if ((train_set['labels'].nunique()<1200) and (not is_y_numeric)):\n",
    "                train_set,_ = train_test_split(train_set,stratify=train_set['labels'],train_size=0.05,random_state=42) # stratified sampling 0 for multiclass\n",
    "            else:\n",
    "                train_set = train_set.sample(frac=0.1,random_state=42)\n",
    "        valid_set = valid_set.sample(frac=0.01)\n",
    "        test_set = test_set.sample(frac=0.01)\n",
    "\n",
    "    \n",
    "    train_set_size = len(train_set)\n",
    "    test_set_size = len(test_set)\n",
    "    valid_set_size =  len(valid_set)\n",
    "    \n",
    "    print(f'{train_set_size} training-set records, {valid_set_size} valid-set records, {test_set_size} test-set records')\n",
    "      \n",
    "    '''\n",
    "    Determining which of the following y is:\n",
    "    1. Numeric (could be either probabalistic (i.e. in the range 0-1) or not)\n",
    "    2. Sequence\n",
    "    3. Categorical\n",
    "    '''\n",
    "    if is_y_numeric:\n",
    "        is_y_seq = False\n",
    "        is_y_probability = train_set['labels'].min() >= 0 and train_set['labels'].max() <= 1\n",
    "        print('Numeric (%sprobabilistic) label' % ('' if is_y_probability else 'not '))\n",
    "    else: \n",
    "        ## we already know y isn't a number / \"1.76322\"\n",
    "#         is_y_seq = train_set['labels'].apply(lambda value: isinstance(value, str)).all() and \\\n",
    "#                 (train_set['text'].str.len() == train_set['labels'].str.len()).all() ###ORIG\n",
    "        is_y_seq = train_set['labels'].astype(str).str.len().max()>15 ### Dan hack - phosphosite (0/1s seq) wasn't being identified\n",
    "    \n",
    "        print(\"is_y_seq\",is_y_seq)\n",
    "        if is_y_seq:\n",
    "            train_set['labels'] = train_set['labels'].astype(str)## add \"astype(str)\"\n",
    "            unique_labels = sorted(set.union(*train_set['labels'].apply(set))) \n",
    "        else:\n",
    "            unique_labels = sorted(train_set['labels'].unique())\n",
    "                    \n",
    "        n_labels = len(unique_labels)\n",
    "        label_to_index = {label: i for i, label in enumerate(unique_labels)}\n",
    "        \n",
    "        if is_y_seq:\n",
    "            print('Sequence output with %d tokens.' % n_labels)\n",
    "        else:\n",
    "            print('Categorical output with %d labels.' % n_labels)\n",
    "    ################\n",
    "    #### sequence/label length truncation or splitting done here, depending on if y is sequence\n",
    "    if is_y_seq == False:\n",
    "        train_set[\"text\"] = train_set[\"text\"].apply(sliding_truncate_df_seqs_lengthwise)\n",
    "        test_set[\"text\"] = test_set[\"text\"].apply(sliding_truncate_df_seqs_lengthwise)\n",
    "        valid_set[\"text\"] = valid_set[\"text\"].apply(sliding_truncate_df_seqs_lengthwise)\n",
    "    else:\n",
    "        train_set = chunk_df(train_set)      \n",
    "        test_set = chunk_df(test_set)\n",
    "        valid_set = chunk_df(valid_set)\n",
    "        print(\"chunked train set shape:\",train_set.shape)\n",
    "#         print(\"chunked valid_set size:\",valid_set.shape[0])\n",
    "        print(\"chunked test_set size:\",test_set.shape[0])\n",
    "        \n",
    "        train_set_size = len(train_set)\n",
    "        test_set_size = len(test_set)\n",
    "        valid_set_size =  len(valid_set)\n",
    "    \n",
    "    ### update max sequence length:\n",
    "    # Adding +2 for the <START> and <END> tokens.\n",
    "    max_seq_len = max(train_set['text'].str.len().max(), test_set['text'].str.len().max(), \\\n",
    "                      valid_set['text'].str.len().max()) + 2\n",
    "    \n",
    "    print(f'Max seq len: {max_seq_len}')\n",
    "    if max_seq_len > MAX_GLOBAL_SEQ_LEN:\n",
    "        max_seq_len = MAX_GLOBAL_SEQ_LEN\n",
    "    \n",
    "    ####################\n",
    "    \n",
    "    model = create_model(max_seq_len)\n",
    "    load_model_weights(model, PRETRAINED_MODEL_WEIGHTS_FILE_PATH)\n",
    "    input_seq_layer, input_annoatations_layer = model.input\n",
    "    output_seq_layer, output_annoatations_layer = model.output\n",
    "    \n",
    "    if is_y_numeric:\n",
    "        output_layer = keras.layers.Dense(1, activation = ('sigmoid' if is_y_probability else None))(output_annoatations_layer)\n",
    "        loss = 'binary_crossentropy' if is_y_probability else 'mse'\n",
    "    elif is_y_seq:\n",
    "        # Adding another token for padding, making it (n_labels + 1) tokens. ## DAN - why would output need a padding token? why n+1 ???\n",
    "        output_layer = keras.layers.Dense(n_labels + 1, activation = 'softmax')(output_seq_layer)\n",
    "        loss = 'categorical_crossentropy'\n",
    "    else:\n",
    "        output_layer = keras.layers.Dense(n_labels, activation = 'softmax')(output_annoatations_layer)\n",
    "        loss = 'categorical_crossentropy'\n",
    "    \n",
    "    model = keras.models.Model(inputs = [input_seq_layer, input_annoatations_layer], outputs = output_layer)\n",
    "    model.compile(optimizer = keras.optimizers.Adam(lr = 1e-04), loss = loss) # DAN - changed to 2 instead of 1. default in literature is 3e-4\n",
    "    \n",
    "    train_X = [\n",
    "        tokenize_seqs(train_set['text'].values, max_seq_len).astype(np.int32),\n",
    "        np.zeros((train_set_size, N_ANNOTATIONS), dtype = np.int8)\n",
    "    ]\n",
    "    \n",
    "    valid_X = [\n",
    "        tokenize_seqs(valid_set['text'].values, max_seq_len).astype(np.int32),\n",
    "        np.zeros((valid_set_size, N_ANNOTATIONS), dtype = np.int8)\n",
    "    ]\n",
    "    \n",
    "    test_X = [\n",
    "        tokenize_seqs(test_set['text'].values, max_seq_len).astype(np.int32),\n",
    "        np.zeros((test_set_size, N_ANNOTATIONS), dtype = np.int8)\n",
    "    ]\n",
    "    \n",
    "    ### I added code to apply the transformations to the validset - hopefully without bugs... DAN ? \n",
    "    if is_y_numeric:\n",
    "        train_Y = train_set['labels'].values\n",
    "        valid_Y = valid_set['labels'].values\n",
    "    elif is_y_seq:\n",
    "        \n",
    "        train_Y = np.zeros((train_set_size, max_seq_len, n_labels + 1), dtype = np.int8) ## DAN - try +1 from max_seq_len due to padding\n",
    "        valid_Y = np.zeros((valid_set_size, max_seq_len, n_labels + 1), dtype = np.int8) ## DAN - try +1 from max_seq_len due to padding\n",
    "# valid_Y[i, j, label_to_index[token]] = 1        \n",
    "        \n",
    "        for i, output_seq in enumerate(train_set['labels']):\n",
    "            for j, token in enumerate(output_seq):\n",
    "                train_Y[i, j, label_to_index[token]] = 1\n",
    "            # Add a pad token for the rest of the output.\n",
    "            train_Y[i, np.arange(len(output_seq), max_seq_len), n_labels] = 1\n",
    "        ## duplicate above, for validation set. Unsure if I could just add in a row of \"valid_y above?? DAN  \n",
    "        for i, output_seq in enumerate(valid_set['labels']):           \n",
    "            for j, token in enumerate(output_seq):\n",
    "                valid_Y[i, j, label_to_index[token]] = 1\n",
    "            # Add a pad token for the rest of the output.\n",
    "            valid_Y[i, np.arange(len(output_seq), max_seq_len), n_labels] = 1\n",
    "    else:\n",
    "        train_Y = np.zeros((train_set_size, n_labels), dtype = np.int8)\n",
    "        train_Y[np.arange(train_set_size),[label_to_index[label] for label in train_set['labels']]] = 1\n",
    "        \n",
    "        valid_Y = np.zeros((valid_set_size, n_labels), dtype = np.int8)\n",
    "        valid_Y[np.arange(valid_set_size),[label_to_index[label] for label in valid_set['labels']]] = 1\n",
    "        \n",
    "#########\n",
    "    ### Train model, with early stopping on validation set\n",
    "    \n",
    "    model.fit(train_X, train_Y,\n",
    "              batch_size = BATCH_SIZE,\n",
    "              validation_data=(valid_X,valid_Y),\n",
    "              callbacks = [ReduceLROnPlateau(patience=2,factor=0.35), EarlyStopping(patience=4)],\n",
    "              epochs = MAX_EPOCHS,\n",
    "              validation_batch_size=BATCH_SIZE,\n",
    "              verbose=2)\n",
    "    #### DAN: NOTE - keras already keeps evaluation, train error data..\n",
    "    print('\\n*** Training-set performance: ***')\n",
    "    evaluate(train_X, train_set['labels'].values)\n",
    "    print('*** validation performance: ***')\n",
    "    evaluate(valid_X, valid_set['labels'].values)\n",
    "    print('*** Test-set performance: ***')\n",
    "    evaluate(test_X, test_set['labels'].values)\n",
    "    \n",
    "    print('\\n' * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.27362844, 0.19358492, 0.25285643, 0.2799302 ],\n",
       "        [0.19041027, 0.26470122, 0.27283078, 0.2720577 ],\n",
       "        [0.29056835, 0.17761472, 0.27154985, 0.26026708],\n",
       "        ...,\n",
       "        [0.22333321, 0.19653027, 0.3090035 , 0.2711331 ],\n",
       "        [0.22333321, 0.19653027, 0.3090035 , 0.2711331 ],\n",
       "        [0.22333321, 0.19653027, 0.3090035 , 0.2711331 ]],\n",
       "\n",
       "       [[0.27362666, 0.1935881 , 0.2528556 , 0.2799296 ],\n",
       "        [0.19044796, 0.2646827 , 0.2728122 , 0.27205712],\n",
       "        [0.24825507, 0.24139766, 0.22144781, 0.28889942],\n",
       "        ...,\n",
       "        [0.23692396, 0.28960449, 0.19760235, 0.27586916],\n",
       "        [0.20648701, 0.26560277, 0.2574372 , 0.270473  ],\n",
       "        [0.18884726, 0.23590352, 0.22572857, 0.34952065]],\n",
       "\n",
       "       [[0.27359593, 0.19363168, 0.25284764, 0.27992472],\n",
       "        [0.20930295, 0.2546454 , 0.2584631 , 0.27758855],\n",
       "        [0.21482061, 0.24689646, 0.23582028, 0.3024626 ],\n",
       "        ...,\n",
       "        [0.23638913, 0.3140212 , 0.18939406, 0.26019555],\n",
       "        [0.22478743, 0.22754577, 0.22618861, 0.32147822],\n",
       "        [0.1891026 , 0.23587735, 0.22582693, 0.34919307]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.27362844, 0.19358492, 0.25285643, 0.2799302 ],\n",
       "        [0.19748142, 0.25933692, 0.2619222 , 0.28125942],\n",
       "        [0.25021687, 0.23820238, 0.22418602, 0.28739476],\n",
       "        ...,\n",
       "        [0.22333321, 0.19653027, 0.3090035 , 0.2711331 ],\n",
       "        [0.22333321, 0.19653027, 0.3090035 , 0.2711331 ],\n",
       "        [0.22333321, 0.19653027, 0.3090035 , 0.2711331 ]],\n",
       "\n",
       "       [[0.27362847, 0.19358493, 0.2528564 , 0.27993023],\n",
       "        [0.20915608, 0.24587938, 0.27221516, 0.27274936],\n",
       "        [0.21370535, 0.24682467, 0.23604336, 0.30342662],\n",
       "        ...,\n",
       "        [0.2233332 , 0.19653024, 0.30900344, 0.27113307],\n",
       "        [0.2233332 , 0.19653024, 0.30900344, 0.27113307],\n",
       "        [0.2233332 , 0.19653024, 0.30900344, 0.27113307]],\n",
       "\n",
       "       [[0.27362844, 0.19358492, 0.25285643, 0.2799302 ],\n",
       "        [0.21258928, 0.24254611, 0.2720832 , 0.27278137],\n",
       "        [0.3220276 , 0.13980602, 0.2947915 , 0.2433749 ],\n",
       "        ...,\n",
       "        [0.22333321, 0.19653027, 0.3090035 , 0.2711331 ],\n",
       "        [0.22333321, 0.19653027, 0.3090035 , 0.2711331 ],\n",
       "        [0.22333321, 0.19653027, 0.3090035 , 0.2711331 ]]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = model.predict(train_X)\n",
    "# Y_pred = model.predict(test_X).argmax(-1)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
