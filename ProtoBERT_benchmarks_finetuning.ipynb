{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dan changes - writing code blindly\n",
    "    0. Add more evaluation metrics. \n",
    "    1. use early stopping (based on validatio nset)\n",
    "    2. For longer sequences than max_len - should truncate rather than dropping...\n",
    "        * COmplications - for problems with local labelling we can't truncate, as we would lose the label! \n",
    "        For local (per position) labeling - we will extract a sliding window / chunks. For other problems (whole seqence), we'll simply truncate (start+end) I think!\n",
    "    * put MAX_GLOBAL_SEQ_LEN as parameter instead of magic number\n",
    "    \n",
    "    \n",
    "    * NOTE: In `build_model` - I believe input datatypes chould be changed to int16, instead of int32 and float 32. (Would save on memory per batch) - it's just the input encoding and input sequences encoding?. \n",
    "    \n",
    "    \n",
    "* The \"target processing\" code could be cleaned up a LOT by refactoring to expect a TUPLE :\n",
    "    * `(\"name\", whole sequence/local\", \"binary/multiclass/regression\"). \n",
    "    \n",
    "    \n",
    "    \n",
    "    * Nadav's transformer code - \n",
    "         * https://github.com/nadavbra/keras-seq-vec-transformer/blob/master/keras_seq_vec_transformer.py\n",
    "         * (Huji CSE server - chaperone-02): `/cs/phd/nadavb/github_projects/keras_seq_vec_transformer/`\n",
    "         \n",
    "         \n",
    "DAN CHANGES - \n",
    "\n",
    "*  i replaced our encode_categorical_y with TF's `to_categorical` . TF's expects an integer input. I deleted the old function. (It gave errors).\n",
    "    * This necessitated processing the categorical class labels into numbers\n",
    "* Added `is_y_discrete` usage, and variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# from tensorflow import keras \n",
    "from tensorflow import keras as keras\n",
    "from IPython.display import display\n",
    "\n",
    "# from pwas.shared_utils.util import log\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, r2_score, f1_score, precision_score, recall_score, balanced_accuracy_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import classification_report, roc_auc_score, log_loss # DAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "KERAS_SEQ_VEC_SCRIPT_PATH = \"keras_seq_vec_transformer.py\"\n",
    "# \"\"/cs/phd/nadavb/github_projects/keras_seq_vec_transformer/keras_seq_vec_transformer.py\" # ORIGINAL path in CSE server\n",
    "with open(KERAS_SEQ_VEC_SCRIPT_PATH, 'r') as f:\n",
    "    '''\n",
    "    Some horrible hacks. To make keras_seq_vec_transformer work with tf.keras instead of just keras we run the following instead of\n",
    "    just exec(f.read()).\n",
    "    Note that also above we run from tensorflow import keras instead of just import keras\n",
    "    '''\n",
    "    import tensorflow.keras.backend as K \n",
    "    from tensorflow.keras.layers import LayerNormalization\n",
    "    exec('\\n'.join([line for line in f.read().splitlines() if not line.startswith('import keras') and not \\\n",
    "            line.startswith('from keras_layer_normalization')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_AAS = 'ACDEFGHIKLMNPQRSTUVWXY'\n",
    "N_ANNOTATIONS = 8943\n",
    "\n",
    "# BENCHMARKS_DIR = '/cs/phd/nadavb/cafa_project/data/proteomic_benchmarks'\n",
    "BENCHMARKS_DIR = '../data'\n",
    "\n",
    "PRETRAINED_MODEL_WEIGHTS_FILE_PATH = '../models/epoch_28530_sample_91400000.pkl'\n",
    "\n",
    "BENCHMARKS = [\n",
    "    'signalP_binary.dataset',\n",
    "#     'scop.dataset',\n",
    "#     'fluorescence', \n",
    "#     'secondary_structure',\n",
    "    \n",
    "#     'remote_homology', ## currently has bug with unseen labels / label encoding ?? \n",
    "#     'disorder_secondary_structure', ## error : ValueError: could not convert string to float: '<PAD>' \n",
    "#     \"PhosphositePTM.dataset\", ## error : ValueError: could not convert string to float: '<PAD>'  ### 9 min per epoch\n",
    "#     \"stability\" \n",
    "             ]\n",
    "  #### \"phosphoserine.dataset\",   ## sequence texts and label lengths are different - dataset needs to be changed in advance or dropped\n",
    "\n",
    "MAX_GLOBAL_SEQ_LEN = 120#450 # 450 # set it here to make it easier to change. should be length used in training.  # DAN\n",
    "\n",
    "MAX_ALLOWED_INPUT_SEQ = MAX_GLOBAL_SEQ_LEN - 2\n",
    "\n",
    "MAX_EPOCHS=3 #80 # max train epochs,\n",
    "BATCH_SIZE = 32#16\n",
    "\n",
    "DEBUG_MODE = True\n",
    "USE_PRETRAINED_WEIGHTS = False\n",
    "\n",
    "FAST_RUN = True # True # False#\n",
    "FAST_SAMPLE_RATIO = 0.01 # if doing fast run, downsample to roughly this percent of data from train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FAST_RUN:\n",
    "    MAX_EPOCHS = 1\n",
    "#     BATCH_SIZE = BATCH_SIZE//2 \n",
    "#     MAX_ALLOWED_INPUT_SEQ = 132"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_AAS = 'ACDEFGHIKLMNPQRSTUVWXY'\n",
    "ADDITIONAL_TOKENS = ['<OTHER>', '<START>', '<END>', '<PAD>']\n",
    "ADDED_TOKENS_PER_SEQ = 2\n",
    "n_aas = len(ALL_AAS)\n",
    "aa_to_token_index = {aa: i for i, aa in enumerate(ALL_AAS)}\n",
    "additional_token_to_index = {token: i + len(ALL_AAS) for i, token in enumerate(ADDITIONAL_TOKENS)}\n",
    "token_to_index = {**aa_to_token_index, **additional_token_to_index}\n",
    "index_to_token = {index: token for token, index in token_to_index.items()}\n",
    "n_tokens = len(token_to_index)\n",
    "\n",
    "def tokenize_seq(seq):\n",
    "    other_token_index = additional_token_to_index['<OTHER>']\n",
    "#     return [additional_token_to_index['<START>']] + [aa_to_token_index.get(aa, other_token_index) for aa in seq] + [additional_token_to_index['<END>']] # ORIG\n",
    "    return [aa_to_token_index.get(aa, other_token_index) for aa in seq]\n",
    "\n",
    "\n",
    "def tokenize_seqs(seqs,max_seq_len=MAX_GLOBAL_SEQ_LEN):   \n",
    "    tokenized_seqs = additional_token_to_index['<PAD>'] * np.ones((len(seqs), max_seq_len))\n",
    "    for i, seq in enumerate(seqs):\n",
    "        tokenized_seq = tokenize_seq(seq)\n",
    "        assert len(tokenized_seq) <= max_seq_len\n",
    "        tokenized_seqs[i, :len(tokenized_seq)] = tokenized_seq\n",
    "    return tokenized_seqs\n",
    "\n",
    "def create_model(max_seq_len):   \n",
    "    input_seq_layer = keras.layers.Input(shape = (max_seq_len,), dtype = np.int32, name = 'input-seq')\n",
    "    input_annoatations_layer = keras.layers.Input(shape = (N_ANNOTATIONS,), dtype = np.float32, name = 'input-annotations')\n",
    "    output_seq_layer, output_annoatations_layer = TransformerAutoEncoder(vocab_size = n_tokens, d_vec = N_ANNOTATIONS, output_vec_activation = 'sigmoid', name = 'auto-encoder')([input_seq_layer, input_annoatations_layer])\n",
    "    output_seq_layer = keras.layers.Reshape(output_seq_layer.shape[1:], name = 'output_seq_layer')(output_seq_layer)\n",
    "    output_annoatations_layer = keras.layers.Reshape(output_annoatations_layer.shape[1:], name = 'output_annoatations_layer')(output_annoatations_layer)\n",
    "    return keras.models.Model(inputs = [input_seq_layer, input_annoatations_layer], outputs = [output_seq_layer, output_annoatations_layer])\n",
    "\n",
    "def load_model_weights(model, path):\n",
    "    with open(path, 'rb') as f:\n",
    "        model_weights, optimizer_weights = pickle.load(f)\n",
    "        model.set_weights(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Y_pred, raw_y_true, is_y_numeric, is_y_seq, unique_labels):     \n",
    "    n_labels = len(unique_labels)\n",
    "    Y_pred_classes = Y_pred.argmax(axis = -1) \n",
    "    try:\n",
    "        print(\"classification Report\\n\")\n",
    "        print(classification_report(raw_y_true,Y_pred_classes))\n",
    "    except:pass    \n",
    "    try:\n",
    "        print(\"MCC %.4f%\" % matthews_corrcoef(raw_y_true,Y_pred_classes))\n",
    "    except:pass\n",
    "    try:\n",
    "        print(\"F1 - macro avg %.4f%\" % f1_score(raw_y_true,Y_pred_classes, average='macro'))\n",
    "        print(\"precision - micro avg %.2f%%\" % (100 * precision_score(raw_y_true,Y_pred_classes, average='micro')))\n",
    "        print(\"Recall - macro avg %.2f%%\" % (100 * recall_score(raw_y_true,Y_pred_classes, average='micro')))              \n",
    "        print(\"balanced_accuracy_score %.4f%\" % balanced_accuracy_score(raw_y_true,Y_pred_classes))\n",
    "    except:pass    \n",
    "    try:\n",
    "        print(\"r2 %.4f%\" % r2_score(raw_y_true,Y_pred.flatten())) # doesn't work? DAN\n",
    "        print(\"mean_absolute_error %.4f%\" % mean_absolute_error(raw_y_true,Y_pred.flatten()))\n",
    "    except: pass   \n",
    "    try: \n",
    "        print(\"roc_auc_score %.4f%\" % roc_auc_score(raw_y_true,Y_pred[:,1]))\n",
    "        print(\"log_loss %.4f%\" % log_loss(raw_y_true,Y_pred[:,1]))\n",
    "    except:pass    \n",
    "    if is_y_numeric:\n",
    "        results = pd.DataFrame({'true': raw_y_true, 'pred': Y_pred.flatten()})\n",
    "        print(\"spearman's rho (correlation)\",results.corr(method=\"spearman\"))\n",
    "        print('R^2 score: %.2g' % r2_score(results['true'], results['pred']))        \n",
    "        print(\"mean absolute error score %.4g\" % mean_absolute_error(results['true'], results['pred']))                    \n",
    "    else:\n",
    "        if is_y_seq:\n",
    "            results_true = []\n",
    "            results_pred = []\n",
    "            for true_seq, pred_seq in zip(raw_y_true, Y_pred.argmax(axis = -1)):\n",
    "                for true_token, pred_token_index in zip(true_seq, pred_seq):\n",
    "                    results_true.append(true_token)\n",
    "                    results_pred.append('<PAD>' if pred_token_index == n_labels else unique_labels[pred_token_index])\n",
    "            results = pd.DataFrame({'true': results_true, 'pred': results_pred})\n",
    "        else:\n",
    "            predicted_labels = [unique_labels[i] for i in Y_pred.argmax(axis = -1)]\n",
    "            results = pd.DataFrame({'true': raw_y_true, 'pred': predicted_labels})\n",
    "        confusion_matrix = results.groupby(['true', 'pred']).size().unstack().fillna(0)\n",
    "        if len(set(unique_labels))<20 and False:\n",
    "            print('Confusion matrix:')\n",
    "            display(confusion_matrix)\n",
    "\n",
    "            #         accuracy = (results['true'].astype(int) == results['pred'].astype(int)).mean() ## DAN - added .astype(int)  -causes error due to pad output\n",
    "        accuracy = (results['true'] == results['pred']).mean() # currently broken\n",
    "    \n",
    "        imbalance = (results['true'].value_counts().max() / len(results))\n",
    "        print('Accuracy: %.2f%%' % (100 * accuracy))\n",
    "        # print('Imbalance (most common label): %.2f%%' % (100 * imbalance))\n",
    "        if len(set(unique_labels)) == 2:\n",
    "            y_true = results['true'].astype(float)\n",
    "            y_pred = results['pred'].astype(float)\n",
    "            print('MCC: %.2f%%' % (100 * matthews_corrcoef(y_true, y_pred)))           \n",
    "            print(\"F1 - macro avg %.2f%%\" % (100 * f1_score(y_true, y_pred, average='macro')))       \n",
    "            print(\"precision - micro avg %.2f%%\" % (100 * precision_score(y_true, y_pred, average='micro')))\n",
    "            print(\"Recall - macro avg %.2f%%\" % (100 * recall_score(y_true, y_pred, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Code comment: It would be great to __refactor__ this - it is currently cumbersome and not productionable. \n",
    "    * It should be rewritten to expect a train/valid/test set and Ys, (+- the target type, or a function for each type of target) and the Y preprocessing to be a seperate, user defined step, _not trying to infer it here as part of the loop_. \n",
    "    \n",
    "    \n",
    "    \n",
    "Data augmentation idea/experiment: \n",
    "    * Duplicate train subset, with sequence reversed (and also reverse y/labels if target is sequence_label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== signalP_binary.dataset ==========\n",
      "\n",
      "\n",
      "validation set ../data\\signalP_binary.dataset.valid.csv missing\n",
      "splitting train to train and random validation set\n",
      "Stratified sampling of validation set\n",
      "Numeric (not probabilistic) label\n",
      "y_discrete\n",
      "14945 training-set records, 1661 valid-set records, 4152 test-set records\n",
      "labels     int64\n",
      "text      object\n",
      "dtype: object\n",
      "labels     int64\n",
      "text      object\n",
      "dtype: object\n",
      "Categorical output with 2 labels.\n",
      "n_labels 2\n",
      "build_fine_tuning_model - n_labels 2\n",
      "model loss: binary_crossentropy\n",
      "Epoch 1/6\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/gamma:0', 'auto-encoder_2/beta:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/gamma:0', 'auto-encoder_2/beta:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/gamma:0', 'auto-encoder_2/beta:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/gamma:0', 'auto-encoder_2/beta:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/gamma:0', 'auto-encoder_2/beta:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/gamma:0', 'auto-encoder_2/beta:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/gamma:0', 'auto-encoder_2/beta:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/gamma:0', 'auto-encoder_2/beta:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0'] when minimizing the loss.\n",
      "468/468 [==============================] - 57s 123ms/step - loss: 0.4758 - val_loss: 0.4585 - lr: 2.0000e-04\n",
      "Epoch 2/6\n",
      "468/468 [==============================] - 62s 132ms/step - loss: 0.4694 - val_loss: 0.4586 - lr: 2.0000e-04\n",
      "Epoch 3/6\n",
      "468/468 [==============================] - 61s 131ms/step - loss: 0.4647 - val_loss: 0.4470 - lr: 2.0000e-04\n",
      "Epoch 4/6\n",
      "468/468 [==============================] - 62s 132ms/step - loss: 0.4822 - val_loss: 0.4473 - lr: 2.0000e-04\n",
      "Epoch 5/6\n",
      "468/468 [==============================] - 62s 132ms/step - loss: 0.4806 - val_loss: 0.4443 - lr: 2.0000e-04\n",
      "Epoch 6/6\n",
      "468/468 [==============================] - 60s 129ms/step - loss: 0.4656 - val_loss: 0.4449 - lr: 2.0000e-04\n",
      "\n",
      "*** Training-set performance: ***\n",
      "classification Report\n",
      "\n",
      "Accuracy: 83.75%\n",
      "MCC: 0.00%\n",
      "F1 - macro avg 45.58%\n",
      "precision - micro avg 83.75%\n",
      "Recall - macro avg 83.75%\n",
      "*** validation performance: ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91      1391\n",
      "           1       0.00      0.00      0.00       270\n",
      "\n",
      "    accuracy                           0.84      1661\n",
      "   macro avg       0.42      0.50      0.46      1661\n",
      "weighted avg       0.70      0.84      0.76      1661\n",
      "\n",
      "Accuracy: 0.00%\n",
      "MCC: 0.00%\n",
      "F1 - macro avg 45.58%\n",
      "precision - micro avg 83.74%\n",
      "Recall - macro avg 83.74%\n",
      "*** Test-set performance: ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91      3478\n",
      "           1       0.00      0.00      0.00       674\n",
      "\n",
      "    accuracy                           0.84      4152\n",
      "   macro avg       0.42      0.50      0.46      4152\n",
      "weighted avg       0.70      0.84      0.76      4152\n",
      "\n",
      "Accuracy: 0.00%\n",
      "MCC: 0.00%\n",
      "F1 - macro avg 45.58%\n",
      "precision - micro avg 83.77%\n",
      "Recall - macro avg 83.77%\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "def chunk_string(string:str, chunk_size:int):\n",
    "    return [string[i:(chunk_size + i)] for i in range(0, len(string), chunk_size)]\n",
    "\n",
    "def chunk_df(df, chunk_size:int):\n",
    "    return pd.DataFrame({col: df[col].apply(chunk_string, chunk_size = chunk_size).explode().values for col in df.columns})\n",
    "\n",
    "def load_benchmark_dataset(benchmark_name):\n",
    "    \n",
    "    text_file_path = os.path.join(BENCHMARKS_DIR, '%s.benchmark.txt' % benchmark_name)    \n",
    "    train_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.train.csv' % benchmark_name)\n",
    "    valid_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.valid.csv' % benchmark_name)\n",
    "    test_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.test.csv' % benchmark_name)\n",
    "    \n",
    "    train_set = pd.read_csv(train_set_file_path).dropna().drop_duplicates()\n",
    "          \n",
    "    if os.path.exists(valid_set_file_path):\n",
    "        valid_set = pd.read_csv(valid_set_file_path).dropna().drop_duplicates()\n",
    "    else:\n",
    "        print(f\"validation set {valid_set_file_path} missing\")\n",
    "        print(\"splitting train to train and random validation set\")\n",
    "        try: \n",
    "            train_set, valid_set = train_test_split(train_set, stratify=train_set['labels'],test_size=0.1,random_state=42)\n",
    "            print(\"Stratified sampling of validation set\")\n",
    "        except:\n",
    "            print(\"randomly sampling validation set\")\n",
    "            train_set, valid_set = train_test_split(train_set,test_size=0.1,random_state=42)    \n",
    "    \n",
    "    test_set = pd.read_csv(test_set_file_path).dropna().drop_duplicates()\n",
    "    \n",
    "    return text_file_path, train_set, valid_set, test_set\n",
    "\n",
    "\n",
    "## new\n",
    "def fast_run(train_set, valid_set, test_set, is_y_discrete):\n",
    "#     if \"remote_homology\" in benchmark_name:\n",
    "#         train_set = pd.concat([train_set.sample(frac=FAST_SAMPLE_RATIO,random_state=42),train_set.drop_duplicates('labels')]).drop_duplicates()\n",
    "    if is_y_discrete:\n",
    "        print(\"discrete_sampling\")\n",
    "#         t1,_ = train_test_split(train_set,stratify=train_set['labels'],train_size=FAST_SAMPLE_RATIO,random_state=42) # stratified sampling 0 for multiclass\n",
    "        t1,_ = train_test_split(train_set,train_size=FAST_SAMPLE_RATIO,random_state=42) # random, unstratified sampling - avoids errors with to ofew smaples per class (we combine it with at least 1 example per class below)\n",
    "        t2 = train_set.drop_duplicates('labels')\n",
    "#         t2 = train_set.groupby(['labels']).apply(lambda x: x.sample(2))\n",
    "        print(\"t2 shape\",t2.shape)\n",
    "        train_set = pd.concat([t1,t2]).drop_duplicates()\n",
    "                \n",
    "    else:\n",
    "        train_set = train_set.sample(frac=FAST_SAMPLE_RATIO,random_state=42)\n",
    "    valid_set = valid_set.sample(frac=FAST_SAMPLE_RATIO)\n",
    "    test_set = test_set.sample(frac=FAST_SAMPLE_RATIO)   \n",
    "    return train_set, valid_set, test_set\n",
    "\n",
    "def get_y_type(y):\n",
    "    '''\n",
    "    Determining which of the following y is:\n",
    "    1. Numeric (could be either probabalistic (i.e. in the range 0-1) or not)\n",
    "    2. Sequence\n",
    "    3. Categorical ### confusing - we return is_y_probability ??? # DAN\n",
    "    '''\n",
    "    is_y_numeric = np.issubdtype(y.dtype, np.floating)\n",
    "#     is_y_numeric = np.issubdtype(y.dtype, np.number)\n",
    "    if is_y_numeric:\n",
    "#         is_y_probability =  not isinstance(y, int)# ORIG - #y.min() >= 0 and y.max() <= 1\n",
    "        is_y_probability =  y.min() >= 0 and y.max() <= 1\n",
    "        is_y_seq = False\n",
    "    else: \n",
    "        is_y_probability = False\n",
    "        is_y_seq = y.astype(str).str.len().max()>14  # duuuuuudeeeeee\n",
    "        print('Numeric (%sprobabilistic) label' % ('' if is_y_probability else 'not '))\n",
    "    is_y_discrete = (not (is_y_seq or is_y_probability)) # may want to add condition based on cardinality? Otherwise fails for fluorescence\n",
    "    return is_y_numeric, is_y_seq, is_y_probability,is_y_discrete\n",
    "\n",
    "def build_fine_tuning_model(is_y_numeric, is_y_probability, is_y_seq,max_seq_len, n_labels,learning_rate= 2e-04):\n",
    "    model = create_model(max_seq_len)\n",
    "    if USE_PRETRAINED_WEIGHTS:\n",
    "        load_model_weights(model, PRETRAINED_MODEL_WEIGHTS_FILE_PATH)\n",
    "    input_seq_layer, input_annoatations_layer = model.input\n",
    "    output_seq_layer, output_annoatations_layer = model.output    \n",
    "    if is_y_numeric:\n",
    "        output_layer = keras.layers.Dense(1, activation = ('sigmoid' if is_y_probability else None))(output_annoatations_layer)\n",
    "        loss = 'binary_crossentropy' if (not is_y_probability) else 'mse' # DAN fix\n",
    "    elif is_y_seq:\n",
    "        output_layer = keras.layers.Dense(n_labels + 1, activation = 'softmax')(output_seq_layer)\n",
    "        loss = 'categorical_crossentropy'\n",
    "    \n",
    "    else: # non-seq categorical\n",
    "        print(\"build_fine_tuning_model - n_labels\",n_labels)\n",
    "#         if n_labels ==2:\n",
    "#             loss = 'binary_crossentropy' \n",
    "#         else:\n",
    "        loss = 'categorical_crossentropy' \n",
    "        output_layer = keras.layers.Dense(n_labels, activation = 'softmax')(output_annoatations_layer)\n",
    "    \n",
    "    if n_labels ==2:\n",
    "        loss = 'binary_crossentropy' \n",
    "    if DEBUG_MODE: print(\"model loss:\",loss)\n",
    "    model = keras.models.Model(inputs = [input_seq_layer, input_annoatations_layer], outputs = output_layer)\n",
    "    model.compile(optimizer = keras.optimizers.Adam(lr = learning_rate), loss = loss) \n",
    "    return model\n",
    "\n",
    "def encode_seq_Y(raw_Y, max_seq_len, n_labels, label_to_index):\n",
    "    # +1 for padding\n",
    "    Y = np.zeros((len(raw_Y), max_seq_len, n_labels + 1), dtype = np.int8)\n",
    "    for i, seq in enumerate(raw_Y):\n",
    "        for j, token in enumerate(seq):\n",
    "            Y[i, j, label_to_index[token]] = 1\n",
    "        Y[i, np.arange(len(seq), max_seq_len), n_labels] = 1\n",
    "    return Y\n",
    "\n",
    "\n",
    "def preproc_benchmark_dataset(train_set, valid_set, test_set, n_labels, label_to_index, max_seq_len, is_y_numeric, is_y_seq):\n",
    "    train_X = [\n",
    "        tokenize_seqs(train_set['text'].values, max_seq_len).astype(np.int32),\n",
    "        np.zeros((len(train_set), N_ANNOTATIONS), dtype = np.int8)\n",
    "    ]    \n",
    "    valid_X = [\n",
    "        tokenize_seqs(valid_set['text'].values, max_seq_len).astype(np.int32),\n",
    "        np.zeros((len(valid_set), N_ANNOTATIONS), dtype = np.int8)\n",
    "    ]    \n",
    "    test_X = [\n",
    "        tokenize_seqs(test_set['text'].values, max_seq_len).astype(np.int32),\n",
    "        np.zeros((len(test_set), N_ANNOTATIONS), dtype = np.int8)\n",
    "    ]    \n",
    "    if is_y_numeric:\n",
    "        train_Y = train_set['labels'].values\n",
    "        valid_Y = valid_set['labels'].values\n",
    "    elif is_y_seq:  \n",
    "        print(\"y-seq y encoding\")\n",
    "        train_Y = encode_seq_Y(train_set['labels'], max_seq_len, n_labels, label_to_index)\n",
    "        valid_Y = encode_seq_Y(valid_set['labels'], max_seq_len, n_labels, label_to_index)\n",
    "    else: # non-seq categorical       \n",
    "        #### i replaced our encode_categorical_y with TF's to_categoircal . TF's expects an integer input. \n",
    "        \n",
    "#         train_Y = encode_categorical_y(train_set['labels'], n_labels, label_to_index)\n",
    "#         valid_Y = encode_categorical_y(valid_set['labels'], n_labels, label_to_index)\n",
    "        train_Y = to_categorical(train_set['labels'])\n",
    "        valid_Y = to_categorical(valid_set['labels'])\n",
    "        \n",
    "    return train_X, valid_X, test_X, train_Y, valid_Y\n",
    "\n",
    "def train_and_eval(train_set, valid_set, test_set, is_y_numeric, is_y_probability, is_y_seq,\n",
    "                   n_labels, unique_labels, label_to_index):\n",
    "    train_X, valid_X, test_X, train_Y, valid_Y = preproc_benchmark_dataset(train_set, valid_set, test_set, \n",
    "                                                                           n_labels, label_to_index, MAX_GLOBAL_SEQ_LEN, is_y_numeric, is_y_seq)\n",
    "    model = build_fine_tuning_model(is_y_numeric, is_y_probability, is_y_seq, MAX_GLOBAL_SEQ_LEN, n_labels)\n",
    "    ### Train model, with early stopping on validation set\n",
    "    model.fit(train_X, train_Y,\n",
    "              batch_size = BATCH_SIZE,\n",
    "              validation_data=(valid_X,valid_Y),\n",
    "              callbacks = [ReduceLROnPlateau(patience=2,factor=0.35), EarlyStopping(patience=4)],\n",
    "              epochs = MAX_EPOCHS,\n",
    "              validation_batch_size=BATCH_SIZE,\n",
    "              verbose=1)\n",
    "    \n",
    "    #### DAN: NOTE - keras already keeps evaluation, train error data.\n",
    "    print('\\n*** Training-set performance: ***')\n",
    "    train_Y_pred = model.predict(train_X)\n",
    "    evaluate(train_Y_pred, train_set['labels'].values, is_y_numeric, is_y_seq, unique_labels)\n",
    "    print('*** validation performance: ***')\n",
    "    valid_Y_pred = model.predict(valid_X)\n",
    "    evaluate(valid_Y_pred, valid_set['labels'].values, is_y_numeric, is_y_seq, unique_labels)\n",
    "    print('*** Test-set performance: ***')\n",
    "    test_Y_pred = model.predict(test_X)\n",
    "    evaluate(test_Y_pred, test_set['labels'].values, is_y_numeric, is_y_seq, unique_labels)    \n",
    "    print('\\n' * 3)\n",
    "    \n",
    "def train_and_eval_after_removing_too_long_seqs(train_set, valid_set, test_set, is_y_numeric, is_y_probability, is_y_seq,\n",
    "                                                n_labels, unique_labels, label_to_index):\n",
    "    filtered_train_set = train_set[train_set['text'].str.len() <= MAX_ALLOWED_INPUT_SEQ]\n",
    "    filtered_valid_set = valid_set[valid_set['text'].str.len() <= MAX_ALLOWED_INPUT_SEQ]\n",
    "    filtered_test_set = test_set[test_set['text'].str.len() <= MAX_ALLOWED_INPUT_SEQ]\n",
    "    n_removed_train_set = len(train_set) - len(filtered_train_set)\n",
    "    ptg_removed_train_set = 100 * n_removed_train_set / len(train_set)\n",
    "    n_removed_valid_set = len(valid_set) - len(filtered_valid_set)\n",
    "    ptg_removed_valid_set = 100 * n_removed_valid_set / len(valid_set)\n",
    "    n_removed_test_set = len(test_set) - len(filtered_test_set)\n",
    "    ptg_removed_test_set = 100 * n_removed_test_set / len(test_set)\n",
    "    print('Trying to remove too long sequences. Removed %d of %d (%.1g%%) of the training set, %d of %d (%.1g%%) of the validation set and %d of %d (%.1g%%) of the test set' %\n",
    "            (n_removed_train_set, len(train_set), ptg_removed_train_set, n_removed_valid_set, len(valid_set), ptg_removed_valid_set, n_removed_test_set, len(test_set), ptg_removed_test_set))\n",
    "    train_and_eval(filtered_train_set, filtered_valid_set, filtered_test_set, is_y_numeric, is_y_probability, is_y_seq, n_labels, unique_labels, label_to_index)\n",
    "    \n",
    "def truncate_dataset(dataset, is_y_seq):\n",
    "    if is_y_seq:\n",
    "        return chunk_df(dataset, MAX_ALLOWED_INPUT_SEQ-2) ## -2 - avoid edge case of length equal to max +1 : would only leave the padding tokenss\n",
    "    else:\n",
    "        dataset = dataset.copy()\n",
    "        dataset['text'] = dataset['text'].apply(lambda seq: seq[:MAX_ALLOWED_INPUT_SEQ])\n",
    "        return dataset\n",
    "    \n",
    "def train_and_eval_after_trancating_too_long_seqs(train_set, valid_set, test_set, is_y_numeric, is_y_probability,\n",
    "                                                  is_y_seq, n_labels, unique_labels, label_to_index):\n",
    "    print('Will now truncate too-long sequences.')\n",
    "    train_and_eval(truncate_dataset(train_set, is_y_seq), truncate_dataset(valid_set, is_y_seq), truncate_dataset(test_set, is_y_seq), is_y_numeric, is_y_probability, is_y_seq,\n",
    "                   n_labels, unique_labels, label_to_index)\n",
    "\n",
    "    \n",
    "def run_benchmark(benchmark_name):\n",
    "    \n",
    "    print('========== %s ==========' % benchmark_name)   \n",
    "    print('\\n')\n",
    "    text_file_path, train_set, valid_set, test_set = load_benchmark_dataset(benchmark_name)\n",
    "    is_y_numeric, is_y_seq, is_y_probability,is_y_discrete = get_y_type(train_set['labels'])\n",
    "    if is_y_numeric: print(\"y_numeric\")\n",
    "    if is_y_seq: print(\"y_seq\")\n",
    "    if is_y_probability: print(\"y_probability\")\n",
    "    if is_y_discrete: print(\"y_discrete\")\n",
    "\n",
    "    if FAST_RUN: train_set, valid_set, test_set = fast_run(train_set, valid_set, test_set,is_y_discrete) # dan change - use new is_y_discrete\n",
    "    print(f'{len(train_set)} training-set records, {len(valid_set)} valid-set records, {len(test_set)} test-set records')     \n",
    "    \n",
    "#         ## add start and end tokens in advance of processing \n",
    "    train_set[\"text\"] = ['<START>']+train_set[\"text\"]+['<END>']\n",
    "    valid_set[\"text\"] = ['<START>']+valid_set[\"text\"]+['<END>']\n",
    "    test_set[\"text\"] = ['<START>']+test_set[\"text\"]+['<END>']\n",
    "\n",
    "    \n",
    "    if DEBUG_MODE:\n",
    "        print(train_set.dtypes)\n",
    "        print(valid_set.dtypes)\n",
    "        \n",
    "    if is_y_numeric:\n",
    "        ### stupid ugly hack - the numeric/continous targets don't have n_labels/unique_labels, but their surroudning functions expect them - I hack in a default for now, expect a refactor - DAN\n",
    "        n_labels=2 # default hack\n",
    "        unique_labels=(\"0\",\"1\") # default hack\n",
    "        label_to_index = {}\n",
    "        print(\"y numeric label hack done\")\n",
    "\n",
    "    if not is_y_numeric:\n",
    "        train_set['labels'] = train_set['labels'].astype(str)\n",
    "        if is_y_seq:            \n",
    "            unique_labels = sorted(set.union(*train_set['labels'].apply(set))) \n",
    "        else:\n",
    "            unique_labels = sorted(train_set['labels'].unique())                               \n",
    "        n_labels = len(unique_labels)\n",
    "        label_to_index = {label: i for i, label in enumerate(unique_labels)}        \n",
    "        print('Sequence output with %d tokens.' % n_labels if is_y_seq else 'Categorical output with %d labels.' % n_labels)\n",
    "     \n",
    "    \n",
    "#     if not is_y_numeric:\n",
    "#         if is_y_discrete:\n",
    "#             le = preprocessing.LabelEncoder()\n",
    "# #         if len(unique_labels)>2:\n",
    "#             print(\"\\n label encoding class labels\")\n",
    "# #                 ##i.e y is discrete classes\n",
    "# #                 print(\"unique train labels\",train_set['labels'].head())            \n",
    "# #             le = le.fit(train_set['labels'])\n",
    "\n",
    "#             train_set['labels'] = le.fit_transform(train_set['labels'])\n",
    "#             print(\"le\",le)\n",
    "#             print(\"le.classes_\",le.classes_)\n",
    "#             print(len(list(le.classes_)))\n",
    "#             valid_set['labels'] = le.transform(valid_set['labels'])\n",
    "#             test_set['labels'] = le.transform(test_set['labels'])\n",
    "    \n",
    "    if DEBUG_MODE:\n",
    "        print(\"n_labels\",n_labels)\n",
    "     \n",
    "    if max(train_set[\"text\"].str.len().max(), valid_set[\"text\"].str.len().max(), test_set[\"text\"].str.len().max()) <= MAX_ALLOWED_INPUT_SEQ:\n",
    "        train_and_eval(train_set, valid_set, test_set, is_y_numeric, is_y_probability, is_y_seq, n_labels, unique_labels, label_to_index)\n",
    "    else:\n",
    "        train_and_eval_after_removing_too_long_seqs(train_set, valid_set, test_set, is_y_numeric, is_y_probability, is_y_seq, n_labels, unique_labels, label_to_index)\n",
    "        \n",
    "        ## in 2d : ValueError: arrays must all be same length\n",
    "#         train_and_eval_after_trancating_too_long_seqs(train_set, valid_set, test_set, is_y_numeric, is_y_probability, is_y_seq, n_labels, unique_labels, label_to_index)\n",
    "\n",
    "        \n",
    "for benchmark_name in BENCHMARKS:\n",
    "    run_benchmark(benchmark_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation set ../data\\signalP_binary.dataset.valid.csv missing\n",
      "splitting train to train and random validation set\n",
      "Stratified sampling of validation set\n",
      "Numeric (not probabilistic) label\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    14945.000000\n",
       "mean         0.162462\n",
       "std          0.368887\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max          1.000000\n",
       "Name: labels, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train # unique labels 2\n",
      "valid # unique labels 2\n",
      "test # unique labels 2\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# text_file_path, train_set, valid_set, test_set = load_benchmark_dataset('remote_homology')\n",
    "text_file_path, train_set, valid_set, test_set = load_benchmark_dataset('signalP_binary.dataset')\n",
    "\n",
    "is_y_numeric, is_y_seq, is_y_probability,is_y_discrete = get_y_type(train_set['labels'])\n",
    "display(train_set['labels'].describe())\n",
    "\n",
    "print(\"train # unique labels\",train_set.labels.nunique())\n",
    "print(\"valid # unique labels\",valid_set.labels.nunique())\n",
    "print(\"test # unique labels\",test_set.labels.nunique())\n",
    "print(test_set['labels'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le LabelEncoder()\n",
      "le.classes_ [0 1]\n",
      "2\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "4147    1\n",
      "4148    0\n",
      "4149    0\n",
      "4150    0\n",
      "4151    0\n",
      "Name: labels, Length: 4152, dtype: int64\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "4147    1\n",
      "4148    0\n",
      "4149    0\n",
      "4150    0\n",
      "4151    0\n",
      "Name: labels, Length: 4152, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_set['labels'])\n",
    "print(\"le\",le)\n",
    "print(\"le.classes_\",le.classes_)\n",
    "print(len(list(le.classes_)))\n",
    "train_set['labels'] = le.transform(train_set['labels'])\n",
    "valid_set['labels'] = le.transform(valid_set['labels'])\n",
    "print(test_set['labels'] )\n",
    "test_set['labels'] = le.transform(test_set['labels'])\n",
    "print(test_set['labels'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-b83657a7c831>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-26-b83657a7c831>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    benchmark_name =\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "benchmark_name = \n",
    "text_file_path, train_set, valid_set, test_set = load_benchmark_dataset(benchmark_name)\n",
    "is_y_numeric, is_y_seq, is_y_probability,is_y_discrete = get_y_type(train_set['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
