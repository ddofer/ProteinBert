{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dan changes - writing code blindly\n",
    "    0. Add more evaluation metrics. \n",
    "    1. use early stopping (based on validatio nset)\n",
    "    2. For longer sequences than max_len - should truncate rather than dropping...\n",
    "        * COmplications - for problems with local labelling we can't truncate, as we would lose the label! \n",
    "        For local (per position) labeling - we will extract a sliding window / chunks. For other problems (whole seqence), we'll simply truncate (start+end) I think!\n",
    "    * put MAX_GLOBAL_SEQ_LEN as parameter instead of magic number\n",
    "    \n",
    "    \n",
    "    * NOTE: In `build_model` - I believe input datatypes chould be changed to int16, instead of int32 and float 32. (Would save on memory per batch) - it's just the input encoding and input sequences encoding?. \n",
    "    \n",
    "    \n",
    "* The \"target processing\" code could be cleaned up a LOT by refactoring to expect a TUPLE :\n",
    "    * `(\"name\", whole sequence/local\", \"binary/multiclass/regression\"). \n",
    "    \n",
    "    \n",
    "    \n",
    "    * Nadav's transformer code - \n",
    "         * https://github.com/nadavbra/keras-seq-vec-transformer/blob/master/keras_seq_vec_transformer.py\n",
    "         * (Huji CSE server - chaperone-02): `/cs/phd/nadavb/github_projects/keras_seq_vec_transformer/`\n",
    "         \n",
    "         \n",
    "DAN CHANGES - \n",
    "\n",
    "*  i replaced our encode_categorical_y with TF's `to_categorical` . TF's expects an integer input. I deleted the old function. (It gave errors).\n",
    "    * This necessitated processing the categorical class labels into numbers\n",
    "* Added `is_y_discrete` usage, and variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# from tensorflow import keras \n",
    "from tensorflow import keras as keras\n",
    "from IPython.display import display\n",
    "\n",
    "# from pwas.shared_utils.util import log\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, r2_score, f1_score, precision_score, recall_score, balanced_accuracy_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import classification_report, roc_auc_score, log_loss # DAN\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "KERAS_SEQ_VEC_SCRIPT_PATH = \"keras_seq_vec_transformer.py\"\n",
    "# \"\"/cs/phd/nadavb/github_projects/keras_seq_vec_transformer/keras_seq_vec_transformer.py\" # ORIGINAL path in CSE server\n",
    "with open(KERAS_SEQ_VEC_SCRIPT_PATH, 'r') as f:\n",
    "    '''\n",
    "    Some horrible hacks. To make keras_seq_vec_transformer work with tf.keras instead of just keras we run the following instead of\n",
    "    just exec(f.read()).\n",
    "    Note that also above we run from tensorflow import keras instead of just import keras\n",
    "    '''\n",
    "    import tensorflow.keras.backend as K \n",
    "    from tensorflow.keras.layers import LayerNormalization\n",
    "    exec('\\n'.join([line for line in f.read().splitlines() if not line.startswith('import keras') and not \\\n",
    "            line.startswith('from keras_layer_normalization')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_AAS = 'ACDEFGHIKLMNPQRSTUVWXY'\n",
    "N_ANNOTATIONS = 8943\n",
    "\n",
    "# BENCHMARKS_DIR = '/cs/phd/nadavb/cafa_project/data/proteomic_benchmarks'\n",
    "BENCHMARKS_DIR = '../data'\n",
    "\n",
    "PRETRAINED_MODEL_WEIGHTS_FILE_PATH = '../models/epoch_28530_sample_91400000.pkl'\n",
    "\n",
    "BENCHMARKS = [\n",
    "    'signalP_binary.dataset',\n",
    "    'scop.dataset',\n",
    "    'fluorescence', \n",
    "    'secondary_structure',\n",
    "    \n",
    "    'remote_homology', ## currently has bug with unseen labels / label encoding ?? \n",
    "#     'disorder_secondary_structure', ## error : ValueError: could not convert string to float: '<PAD>' \n",
    "#     \"PhosphositePTM.dataset\", ## error : ValueError: could not convert string to float: '<PAD>'  ### 9 min per epoch\n",
    "    \"stability\" \n",
    "             ]\n",
    "  #### \"phosphoserine.dataset\",   ## sequence texts and label lengths are different - dataset needs to be changed in advance or dropped\n",
    "\n",
    "MAX_GLOBAL_SEQ_LEN = 450 # 450 # set it here to make it easier to change. should be length used in training.  # DAN\n",
    "\n",
    "MAX_ALLOWED_INPUT_SEQ = MAX_GLOBAL_SEQ_LEN - 2\n",
    "\n",
    "MAX_EPOCHS=100 #80 # max train epochs,\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "DEBUG_MODE = False\n",
    "\n",
    "FAST_RUN = True # True # False#\n",
    "FAST_SAMPLE_RATIO = 0.15 # if doing fast run, downsample to roughly this percent of data from train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FAST_RUN:\n",
    "    MAX_EPOCHS = 1\n",
    "#     BATCH_SIZE = BATCH_SIZE//2 \n",
    "#     MAX_ALLOWED_INPUT_SEQ = 132"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_AAS = 'ACDEFGHIKLMNPQRSTUVWXY'\n",
    "ADDITIONAL_TOKENS = ['<OTHER>', '<START>', '<END>', '<PAD>']\n",
    "ADDED_TOKENS_PER_SEQ = 2\n",
    "n_aas = len(ALL_AAS)\n",
    "aa_to_token_index = {aa: i for i, aa in enumerate(ALL_AAS)}\n",
    "additional_token_to_index = {token: i + len(ALL_AAS) for i, token in enumerate(ADDITIONAL_TOKENS)}\n",
    "token_to_index = {**aa_to_token_index, **additional_token_to_index}\n",
    "index_to_token = {index: token for token, index in token_to_index.items()}\n",
    "n_tokens = len(token_to_index)\n",
    "\n",
    "def tokenize_seq(seq):\n",
    "    other_token_index = additional_token_to_index['<OTHER>']\n",
    "    return [additional_token_to_index['<START>']] + [aa_to_token_index.get(aa, other_token_index) for aa in seq] + [additional_token_to_index['<END>']]\n",
    "\n",
    "def tokenize_seqs(seqs,max_seq_len=MAX_GLOBAL_SEQ_LEN):   \n",
    "    tokenized_seqs = additional_token_to_index['<PAD>'] * np.ones((len(seqs), max_seq_len))\n",
    "    for i, seq in enumerate(seqs):\n",
    "        tokenized_seq = tokenize_seq(seq)\n",
    "        assert len(tokenized_seq) <= max_seq_len\n",
    "        tokenized_seqs[i, :len(tokenized_seq)] = tokenized_seq\n",
    "    return tokenized_seqs\n",
    "\n",
    "def create_model(max_seq_len):   \n",
    "    input_seq_layer = keras.layers.Input(shape = (max_seq_len,), dtype = np.int32, name = 'input-seq')\n",
    "    input_annoatations_layer = keras.layers.Input(shape = (N_ANNOTATIONS,), dtype = np.float32, name = 'input-annotations')\n",
    "    output_seq_layer, output_annoatations_layer = TransformerAutoEncoder(vocab_size = n_tokens, d_vec = N_ANNOTATIONS, output_vec_activation = 'sigmoid', name = 'auto-encoder')([input_seq_layer, input_annoatations_layer])\n",
    "    output_seq_layer = keras.layers.Reshape(output_seq_layer.shape[1:], name = 'output_seq_layer')(output_seq_layer)\n",
    "    output_annoatations_layer = keras.layers.Reshape(output_annoatations_layer.shape[1:], name = 'output_annoatations_layer')(output_annoatations_layer)\n",
    "    return keras.models.Model(inputs = [input_seq_layer, input_annoatations_layer], outputs = [output_seq_layer, output_annoatations_layer])\n",
    "\n",
    "def load_model_weights(model, path):\n",
    "    with open(path, 'rb') as f:\n",
    "        model_weights, optimizer_weights = pickle.load(f)\n",
    "        model.set_weights(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Y_pred, raw_y_true, is_y_numeric, is_y_seq, unique_labels):     \n",
    "    n_labels = len(unique_labels)\n",
    "    Y_pred_classes = Y_pred.argmax(axis = -1) \n",
    "    try:\n",
    "        print(\"classification Report\\n\")\n",
    "        print(classification_report(raw_y_true,Y_pred_classes))\n",
    "    except:pass    \n",
    "    try:\n",
    "        print(\"MCC %.4f%\" % matthews_corrcoef(raw_y_true,Y_pred_classes))\n",
    "    except:pass\n",
    "    try:\n",
    "        print(\"F1 - macro avg %.4f%\" % f1_score(raw_y_true,Y_pred_classes, average='macro'))\n",
    "        print(\"precision - micro avg %.2f%%\" % (100 * precision_score(raw_y_true,Y_pred_classes, average='micro')))\n",
    "        print(\"Recall - macro avg %.2f%%\" % (100 * recall_score(raw_y_true,Y_pred_classes, average='micro')))              \n",
    "        print(\"balanced_accuracy_score %.4f%\" % balanced_accuracy_score(raw_y_true,Y_pred_classes))\n",
    "    except:pass    \n",
    "    try:\n",
    "        print(\"r2 %.4f%\" % r2_score(raw_y_true,Y_pred.flatten())) # doesn't work? DAN\n",
    "        print(\"mean_absolute_error %.4f%\" % mean_absolute_error(raw_y_true,Y_pred.flatten()))\n",
    "    except: pass   \n",
    "    try: \n",
    "        print(\"roc_auc_score %.4f%\" % roc_auc_score(raw_y_true,Y_pred[:,1]))\n",
    "        print(\"log_loss %.4f%\" % log_loss(raw_y_true,Y_pred[:,1]))\n",
    "    except:pass    \n",
    "    if is_y_numeric:\n",
    "        results = pd.DataFrame({'true': raw_y_true, 'pred': Y_pred.flatten()})\n",
    "        print(\"spearman's rho (correlation)\",results.corr(method=\"spearman\"))\n",
    "        print('R^2 score: %.2g' % r2_score(results['true'], results['pred']))        \n",
    "        print(\"mean absolute error score %.4g\" % mean_absolute_error(results['true'], results['pred']))                    \n",
    "    else:\n",
    "        if is_y_seq:\n",
    "            results_true = []\n",
    "            results_pred = []\n",
    "            for true_seq, pred_seq in zip(raw_y_true, Y_pred.argmax(axis = -1)):\n",
    "                for true_token, pred_token_index in zip(true_seq, pred_seq):\n",
    "                    results_true.append(true_token)\n",
    "                    results_pred.append('<PAD>' if pred_token_index == n_labels else unique_labels[pred_token_index])\n",
    "            results = pd.DataFrame({'true': results_true, 'pred': results_pred})\n",
    "        else:\n",
    "            predicted_labels = [unique_labels[i] for i in Y_pred.argmax(axis = -1)]\n",
    "            results = pd.DataFrame({'true': raw_y_true, 'pred': predicted_labels})\n",
    "        confusion_matrix = results.groupby(['true', 'pred']).size().unstack().fillna(0)\n",
    "        if len(set(unique_labels))<20 and False:\n",
    "            print('Confusion matrix:')\n",
    "            display(confusion_matrix)\n",
    "\n",
    "            #         accuracy = (results['true'].astype(int) == results['pred'].astype(int)).mean() ## DAN - added .astype(int)  -causes error due to pad output\n",
    "        accuracy = (results['true'] == results['pred']).mean() # currently broken\n",
    "    \n",
    "        imbalance = (results['true'].value_counts().max() / len(results))\n",
    "        print('Accuracy: %.2f%%' % (100 * accuracy))\n",
    "        # print('Imbalance (most common label): %.2f%%' % (100 * imbalance))\n",
    "        if len(set(unique_labels)) == 2:\n",
    "            y_true = results['true'].astype(float)\n",
    "            y_pred = results['pred'].astype(float)\n",
    "            print('MCC: %.2f%%' % (100 * matthews_corrcoef(y_true, y_pred)))           \n",
    "            print(\"F1 - macro avg %.2f%%\" % (100 * f1_score(y_true, y_pred, average='macro')))       \n",
    "            print(\"precision - micro avg %.2f%%\" % (100 * precision_score(y_true, y_pred, average='micro')))\n",
    "            print(\"Recall - macro avg %.2f%%\" % (100 * recall_score(y_true, y_pred, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Code comment: It would be great to __refactor__ this - it is currently cumbersome and not productionable. \n",
    "    * It should be rewritten to expect a train/valid/test set and Ys, (+- the target type, or a function for each type of target) and the Y preprocessing to be a seperate, user defined step, _not trying to infer it here as part of the loop_. \n",
    "    \n",
    "    \n",
    "    \n",
    "Data augmentation idea/experiment: \n",
    "    * Duplicate train subset, with sequence reversed (and also reverse y/labels if target is sequence_label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== signalP_binary.dataset ==========\n",
      "\n",
      "\n",
      "validation set ../data\\signalP_binary.dataset.valid.csv missing\n",
      "splitting train to train and random validation set\n",
      "Stratified sampling of validation set\n",
      "y_numeric\n",
      "y_probability\n",
      "2242 training-set records, 249 valid-set records, 623 test-set records\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder/kernel:0', 'auto-encoder/bias:0', 'auto-encoder/kernel:0', 'auto-encoder/bias:0', 'auto-encoder/gamma:0', 'auto-encoder/beta:0', 'auto-encoder/kernel:0', 'auto-encoder/bias:0', 'auto-encoder/gamma:0', 'auto-encoder/beta:0', 'auto-encoder/kernel:0', 'auto-encoder/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder/kernel:0', 'auto-encoder/bias:0', 'auto-encoder/kernel:0', 'auto-encoder/bias:0', 'auto-encoder/gamma:0', 'auto-encoder/beta:0', 'auto-encoder/kernel:0', 'auto-encoder/bias:0', 'auto-encoder/gamma:0', 'auto-encoder/beta:0', 'auto-encoder/kernel:0', 'auto-encoder/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder/kernel:0', 'auto-encoder/bias:0', 'auto-encoder/kernel:0', 'auto-encoder/bias:0', 'auto-encoder/gamma:0', 'auto-encoder/beta:0', 'auto-encoder/kernel:0', 'auto-encoder/bias:0', 'auto-encoder/gamma:0', 'auto-encoder/beta:0', 'auto-encoder/kernel:0', 'auto-encoder/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder/kernel:0', 'auto-encoder/bias:0', 'auto-encoder/kernel:0', 'auto-encoder/bias:0', 'auto-encoder/gamma:0', 'auto-encoder/beta:0', 'auto-encoder/kernel:0', 'auto-encoder/bias:0', 'auto-encoder/gamma:0', 'auto-encoder/beta:0', 'auto-encoder/kernel:0', 'auto-encoder/bias:0'] when minimizing the loss.\n",
      "141/141 [==============================] - 34s 238ms/step - loss: 0.0944 - val_loss: 0.0485 - lr: 2.0000e-04\n",
      "Epoch 2/2\n",
      "141/141 [==============================] - 32s 228ms/step - loss: 0.0392 - val_loss: 0.0446 - lr: 2.0000e-04\n",
      "\n",
      "*** Training-set performance: ***\n",
      "classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91      1859\n",
      "           1       0.00      0.00      0.00       383\n",
      "\n",
      "    accuracy                           0.83      2242\n",
      "   macro avg       0.41      0.50      0.45      2242\n",
      "weighted avg       0.69      0.83      0.75      2242\n",
      "\n",
      "spearman's rho (correlation)           true      pred\n",
      "true  1.000000  0.629348\n",
      "pred  0.629348  1.000000\n",
      "R^2 score: 0.8\n",
      "mean absolute error score 0.06833\n",
      "*** validation performance: ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92       211\n",
      "           1       0.00      0.00      0.00        38\n",
      "\n",
      "    accuracy                           0.85       249\n",
      "   macro avg       0.42      0.50      0.46       249\n",
      "weighted avg       0.72      0.85      0.78       249\n",
      "\n",
      "spearman's rho (correlation)          true     pred\n",
      "true  1.00000  0.57424\n",
      "pred  0.57424  1.00000\n",
      "R^2 score: 0.66\n",
      "mean absolute error score 0.08963\n",
      "*** Test-set performance: ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90       512\n",
      "           1       0.00      0.00      0.00       111\n",
      "\n",
      "    accuracy                           0.82       623\n",
      "   macro avg       0.41      0.50      0.45       623\n",
      "weighted avg       0.68      0.82      0.74       623\n",
      "\n",
      "spearman's rho (correlation)           true      pred\n",
      "true  1.000000  0.629683\n",
      "pred  0.629683  1.000000\n",
      "R^2 score: 0.66\n",
      "mean absolute error score 0.09571\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "========== scop.dataset ==========\n",
      "\n",
      "\n",
      "validation set ../data\\scop.dataset.valid.csv missing\n",
      "splitting train to train and random validation set\n",
      "Stratified sampling of validation set\n",
      "Numeric (not probabilistic) label\n",
      "y_discrete\n",
      "discrete_sampling\n",
      "t2 shape (7, 2)\n",
      "2121 training-set records, 235 valid-set records, 588 test-set records\n",
      "Categorical output with 7 labels.\n",
      "\n",
      " label encoding class labels\n",
      "le LabelEncoder()\n",
      "le.classes_ ['a' 'b' 'c' 'd' 'e' 'f' 'g']\n",
      "7\n",
      "Trying to remove too long sequences. Removed 42 of 2121 (2%) of the training set, 5 of 235 (2%) of the validation set and 13 of 588 (2%) of the test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_fine_tuning_model - n_labels 7\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_1/kernel:0', 'auto-encoder_1/bias:0', 'auto-encoder_1/kernel:0', 'auto-encoder_1/bias:0', 'auto-encoder_1/gamma:0', 'auto-encoder_1/beta:0', 'auto-encoder_1/kernel:0', 'auto-encoder_1/bias:0', 'auto-encoder_1/gamma:0', 'auto-encoder_1/beta:0', 'auto-encoder_1/kernel:0', 'auto-encoder_1/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_1/kernel:0', 'auto-encoder_1/bias:0', 'auto-encoder_1/kernel:0', 'auto-encoder_1/bias:0', 'auto-encoder_1/gamma:0', 'auto-encoder_1/beta:0', 'auto-encoder_1/kernel:0', 'auto-encoder_1/bias:0', 'auto-encoder_1/gamma:0', 'auto-encoder_1/beta:0', 'auto-encoder_1/kernel:0', 'auto-encoder_1/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_1/kernel:0', 'auto-encoder_1/bias:0', 'auto-encoder_1/kernel:0', 'auto-encoder_1/bias:0', 'auto-encoder_1/gamma:0', 'auto-encoder_1/beta:0', 'auto-encoder_1/kernel:0', 'auto-encoder_1/bias:0', 'auto-encoder_1/gamma:0', 'auto-encoder_1/beta:0', 'auto-encoder_1/kernel:0', 'auto-encoder_1/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_1/kernel:0', 'auto-encoder_1/bias:0', 'auto-encoder_1/kernel:0', 'auto-encoder_1/bias:0', 'auto-encoder_1/gamma:0', 'auto-encoder_1/beta:0', 'auto-encoder_1/kernel:0', 'auto-encoder_1/bias:0', 'auto-encoder_1/gamma:0', 'auto-encoder_1/beta:0', 'auto-encoder_1/kernel:0', 'auto-encoder_1/bias:0'] when minimizing the loss.\n",
      "130/130 [==============================] - 32s 243ms/step - loss: 1.3839 - val_loss: 1.0617 - lr: 2.0000e-04\n",
      "Epoch 2/2\n",
      "130/130 [==============================] - 30s 232ms/step - loss: 0.7690 - val_loss: 0.9974 - lr: 2.0000e-04\n",
      "\n",
      "*** Training-set performance: ***\n",
      "classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.92      0.82       336\n",
      "           1       0.79      0.88      0.83       456\n",
      "           2       0.87      0.92      0.90       597\n",
      "           3       0.83      0.62      0.71       503\n",
      "           4       1.00      0.05      0.10        39\n",
      "           5       1.00      0.66      0.79        29\n",
      "           6       0.89      0.99      0.94       119\n",
      "\n",
      "    accuracy                           0.82      2079\n",
      "   macro avg       0.88      0.72      0.73      2079\n",
      "weighted avg       0.83      0.82      0.81      2079\n",
      "\n",
      "Accuracy: 0.00%\n",
      "*** validation performance: ***\n",
      "classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.79      0.65        38\n",
      "           1       0.59      0.67      0.63        49\n",
      "           2       0.74      0.76      0.75        68\n",
      "           3       0.47      0.31      0.37        62\n",
      "           4       0.00      0.00      0.00         5\n",
      "           5       1.00      0.50      0.67         4\n",
      "           6       0.57      1.00      0.73         4\n",
      "\n",
      "    accuracy                           0.61       230\n",
      "   macro avg       0.56      0.58      0.54       230\n",
      "weighted avg       0.59      0.61      0.59       230\n",
      "\n",
      "Accuracy: 0.00%\n",
      "*** Test-set performance: ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.76      0.69       107\n",
      "           1       0.58      0.65      0.61       111\n",
      "           2       0.73      0.85      0.79       163\n",
      "           3       0.59      0.38      0.46       134\n",
      "           4       0.00      0.00      0.00        13\n",
      "           5       1.00      0.56      0.71         9\n",
      "           6       0.81      0.89      0.85        38\n",
      "\n",
      "    accuracy                           0.66       575\n",
      "   macro avg       0.62      0.58      0.59       575\n",
      "weighted avg       0.64      0.66      0.64       575\n",
      "\n",
      "Accuracy: 0.00%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Will now truncate too-long sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_fine_tuning_model - n_labels 7\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/gamma:0', 'auto-encoder_2/beta:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/gamma:0', 'auto-encoder_2/beta:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/gamma:0', 'auto-encoder_2/beta:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/gamma:0', 'auto-encoder_2/beta:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/gamma:0', 'auto-encoder_2/beta:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/gamma:0', 'auto-encoder_2/beta:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/gamma:0', 'auto-encoder_2/beta:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0', 'auto-encoder_2/gamma:0', 'auto-encoder_2/beta:0', 'auto-encoder_2/kernel:0', 'auto-encoder_2/bias:0'] when minimizing the loss.\n",
      "133/133 [==============================] - 32s 241ms/step - loss: 1.3799 - val_loss: 1.0611 - lr: 2.0000e-04\n",
      "Epoch 2/2\n",
      "133/133 [==============================] - 31s 234ms/step - loss: 0.7612 - val_loss: 1.0096 - lr: 2.0000e-04\n",
      "\n",
      "*** Training-set performance: ***\n",
      "classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.92      0.83       345\n",
      "           1       0.81      0.83      0.82       460\n",
      "           2       0.89      0.93      0.91       615\n",
      "           3       0.78      0.69      0.73       505\n",
      "           4       1.00      0.11      0.20        45\n",
      "           5       1.00      0.62      0.77        32\n",
      "           6       0.95      0.97      0.96       119\n",
      "\n",
      "    accuracy                           0.83      2121\n",
      "   macro avg       0.89      0.73      0.75      2121\n",
      "weighted avg       0.83      0.83      0.82      2121\n",
      "\n",
      "Accuracy: 0.00%\n",
      "*** validation performance: ***\n",
      "classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.85      0.68        40\n",
      "           1       0.63      0.63      0.63        49\n",
      "           2       0.76      0.72      0.74        69\n",
      "           3       0.52      0.44      0.47        62\n",
      "           4       0.00      0.00      0.00         7\n",
      "           5       1.00      0.50      0.67         4\n",
      "           6       0.67      1.00      0.80         4\n",
      "\n",
      "    accuracy                           0.63       235\n",
      "   macro avg       0.59      0.59      0.57       235\n",
      "weighted avg       0.62      0.63      0.62       235\n",
      "\n",
      "Accuracy: 0.00%\n",
      "*** Test-set performance: ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.78      0.70       108\n",
      "           1       0.62      0.62      0.62       111\n",
      "           2       0.76      0.84      0.80       174\n",
      "           3       0.53      0.41      0.47       135\n",
      "           4       0.00      0.00      0.00        13\n",
      "           5       1.00      0.56      0.71         9\n",
      "           6       0.85      0.87      0.86        38\n",
      "\n",
      "    accuracy                           0.67       588\n",
      "   macro avg       0.63      0.58      0.59       588\n",
      "weighted avg       0.65      0.67      0.66       588\n",
      "\n",
      "Accuracy: 0.00%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "========== fluorescence ==========\n",
      "\n",
      "\n",
      "y_numeric\n",
      "y_probability\n",
      "3217 training-set records, 804 valid-set records, 4083 test-set records\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_3/kernel:0', 'auto-encoder_3/bias:0', 'auto-encoder_3/kernel:0', 'auto-encoder_3/bias:0', 'auto-encoder_3/gamma:0', 'auto-encoder_3/beta:0', 'auto-encoder_3/kernel:0', 'auto-encoder_3/bias:0', 'auto-encoder_3/gamma:0', 'auto-encoder_3/beta:0', 'auto-encoder_3/kernel:0', 'auto-encoder_3/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_3/kernel:0', 'auto-encoder_3/bias:0', 'auto-encoder_3/kernel:0', 'auto-encoder_3/bias:0', 'auto-encoder_3/gamma:0', 'auto-encoder_3/beta:0', 'auto-encoder_3/kernel:0', 'auto-encoder_3/bias:0', 'auto-encoder_3/gamma:0', 'auto-encoder_3/beta:0', 'auto-encoder_3/kernel:0', 'auto-encoder_3/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_3/kernel:0', 'auto-encoder_3/bias:0', 'auto-encoder_3/kernel:0', 'auto-encoder_3/bias:0', 'auto-encoder_3/gamma:0', 'auto-encoder_3/beta:0', 'auto-encoder_3/kernel:0', 'auto-encoder_3/bias:0', 'auto-encoder_3/gamma:0', 'auto-encoder_3/beta:0', 'auto-encoder_3/kernel:0', 'auto-encoder_3/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_3/kernel:0', 'auto-encoder_3/bias:0', 'auto-encoder_3/kernel:0', 'auto-encoder_3/bias:0', 'auto-encoder_3/gamma:0', 'auto-encoder_3/beta:0', 'auto-encoder_3/kernel:0', 'auto-encoder_3/bias:0', 'auto-encoder_3/gamma:0', 'auto-encoder_3/beta:0', 'auto-encoder_3/kernel:0', 'auto-encoder_3/bias:0'] when minimizing the loss.\n",
      "202/202 [==============================] - 51s 254ms/step - loss: 5.4859 - val_loss: 5.4930 - lr: 2.0000e-04\n",
      "Epoch 2/2\n",
      "202/202 [==============================] - 49s 242ms/step - loss: 5.3572 - val_loss: 5.4930 - lr: 2.0000e-04\n",
      "\n",
      "*** Training-set performance: ***\n",
      "classification Report\n",
      "\n",
      "spearman's rho (correlation)           true      pred\n",
      "true  1.000000  0.112098\n",
      "pred  0.112098  1.000000\n",
      "R^2 score: -6.4\n",
      "mean absolute error score 2.153\n",
      "*** validation performance: ***\n",
      "classification Report\n",
      "\n",
      "spearman's rho (correlation)           true      pred\n",
      "true  1.000000  0.115177\n",
      "pred  0.115177  1.000000\n",
      "R^2 score: -7\n",
      "mean absolute error score 2.192\n",
      "*** Test-set performance: ***\n",
      "classification Report\n",
      "\n",
      "spearman's rho (correlation)           true      pred\n",
      "true  1.000000  0.189426\n",
      "pred  0.189426  1.000000\n",
      "R^2 score: -1.2\n",
      "mean absolute error score 1.08\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "========== secondary_structure ==========\n",
      "\n",
      "\n",
      "Numeric (not probabilistic) label\n",
      "y_seq\n",
      "1302 training-set records, 326 valid-set records, 65 test-set records\n",
      "Sequence output with 3 tokens.\n",
      "Trying to remove too long sequences. Removed 129 of 1302 (1e+01%) of the training set, 28 of 326 (9%) of the validation set and 6 of 65 (9%) of the test set\n",
      "y-seq y encoding\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_4/kernel:0', 'auto-encoder_4/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_4/kernel:0', 'auto-encoder_4/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_4/kernel:0', 'auto-encoder_4/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_4/kernel:0', 'auto-encoder_4/bias:0'] when minimizing the loss.\n",
      "74/74 [==============================] - 21s 278ms/step - loss: 1.3166 - val_loss: 1.2728 - lr: 2.0000e-04\n",
      "Epoch 2/2\n",
      "74/74 [==============================] - 19s 252ms/step - loss: 1.2373 - val_loss: 1.2187 - lr: 2.0000e-04\n",
      "\n",
      "*** Training-set performance: ***\n",
      "classification Report\n",
      "\n",
      "Accuracy: 42.05%\n",
      "*** validation performance: ***\n",
      "classification Report\n",
      "\n",
      "Accuracy: 42.09%\n",
      "*** Test-set performance: ***\n",
      "classification Report\n",
      "\n",
      "Accuracy: 43.91%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Will now truncate too-long sequences.\n",
      "y-seq y encoding\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_5/kernel:0', 'auto-encoder_5/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_5/kernel:0', 'auto-encoder_5/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_5/kernel:0', 'auto-encoder_5/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_5/kernel:0', 'auto-encoder_5/bias:0'] when minimizing the loss.\n",
      "90/90 [==============================] - 25s 277ms/step - loss: 1.2865 - val_loss: 1.2419 - lr: 2.0000e-04\n",
      "Epoch 2/2\n",
      "90/90 [==============================] - 23s 256ms/step - loss: 1.2016 - val_loss: 1.1722 - lr: 2.0000e-04\n",
      "\n",
      "*** Training-set performance: ***\n",
      "classification Report\n",
      "\n",
      "Accuracy: 52.09%\n",
      "*** validation performance: ***\n",
      "classification Report\n",
      "\n",
      "Accuracy: 52.19%\n",
      "*** Test-set performance: ***\n",
      "classification Report\n",
      "\n",
      "Accuracy: 52.13%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "========== remote_homology ==========\n",
      "\n",
      "\n",
      "y_numeric\n",
      "y_probability\n",
      "1847 training-set records, 110 valid-set records, 108 test-set records\n",
      "Trying to remove too long sequences. Removed 41 of 1847 (2%) of the training set, 5 of 110 (5%) of the validation set and 1 of 108 (0.9%) of the test set\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_6/kernel:0', 'auto-encoder_6/bias:0', 'auto-encoder_6/kernel:0', 'auto-encoder_6/bias:0', 'auto-encoder_6/gamma:0', 'auto-encoder_6/beta:0', 'auto-encoder_6/kernel:0', 'auto-encoder_6/bias:0', 'auto-encoder_6/gamma:0', 'auto-encoder_6/beta:0', 'auto-encoder_6/kernel:0', 'auto-encoder_6/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_6/kernel:0', 'auto-encoder_6/bias:0', 'auto-encoder_6/kernel:0', 'auto-encoder_6/bias:0', 'auto-encoder_6/gamma:0', 'auto-encoder_6/beta:0', 'auto-encoder_6/kernel:0', 'auto-encoder_6/bias:0', 'auto-encoder_6/gamma:0', 'auto-encoder_6/beta:0', 'auto-encoder_6/kernel:0', 'auto-encoder_6/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_6/kernel:0', 'auto-encoder_6/bias:0', 'auto-encoder_6/kernel:0', 'auto-encoder_6/bias:0', 'auto-encoder_6/gamma:0', 'auto-encoder_6/beta:0', 'auto-encoder_6/kernel:0', 'auto-encoder_6/bias:0', 'auto-encoder_6/gamma:0', 'auto-encoder_6/beta:0', 'auto-encoder_6/kernel:0', 'auto-encoder_6/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_6/kernel:0', 'auto-encoder_6/bias:0', 'auto-encoder_6/kernel:0', 'auto-encoder_6/bias:0', 'auto-encoder_6/gamma:0', 'auto-encoder_6/beta:0', 'auto-encoder_6/kernel:0', 'auto-encoder_6/bias:0', 'auto-encoder_6/gamma:0', 'auto-encoder_6/beta:0', 'auto-encoder_6/kernel:0', 'auto-encoder_6/bias:0'] when minimizing the loss.\n",
      "113/113 [==============================] - 27s 241ms/step - loss: 109804.8906 - val_loss: 49355.7539 - lr: 2.0000e-04\n",
      "Epoch 2/2\n",
      "113/113 [==============================] - 25s 223ms/step - loss: 109750.5703 - val_loss: 49354.2109 - lr: 2.0000e-04\n",
      "\n",
      "*** Training-set performance: ***\n",
      "classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      1.00      0.01        11\n",
      "           1       0.00      0.00      0.00         5\n",
      "           3       0.00      0.00      0.00         9\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00         2\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00         6\n",
      "          12       0.00      0.00      0.00         4\n",
      "          13       0.00      0.00      0.00         9\n",
      "          14       0.00      0.00      0.00        10\n",
      "          15       0.00      0.00      0.00         2\n",
      "          16       0.00      0.00      0.00         1\n",
      "          17       0.00      0.00      0.00         8\n",
      "          18       0.00      0.00      0.00        11\n",
      "          19       0.00      0.00      0.00        11\n",
      "          20       0.00      0.00      0.00         2\n",
      "          21       0.00      0.00      0.00        22\n",
      "          22       0.00      0.00      0.00        52\n",
      "          23       0.00      0.00      0.00         1\n",
      "          24       0.00      0.00      0.00        10\n",
      "          25       0.00      0.00      0.00         6\n",
      "          26       0.00      0.00      0.00        11\n",
      "          27       0.00      0.00      0.00         2\n",
      "          28       0.00      0.00      0.00         5\n",
      "          31       0.00      0.00      0.00         2\n",
      "          32       0.00      0.00      0.00         1\n",
      "          33       0.00      0.00      0.00         2\n",
      "          34       0.00      0.00      0.00         1\n",
      "          35       0.00      0.00      0.00         2\n",
      "          36       0.00      0.00      0.00       138\n",
      "          37       0.00      0.00      0.00         1\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00        10\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.00      0.00      0.00         1\n",
      "          42       0.00      0.00      0.00         8\n",
      "          43       0.00      0.00      0.00         4\n",
      "          44       0.00      0.00      0.00         2\n",
      "          45       0.00      0.00      0.00        17\n",
      "          46       0.00      0.00      0.00         2\n",
      "          47       0.00      0.00      0.00        32\n",
      "          48       0.00      0.00      0.00         4\n",
      "          49       0.00      0.00      0.00         7\n",
      "          50       0.00      0.00      0.00         2\n",
      "          51       0.00      0.00      0.00        31\n",
      "          52       0.00      0.00      0.00         6\n",
      "          53       0.00      0.00      0.00         4\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.00      0.00      0.00         4\n",
      "          56       0.00      0.00      0.00        10\n",
      "          57       0.00      0.00      0.00         6\n",
      "          58       0.00      0.00      0.00         2\n",
      "          59       0.00      0.00      0.00         7\n",
      "          60       0.00      0.00      0.00        18\n",
      "          61       0.00      0.00      0.00         6\n",
      "          63       0.00      0.00      0.00         2\n",
      "          64       0.00      0.00      0.00         3\n",
      "          65       0.00      0.00      0.00         2\n",
      "          66       0.00      0.00      0.00         7\n",
      "          67       0.00      0.00      0.00         2\n",
      "          68       0.00      0.00      0.00         4\n",
      "          69       0.00      0.00      0.00         1\n",
      "          70       0.00      0.00      0.00        16\n",
      "          71       0.00      0.00      0.00         2\n",
      "          72       0.00      0.00      0.00         4\n",
      "          73       0.00      0.00      0.00        65\n",
      "          74       0.00      0.00      0.00         2\n",
      "          75       0.00      0.00      0.00         8\n",
      "          76       0.00      0.00      0.00         2\n",
      "          77       0.00      0.00      0.00        34\n",
      "          78       0.00      0.00      0.00        20\n",
      "          79       0.00      0.00      0.00         2\n",
      "          80       0.00      0.00      0.00         4\n",
      "          81       0.00      0.00      0.00        18\n",
      "          83       0.00      0.00      0.00         3\n",
      "          84       0.00      0.00      0.00        43\n",
      "          85       0.00      0.00      0.00         2\n",
      "          86       0.00      0.00      0.00         2\n",
      "          87       0.00      0.00      0.00         3\n",
      "          88       0.00      0.00      0.00        24\n",
      "          89       0.00      0.00      0.00         6\n",
      "          90       0.00      0.00      0.00        20\n",
      "          91       0.00      0.00      0.00         3\n",
      "          92       0.00      0.00      0.00         4\n",
      "          93       0.00      0.00      0.00         2\n",
      "          94       0.00      0.00      0.00         2\n",
      "          95       0.00      0.00      0.00        11\n",
      "          96       0.00      0.00      0.00         2\n",
      "          97       0.00      0.00      0.00         8\n",
      "          98       0.00      0.00      0.00         1\n",
      "         100       0.00      0.00      0.00         2\n",
      "         101       0.00      0.00      0.00         2\n",
      "         102       0.00      0.00      0.00         3\n",
      "         103       0.00      0.00      0.00         4\n",
      "         105       0.00      0.00      0.00         1\n",
      "         106       0.00      0.00      0.00         1\n",
      "         107       0.00      0.00      0.00         2\n",
      "         108       0.00      0.00      0.00         5\n",
      "         109       0.00      0.00      0.00         6\n",
      "         110       0.00      0.00      0.00         3\n",
      "         112       0.00      0.00      0.00         3\n",
      "         113       0.00      0.00      0.00        10\n",
      "         114       0.00      0.00      0.00         3\n",
      "         115       0.00      0.00      0.00         4\n",
      "         117       0.00      0.00      0.00         1\n",
      "         118       0.00      0.00      0.00         4\n",
      "         119       0.00      0.00      0.00         9\n",
      "         120       0.00      0.00      0.00         1\n",
      "         121       0.00      0.00      0.00         1\n",
      "         122       0.00      0.00      0.00         4\n",
      "         124       0.00      0.00      0.00         8\n",
      "         125       0.00      0.00      0.00         1\n",
      "         126       0.00      0.00      0.00        27\n",
      "         127       0.00      0.00      0.00         7\n",
      "         128       0.00      0.00      0.00         2\n",
      "         129       0.00      0.00      0.00         5\n",
      "         130       0.00      0.00      0.00         5\n",
      "         131       0.00      0.00      0.00         2\n",
      "         132       0.00      0.00      0.00         9\n",
      "         133       0.00      0.00      0.00         9\n",
      "         134       0.00      0.00      0.00         1\n",
      "         135       0.00      0.00      0.00        12\n",
      "         137       0.00      0.00      0.00         2\n",
      "         138       0.00      0.00      0.00         4\n",
      "         139       0.00      0.00      0.00         8\n",
      "         140       0.00      0.00      0.00         6\n",
      "         141       0.00      0.00      0.00         2\n",
      "         142       0.00      0.00      0.00         2\n",
      "         143       0.00      0.00      0.00         8\n",
      "         144       0.00      0.00      0.00         4\n",
      "         145       0.00      0.00      0.00         3\n",
      "         146       0.00      0.00      0.00         5\n",
      "         147       0.00      0.00      0.00         4\n",
      "         148       0.00      0.00      0.00         3\n",
      "         149       0.00      0.00      0.00         2\n",
      "         150       0.00      0.00      0.00         3\n",
      "         151       0.00      0.00      0.00         3\n",
      "         152       0.00      0.00      0.00         4\n",
      "         153       0.00      0.00      0.00        48\n",
      "         154       0.00      0.00      0.00         2\n",
      "         155       0.00      0.00      0.00         2\n",
      "         156       0.00      0.00      0.00         4\n",
      "         157       0.00      0.00      0.00         3\n",
      "         158       0.00      0.00      0.00         2\n",
      "         159       0.00      0.00      0.00         6\n",
      "         161       0.00      0.00      0.00         6\n",
      "         162       0.00      0.00      0.00         9\n",
      "         163       0.00      0.00      0.00         1\n",
      "         164       0.00      0.00      0.00         3\n",
      "         165       0.00      0.00      0.00         5\n",
      "         166       0.00      0.00      0.00         3\n",
      "         167       0.00      0.00      0.00         3\n",
      "         168       0.00      0.00      0.00         3\n",
      "         169       0.00      0.00      0.00         1\n",
      "         171       0.00      0.00      0.00         6\n",
      "         174       0.00      0.00      0.00         2\n",
      "         175       0.00      0.00      0.00         2\n",
      "         176       0.00      0.00      0.00        41\n",
      "         177       0.00      0.00      0.00         1\n",
      "         178       0.00      0.00      0.00        12\n",
      "         179       0.00      0.00      0.00        11\n",
      "         180       0.00      0.00      0.00         8\n",
      "         181       0.00      0.00      0.00         2\n",
      "         182       0.00      0.00      0.00         4\n",
      "         183       0.00      0.00      0.00         1\n",
      "         184       0.00      0.00      0.00         1\n",
      "         186       0.00      0.00      0.00         1\n",
      "         187       0.00      0.00      0.00         5\n",
      "         189       0.00      0.00      0.00         1\n",
      "         195       0.00      0.00      0.00         5\n",
      "         196       0.00      0.00      0.00         1\n",
      "         197       0.00      0.00      0.00         1\n",
      "         198       0.00      0.00      0.00         1\n",
      "         199       0.00      0.00      0.00         1\n",
      "         201       0.00      0.00      0.00         2\n",
      "         202       0.00      0.00      0.00         1\n",
      "         203       0.00      0.00      0.00         1\n",
      "         204       0.00      0.00      0.00         1\n",
      "         208       0.00      0.00      0.00         2\n",
      "         209       0.00      0.00      0.00         3\n",
      "         210       0.00      0.00      0.00         2\n",
      "         213       0.00      0.00      0.00         1\n",
      "         218       0.00      0.00      0.00         3\n",
      "         222       0.00      0.00      0.00         3\n",
      "         224       0.00      0.00      0.00         1\n",
      "         225       0.00      0.00      0.00         2\n",
      "         226       0.00      0.00      0.00         1\n",
      "         227       0.00      0.00      0.00         1\n",
      "         229       0.00      0.00      0.00         3\n",
      "         231       0.00      0.00      0.00         2\n",
      "         233       0.00      0.00      0.00         2\n",
      "         234       0.00      0.00      0.00         2\n",
      "         235       0.00      0.00      0.00         2\n",
      "         237       0.00      0.00      0.00         1\n",
      "         238       0.00      0.00      0.00         1\n",
      "         241       0.00      0.00      0.00         1\n",
      "         242       0.00      0.00      0.00         1\n",
      "         246       0.00      0.00      0.00         6\n",
      "         247       0.00      0.00      0.00         2\n",
      "         249       0.00      0.00      0.00         1\n",
      "         250       0.00      0.00      0.00         1\n",
      "         251       0.00      0.00      0.00         1\n",
      "         252       0.00      0.00      0.00         2\n",
      "         255       0.00      0.00      0.00         4\n",
      "         256       0.00      0.00      0.00         1\n",
      "         257       0.00      0.00      0.00         2\n",
      "         258       0.00      0.00      0.00         7\n",
      "         259       0.00      0.00      0.00         1\n",
      "         261       0.00      0.00      0.00         1\n",
      "         262       0.00      0.00      0.00         1\n",
      "         263       0.00      0.00      0.00         1\n",
      "         264       0.00      0.00      0.00         3\n",
      "         268       0.00      0.00      0.00         1\n",
      "         269       0.00      0.00      0.00         4\n",
      "         272       0.00      0.00      0.00         7\n",
      "         276       0.00      0.00      0.00         1\n",
      "         279       0.00      0.00      0.00         1\n",
      "         280       0.00      0.00      0.00         1\n",
      "         281       0.00      0.00      0.00         1\n",
      "         282       0.00      0.00      0.00         3\n",
      "         284       0.00      0.00      0.00         2\n",
      "         288       0.00      0.00      0.00         1\n",
      "         289       0.00      0.00      0.00         3\n",
      "         292       0.00      0.00      0.00         1\n",
      "         293       0.00      0.00      0.00         2\n",
      "         294       0.00      0.00      0.00         2\n",
      "         295       0.00      0.00      0.00        17\n",
      "         297       0.00      0.00      0.00         1\n",
      "         299       0.00      0.00      0.00         2\n",
      "         302       0.00      0.00      0.00         4\n",
      "         303       0.00      0.00      0.00         1\n",
      "         304       0.00      0.00      0.00         1\n",
      "         306       0.00      0.00      0.00         1\n",
      "         307       0.00      0.00      0.00         1\n",
      "         310       0.00      0.00      0.00         2\n",
      "         311       0.00      0.00      0.00         1\n",
      "         312       0.00      0.00      0.00         9\n",
      "         314       0.00      0.00      0.00         2\n",
      "         315       0.00      0.00      0.00         1\n",
      "         316       0.00      0.00      0.00         2\n",
      "         317       0.00      0.00      0.00         1\n",
      "         318       0.00      0.00      0.00         2\n",
      "         321       0.00      0.00      0.00         2\n",
      "         324       0.00      0.00      0.00         1\n",
      "         326       0.00      0.00      0.00         1\n",
      "         328       0.00      0.00      0.00         4\n",
      "         329       0.00      0.00      0.00         5\n",
      "         330       0.00      0.00      0.00         5\n",
      "         332       0.00      0.00      0.00         2\n",
      "         335       0.00      0.00      0.00         2\n",
      "         337       0.00      0.00      0.00         1\n",
      "         338       0.00      0.00      0.00         2\n",
      "         341       0.00      0.00      0.00         4\n",
      "         342       0.00      0.00      0.00         1\n",
      "         347       0.00      0.00      0.00         4\n",
      "         349       0.00      0.00      0.00         2\n",
      "         351       0.00      0.00      0.00         1\n",
      "         355       0.00      0.00      0.00         1\n",
      "         356       0.00      0.00      0.00         2\n",
      "         357       0.00      0.00      0.00         1\n",
      "         358       0.00      0.00      0.00         1\n",
      "         361       0.00      0.00      0.00         1\n",
      "         365       0.00      0.00      0.00         1\n",
      "         366       0.00      0.00      0.00         2\n",
      "         369       0.00      0.00      0.00         1\n",
      "         370       0.00      0.00      0.00         1\n",
      "         371       0.00      0.00      0.00         2\n",
      "         374       0.00      0.00      0.00         1\n",
      "         375       0.00      0.00      0.00         2\n",
      "         379       0.00      0.00      0.00         1\n",
      "         380       0.00      0.00      0.00         1\n",
      "         381       0.00      0.00      0.00         1\n",
      "         382       0.00      0.00      0.00         1\n",
      "         383       0.00      0.00      0.00         2\n",
      "         384       0.00      0.00      0.00         6\n",
      "         387       0.00      0.00      0.00         1\n",
      "         390       0.00      0.00      0.00         1\n",
      "         401       0.00      0.00      0.00         2\n",
      "         402       0.00      0.00      0.00         1\n",
      "         404       0.00      0.00      0.00         4\n",
      "         405       0.00      0.00      0.00         1\n",
      "         407       0.00      0.00      0.00         1\n",
      "         408       0.00      0.00      0.00         2\n",
      "         409       0.00      0.00      0.00         1\n",
      "         411       0.00      0.00      0.00         1\n",
      "         416       0.00      0.00      0.00         1\n",
      "         418       0.00      0.00      0.00         1\n",
      "         419       0.00      0.00      0.00         3\n",
      "         426       0.00      0.00      0.00         1\n",
      "         428       0.00      0.00      0.00         2\n",
      "         430       0.00      0.00      0.00         3\n",
      "         433       0.00      0.00      0.00         1\n",
      "         434       0.00      0.00      0.00         1\n",
      "         436       0.00      0.00      0.00         1\n",
      "         437       0.00      0.00      0.00         1\n",
      "         438       0.00      0.00      0.00         3\n",
      "         439       0.00      0.00      0.00         1\n",
      "         440       0.00      0.00      0.00         1\n",
      "         442       0.00      0.00      0.00         1\n",
      "         444       0.00      0.00      0.00         1\n",
      "         445       0.00      0.00      0.00         1\n",
      "         446       0.00      0.00      0.00         1\n",
      "         453       0.00      0.00      0.00         1\n",
      "         454       0.00      0.00      0.00         7\n",
      "         455       0.00      0.00      0.00         1\n",
      "         457       0.00      0.00      0.00         2\n",
      "         458       0.00      0.00      0.00         2\n",
      "         459       0.00      0.00      0.00         1\n",
      "         467       0.00      0.00      0.00         2\n",
      "         473       0.00      0.00      0.00         1\n",
      "         479       0.00      0.00      0.00         1\n",
      "         487       0.00      0.00      0.00         1\n",
      "         492       0.00      0.00      0.00         1\n",
      "         497       0.00      0.00      0.00         1\n",
      "         498       0.00      0.00      0.00         1\n",
      "         501       0.00      0.00      0.00         1\n",
      "         502       0.00      0.00      0.00         1\n",
      "         504       0.00      0.00      0.00         2\n",
      "         505       0.00      0.00      0.00         1\n",
      "         507       0.00      0.00      0.00         1\n",
      "         511       0.00      0.00      0.00         1\n",
      "         517       0.00      0.00      0.00         1\n",
      "         521       0.00      0.00      0.00         1\n",
      "         522       0.00      0.00      0.00         2\n",
      "         526       0.00      0.00      0.00         1\n",
      "         527       0.00      0.00      0.00         1\n",
      "         532       0.00      0.00      0.00         1\n",
      "         538       0.00      0.00      0.00         1\n",
      "         539       0.00      0.00      0.00         1\n",
      "         542       0.00      0.00      0.00         1\n",
      "         543       0.00      0.00      0.00         1\n",
      "         555       0.00      0.00      0.00         1\n",
      "         557       0.00      0.00      0.00         2\n",
      "         562       0.00      0.00      0.00         1\n",
      "         572       0.00      0.00      0.00         1\n",
      "         573       0.00      0.00      0.00         1\n",
      "         576       0.00      0.00      0.00         1\n",
      "         577       0.00      0.00      0.00         1\n",
      "         579       0.00      0.00      0.00         1\n",
      "         586       0.00      0.00      0.00         2\n",
      "         588       0.00      0.00      0.00         1\n",
      "         589       0.00      0.00      0.00         1\n",
      "         590       0.00      0.00      0.00         1\n",
      "         596       0.00      0.00      0.00         1\n",
      "         597       0.00      0.00      0.00         1\n",
      "         602       0.00      0.00      0.00         1\n",
      "         606       0.00      0.00      0.00         1\n",
      "         614       0.00      0.00      0.00         1\n",
      "         620       0.00      0.00      0.00         2\n",
      "         626       0.00      0.00      0.00         1\n",
      "         629       0.00      0.00      0.00         1\n",
      "         631       0.00      0.00      0.00         1\n",
      "         633       0.00      0.00      0.00         1\n",
      "         634       0.00      0.00      0.00         1\n",
      "         637       0.00      0.00      0.00         1\n",
      "         638       0.00      0.00      0.00         1\n",
      "         643       0.00      0.00      0.00         1\n",
      "         646       0.00      0.00      0.00         1\n",
      "         648       0.00      0.00      0.00         1\n",
      "         650       0.00      0.00      0.00         1\n",
      "         651       0.00      0.00      0.00         2\n",
      "         655       0.00      0.00      0.00         2\n",
      "         656       0.00      0.00      0.00         1\n",
      "         658       0.00      0.00      0.00         1\n",
      "         660       0.00      0.00      0.00         1\n",
      "         663       0.00      0.00      0.00         1\n",
      "         664       0.00      0.00      0.00         1\n",
      "         668       0.00      0.00      0.00         1\n",
      "         677       0.00      0.00      0.00         1\n",
      "         687       0.00      0.00      0.00         1\n",
      "         689       0.00      0.00      0.00         2\n",
      "         699       0.00      0.00      0.00         1\n",
      "         700       0.00      0.00      0.00         1\n",
      "         710       0.00      0.00      0.00         1\n",
      "         714       0.00      0.00      0.00         2\n",
      "         715       0.00      0.00      0.00         1\n",
      "         717       0.00      0.00      0.00         1\n",
      "         719       0.00      0.00      0.00         1\n",
      "         727       0.00      0.00      0.00         2\n",
      "         731       0.00      0.00      0.00         1\n",
      "         736       0.00      0.00      0.00         1\n",
      "         743       0.00      0.00      0.00         1\n",
      "         747       0.00      0.00      0.00         3\n",
      "         750       0.00      0.00      0.00         1\n",
      "         751       0.00      0.00      0.00         1\n",
      "         752       0.00      0.00      0.00         2\n",
      "         754       0.00      0.00      0.00         1\n",
      "         756       0.00      0.00      0.00         1\n",
      "         760       0.00      0.00      0.00         1\n",
      "         762       0.00      0.00      0.00         1\n",
      "         767       0.00      0.00      0.00         2\n",
      "         768       0.00      0.00      0.00         1\n",
      "         770       0.00      0.00      0.00         2\n",
      "         771       0.00      0.00      0.00         1\n",
      "         779       0.00      0.00      0.00         1\n",
      "         783       0.00      0.00      0.00         2\n",
      "         786       0.00      0.00      0.00         2\n",
      "         787       0.00      0.00      0.00         1\n",
      "         789       0.00      0.00      0.00         1\n",
      "         790       0.00      0.00      0.00         1\n",
      "         791       0.00      0.00      0.00         3\n",
      "         793       0.00      0.00      0.00         1\n",
      "         794       0.00      0.00      0.00         1\n",
      "         796       0.00      0.00      0.00         1\n",
      "         802       0.00      0.00      0.00         2\n",
      "         804       0.00      0.00      0.00         1\n",
      "         806       0.00      0.00      0.00         2\n",
      "         807       0.00      0.00      0.00         1\n",
      "         814       0.00      0.00      0.00         1\n",
      "         818       0.00      0.00      0.00         4\n",
      "         823       0.00      0.00      0.00         1\n",
      "         824       0.00      0.00      0.00         2\n",
      "         833       0.00      0.00      0.00         1\n",
      "         834       0.00      0.00      0.00         2\n",
      "         835       0.00      0.00      0.00         1\n",
      "         836       0.00      0.00      0.00         2\n",
      "         837       0.00      0.00      0.00         1\n",
      "         839       0.00      0.00      0.00         1\n",
      "         842       0.00      0.00      0.00         1\n",
      "         848       0.00      0.00      0.00         1\n",
      "         849       0.00      0.00      0.00         2\n",
      "         850       0.00      0.00      0.00         1\n",
      "         854       0.00      0.00      0.00         1\n",
      "         857       0.00      0.00      0.00         1\n",
      "         860       0.00      0.00      0.00         1\n",
      "         863       0.00      0.00      0.00         1\n",
      "         864       0.00      0.00      0.00         1\n",
      "         866       0.00      0.00      0.00         1\n",
      "         869       0.00      0.00      0.00         1\n",
      "         874       0.00      0.00      0.00         1\n",
      "         876       0.00      0.00      0.00         1\n",
      "         882       0.00      0.00      0.00         1\n",
      "         885       0.00      0.00      0.00         1\n",
      "         893       0.00      0.00      0.00         1\n",
      "         895       0.00      0.00      0.00         1\n",
      "         897       0.00      0.00      0.00         1\n",
      "         902       0.00      0.00      0.00         2\n",
      "         903       0.00      0.00      0.00         1\n",
      "         908       0.00      0.00      0.00         1\n",
      "         913       0.00      0.00      0.00         1\n",
      "         915       0.00      0.00      0.00         1\n",
      "         916       0.00      0.00      0.00         1\n",
      "         924       0.00      0.00      0.00         1\n",
      "         925       0.00      0.00      0.00         1\n",
      "         939       0.00      0.00      0.00         1\n",
      "         940       0.00      0.00      0.00         1\n",
      "         942       0.00      0.00      0.00         1\n",
      "         944       0.00      0.00      0.00         1\n",
      "         945       0.00      0.00      0.00         1\n",
      "         946       0.00      0.00      0.00         2\n",
      "         951       0.00      0.00      0.00         1\n",
      "         956       0.00      0.00      0.00         1\n",
      "         959       0.00      0.00      0.00         1\n",
      "         961       0.00      0.00      0.00         1\n",
      "         962       0.00      0.00      0.00         1\n",
      "         968       0.00      0.00      0.00         1\n",
      "         969       0.00      0.00      0.00         1\n",
      "         970       0.00      0.00      0.00         1\n",
      "         975       0.00      0.00      0.00         1\n",
      "         981       0.00      0.00      0.00         1\n",
      "         994       0.00      0.00      0.00         1\n",
      "         996       0.00      0.00      0.00         1\n",
      "         998       0.00      0.00      0.00         1\n",
      "        1000       0.00      0.00      0.00         1\n",
      "        1003       0.00      0.00      0.00         1\n",
      "        1007       0.00      0.00      0.00         1\n",
      "        1019       0.00      0.00      0.00         1\n",
      "        1022       0.00      0.00      0.00         1\n",
      "        1035       0.00      0.00      0.00         1\n",
      "        1036       0.00      0.00      0.00         1\n",
      "        1038       0.00      0.00      0.00         1\n",
      "        1041       0.00      0.00      0.00         1\n",
      "        1043       0.00      0.00      0.00         2\n",
      "        1050       0.00      0.00      0.00         1\n",
      "        1053       0.00      0.00      0.00         1\n",
      "        1064       0.00      0.00      0.00         1\n",
      "        1094       0.00      0.00      0.00         1\n",
      "        1097       0.00      0.00      0.00         2\n",
      "        1100       0.00      0.00      0.00         1\n",
      "        1102       0.00      0.00      0.00         2\n",
      "        1109       0.00      0.00      0.00         1\n",
      "        1114       0.00      0.00      0.00         1\n",
      "        1123       0.00      0.00      0.00         1\n",
      "        1126       0.00      0.00      0.00         1\n",
      "        1130       0.00      0.00      0.00         1\n",
      "        1139       0.00      0.00      0.00         1\n",
      "        1146       0.00      0.00      0.00         1\n",
      "        1147       0.00      0.00      0.00         1\n",
      "        1148       0.00      0.00      0.00         1\n",
      "        1150       0.00      0.00      0.00         1\n",
      "        1151       0.00      0.00      0.00         1\n",
      "        1153       0.00      0.00      0.00         1\n",
      "        1156       0.00      0.00      0.00         1\n",
      "        1161       0.00      0.00      0.00         1\n",
      "        1163       0.00      0.00      0.00         1\n",
      "        1164       0.00      0.00      0.00         1\n",
      "        1167       0.00      0.00      0.00         1\n",
      "        1169       0.00      0.00      0.00         1\n",
      "        1170       0.00      0.00      0.00         2\n",
      "        1171       0.00      0.00      0.00         1\n",
      "        1173       0.00      0.00      0.00         1\n",
      "        1187       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.01      1806\n",
      "   macro avg       0.00      0.00      0.00      1806\n",
      "weighted avg       0.00      0.01      0.00      1806\n",
      "\n",
      "spearman's rho (correlation)           true      pred\n",
      "true  1.000000 -0.009211\n",
      "pred -0.009211  1.000000\n",
      "R^2 score: -0.66\n",
      "mean absolute error score 208.8\n",
      "*** validation performance: ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           3       0.00      0.00      0.00       1.0\n",
      "           8       0.00      0.00      0.00       1.0\n",
      "          13       0.00      0.00      0.00       2.0\n",
      "          14       0.00      0.00      0.00       1.0\n",
      "          15       0.00      0.00      0.00       1.0\n",
      "          21       0.00      0.00      0.00       1.0\n",
      "          26       0.00      0.00      0.00       1.0\n",
      "          36       0.00      0.00      0.00       1.0\n",
      "          38       0.00      0.00      0.00       1.0\n",
      "          39       0.00      0.00      0.00       1.0\n",
      "          42       0.00      0.00      0.00       2.0\n",
      "          45       0.00      0.00      0.00       2.0\n",
      "          47       0.00      0.00      0.00       1.0\n",
      "          50       0.00      0.00      0.00       1.0\n",
      "          51       0.00      0.00      0.00       4.0\n",
      "          52       0.00      0.00      0.00       1.0\n",
      "          53       0.00      0.00      0.00       1.0\n",
      "          56       0.00      0.00      0.00       1.0\n",
      "          57       0.00      0.00      0.00       1.0\n",
      "          58       0.00      0.00      0.00       2.0\n",
      "          60       0.00      0.00      0.00       1.0\n",
      "          64       0.00      0.00      0.00       2.0\n",
      "          71       0.00      0.00      0.00       2.0\n",
      "          72       0.00      0.00      0.00       1.0\n",
      "          73       0.00      0.00      0.00       5.0\n",
      "          77       0.00      0.00      0.00       1.0\n",
      "          78       0.00      0.00      0.00       1.0\n",
      "          81       0.00      0.00      0.00       1.0\n",
      "          84       0.00      0.00      0.00       1.0\n",
      "          88       0.00      0.00      0.00       2.0\n",
      "          91       0.00      0.00      0.00       2.0\n",
      "          93       0.00      0.00      0.00       1.0\n",
      "          94       0.00      0.00      0.00       1.0\n",
      "          97       0.00      0.00      0.00       1.0\n",
      "          99       0.00      0.00      0.00       1.0\n",
      "         100       0.00      0.00      0.00       1.0\n",
      "         102       0.00      0.00      0.00       1.0\n",
      "         110       0.00      0.00      0.00       1.0\n",
      "         113       0.00      0.00      0.00       1.0\n",
      "         114       0.00      0.00      0.00       1.0\n",
      "         125       0.00      0.00      0.00       1.0\n",
      "         133       0.00      0.00      0.00       1.0\n",
      "         137       0.00      0.00      0.00       1.0\n",
      "         153       0.00      0.00      0.00       1.0\n",
      "         156       0.00      0.00      0.00       1.0\n",
      "         169       0.00      0.00      0.00       1.0\n",
      "         173       0.00      0.00      0.00       1.0\n",
      "         176       0.00      0.00      0.00       1.0\n",
      "         179       0.00      0.00      0.00       1.0\n",
      "         188       0.00      0.00      0.00       1.0\n",
      "         198       0.00      0.00      0.00       1.0\n",
      "         202       0.00      0.00      0.00       1.0\n",
      "         229       0.00      0.00      0.00       1.0\n",
      "         237       0.00      0.00      0.00       1.0\n",
      "         248       0.00      0.00      0.00       1.0\n",
      "         252       0.00      0.00      0.00       3.0\n",
      "         255       0.00      0.00      0.00       1.0\n",
      "         263       0.00      0.00      0.00       1.0\n",
      "         272       0.00      0.00      0.00       1.0\n",
      "         286       0.00      0.00      0.00       1.0\n",
      "         289       0.00      0.00      0.00       1.0\n",
      "         291       0.00      0.00      0.00       1.0\n",
      "         295       0.00      0.00      0.00       2.0\n",
      "         297       0.00      0.00      0.00       1.0\n",
      "         299       0.00      0.00      0.00       1.0\n",
      "         303       0.00      0.00      0.00       1.0\n",
      "         308       0.00      0.00      0.00       1.0\n",
      "         311       0.00      0.00      0.00       1.0\n",
      "         317       0.00      0.00      0.00       1.0\n",
      "         321       0.00      0.00      0.00       1.0\n",
      "         332       0.00      0.00      0.00       1.0\n",
      "         339       0.00      0.00      0.00       1.0\n",
      "         368       0.00      0.00      0.00       1.0\n",
      "         391       0.00      0.00      0.00       1.0\n",
      "         402       0.00      0.00      0.00       1.0\n",
      "         418       0.00      0.00      0.00       1.0\n",
      "         419       0.00      0.00      0.00       2.0\n",
      "         423       0.00      0.00      0.00       1.0\n",
      "         428       0.00      0.00      0.00       1.0\n",
      "         442       0.00      0.00      0.00       2.0\n",
      "         445       0.00      0.00      0.00       1.0\n",
      "         449       0.00      0.00      0.00       1.0\n",
      "         453       0.00      0.00      0.00       1.0\n",
      "         454       0.00      0.00      0.00       2.0\n",
      "\n",
      "    accuracy                           0.00     105.0\n",
      "   macro avg       0.00      0.00      0.00     105.0\n",
      "weighted avg       0.00      0.00      0.00     105.0\n",
      "\n",
      "spearman's rho (correlation)           true      pred\n",
      "true  1.000000  0.059465\n",
      "pred  0.059465  1.000000\n",
      "R^2 score: -1.6\n",
      "mean absolute error score 173.7\n",
      "*** Test-set performance: ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           3       0.00      0.00      0.00       1.0\n",
      "           9       0.00      0.00      0.00       1.0\n",
      "          13       0.00      0.00      0.00       3.0\n",
      "          14       0.00      0.00      0.00       2.0\n",
      "          22       0.00      0.00      0.00       1.0\n",
      "          26       0.00      0.00      0.00       3.0\n",
      "          36       0.00      0.00      0.00       5.0\n",
      "          42       0.00      0.00      0.00       1.0\n",
      "          47       0.00      0.00      0.00       2.0\n",
      "          51       0.00      0.00      0.00       5.0\n",
      "          52       0.00      0.00      0.00       1.0\n",
      "          53       0.00      0.00      0.00       2.0\n",
      "          63       0.00      0.00      0.00       2.0\n",
      "          64       0.00      0.00      0.00       1.0\n",
      "          65       0.00      0.00      0.00       2.0\n",
      "          71       0.00      0.00      0.00       2.0\n",
      "          72       0.00      0.00      0.00       4.0\n",
      "          73       0.00      0.00      0.00       1.0\n",
      "          74       0.00      0.00      0.00       2.0\n",
      "          78       0.00      0.00      0.00       1.0\n",
      "          80       0.00      0.00      0.00       5.0\n",
      "          89       0.00      0.00      0.00       1.0\n",
      "          90       0.00      0.00      0.00       1.0\n",
      "          95       0.00      0.00      0.00       1.0\n",
      "          99       0.00      0.00      0.00       1.0\n",
      "         102       0.00      0.00      0.00       1.0\n",
      "         119       0.00      0.00      0.00       1.0\n",
      "         126       0.00      0.00      0.00       3.0\n",
      "         137       0.00      0.00      0.00       2.0\n",
      "         140       0.00      0.00      0.00       1.0\n",
      "         151       0.00      0.00      0.00       1.0\n",
      "         153       0.00      0.00      0.00       7.0\n",
      "         156       0.00      0.00      0.00       1.0\n",
      "         157       0.00      0.00      0.00       1.0\n",
      "         176       0.00      0.00      0.00       1.0\n",
      "         180       0.00      0.00      0.00       1.0\n",
      "         218       0.00      0.00      0.00       2.0\n",
      "         229       0.00      0.00      0.00       1.0\n",
      "         248       0.00      0.00      0.00       1.0\n",
      "         252       0.00      0.00      0.00       1.0\n",
      "         255       0.00      0.00      0.00       1.0\n",
      "         274       0.00      0.00      0.00       1.0\n",
      "         289       0.00      0.00      0.00       1.0\n",
      "         304       0.00      0.00      0.00       1.0\n",
      "         308       0.00      0.00      0.00       1.0\n",
      "         312       0.00      0.00      0.00       2.0\n",
      "         321       0.00      0.00      0.00       1.0\n",
      "         380       0.00      0.00      0.00       1.0\n",
      "         382       0.00      0.00      0.00       1.0\n",
      "         402       0.00      0.00      0.00       2.0\n",
      "         404       0.00      0.00      0.00       1.0\n",
      "         420       0.00      0.00      0.00       1.0\n",
      "         421       0.00      0.00      0.00       1.0\n",
      "         424       0.00      0.00      0.00       1.0\n",
      "         425       0.00      0.00      0.00       2.0\n",
      "         429       0.00      0.00      0.00       3.0\n",
      "         430       0.00      0.00      0.00       1.0\n",
      "         435       0.00      0.00      0.00       1.0\n",
      "         442       0.00      0.00      0.00       1.0\n",
      "         444       0.00      0.00      0.00       1.0\n",
      "         447       0.00      0.00      0.00       1.0\n",
      "         450       0.00      0.00      0.00       1.0\n",
      "         453       0.00      0.00      0.00       1.0\n",
      "         454       0.00      0.00      0.00       3.0\n",
      "\n",
      "    accuracy                           0.00     107.0\n",
      "   macro avg       0.00      0.00      0.00     107.0\n",
      "weighted avg       0.00      0.00      0.00     107.0\n",
      "\n",
      "spearman's rho (correlation)           true      pred\n",
      "true  1.000000 -0.190677\n",
      "pred -0.190677  1.000000\n",
      "R^2 score: -1.4\n",
      "mean absolute error score 176.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Will now truncate too-long sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_7/kernel:0', 'auto-encoder_7/bias:0', 'auto-encoder_7/kernel:0', 'auto-encoder_7/bias:0', 'auto-encoder_7/gamma:0', 'auto-encoder_7/beta:0', 'auto-encoder_7/kernel:0', 'auto-encoder_7/bias:0', 'auto-encoder_7/gamma:0', 'auto-encoder_7/beta:0', 'auto-encoder_7/kernel:0', 'auto-encoder_7/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_7/kernel:0', 'auto-encoder_7/bias:0', 'auto-encoder_7/kernel:0', 'auto-encoder_7/bias:0', 'auto-encoder_7/gamma:0', 'auto-encoder_7/beta:0', 'auto-encoder_7/kernel:0', 'auto-encoder_7/bias:0', 'auto-encoder_7/gamma:0', 'auto-encoder_7/beta:0', 'auto-encoder_7/kernel:0', 'auto-encoder_7/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_7/kernel:0', 'auto-encoder_7/bias:0', 'auto-encoder_7/kernel:0', 'auto-encoder_7/bias:0', 'auto-encoder_7/gamma:0', 'auto-encoder_7/beta:0', 'auto-encoder_7/kernel:0', 'auto-encoder_7/bias:0', 'auto-encoder_7/gamma:0', 'auto-encoder_7/beta:0', 'auto-encoder_7/kernel:0', 'auto-encoder_7/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['auto-encoder_7/kernel:0', 'auto-encoder_7/bias:0', 'auto-encoder_7/kernel:0', 'auto-encoder_7/bias:0', 'auto-encoder_7/gamma:0', 'auto-encoder_7/beta:0', 'auto-encoder_7/kernel:0', 'auto-encoder_7/bias:0', 'auto-encoder_7/gamma:0', 'auto-encoder_7/beta:0', 'auto-encoder_7/kernel:0', 'auto-encoder_7/bias:0'] when minimizing the loss.\n",
      "116/116 [==============================] - 28s 241ms/step - loss: 113288.1094 - val_loss: 48248.2305 - lr: 2.0000e-04\n",
      "Epoch 2/2\n",
      "116/116 [==============================] - 26s 224ms/step - loss: 113245.6250 - val_loss: 48248.1328 - lr: 2.0000e-04\n",
      "\n",
      "*** Training-set performance: ***\n",
      "classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      1.00      0.01        11\n",
      "           1       0.00      0.00      0.00         5\n",
      "           3       0.00      0.00      0.00        12\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00         3\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00         2\n",
      "           9       0.00      0.00      0.00         6\n",
      "          12       0.00      0.00      0.00         4\n",
      "          13       0.00      0.00      0.00         9\n",
      "          14       0.00      0.00      0.00        10\n",
      "          15       0.00      0.00      0.00         2\n",
      "          16       0.00      0.00      0.00         1\n",
      "          17       0.00      0.00      0.00         8\n",
      "          18       0.00      0.00      0.00        11\n",
      "          19       0.00      0.00      0.00        11\n",
      "          20       0.00      0.00      0.00         2\n",
      "          21       0.00      0.00      0.00        22\n",
      "          22       0.00      0.00      0.00        52\n",
      "          23       0.00      0.00      0.00         1\n",
      "          24       0.00      0.00      0.00        10\n",
      "          25       0.00      0.00      0.00         6\n",
      "          26       0.00      0.00      0.00        11\n",
      "          27       0.00      0.00      0.00         2\n",
      "          28       0.00      0.00      0.00         5\n",
      "          31       0.00      0.00      0.00         2\n",
      "          32       0.00      0.00      0.00         1\n",
      "          33       0.00      0.00      0.00         2\n",
      "          34       0.00      0.00      0.00         1\n",
      "          35       0.00      0.00      0.00         2\n",
      "          36       0.00      0.00      0.00       138\n",
      "          37       0.00      0.00      0.00         1\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00        13\n",
      "          40       0.00      0.00      0.00         1\n",
      "          41       0.00      0.00      0.00         1\n",
      "          42       0.00      0.00      0.00         8\n",
      "          43       0.00      0.00      0.00         4\n",
      "          44       0.00      0.00      0.00         2\n",
      "          45       0.00      0.00      0.00        17\n",
      "          46       0.00      0.00      0.00         2\n",
      "          47       0.00      0.00      0.00        32\n",
      "          48       0.00      0.00      0.00         4\n",
      "          49       0.00      0.00      0.00         7\n",
      "          50       0.00      0.00      0.00         2\n",
      "          51       0.00      0.00      0.00        31\n",
      "          52       0.00      0.00      0.00         6\n",
      "          53       0.00      0.00      0.00         4\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.00      0.00      0.00         4\n",
      "          56       0.00      0.00      0.00        10\n",
      "          57       0.00      0.00      0.00         6\n",
      "          58       0.00      0.00      0.00         2\n",
      "          59       0.00      0.00      0.00         7\n",
      "          60       0.00      0.00      0.00        18\n",
      "          61       0.00      0.00      0.00         6\n",
      "          63       0.00      0.00      0.00         2\n",
      "          64       0.00      0.00      0.00         3\n",
      "          65       0.00      0.00      0.00         2\n",
      "          66       0.00      0.00      0.00         7\n",
      "          67       0.00      0.00      0.00         2\n",
      "          68       0.00      0.00      0.00         4\n",
      "          69       0.00      0.00      0.00         1\n",
      "          70       0.00      0.00      0.00        16\n",
      "          71       0.00      0.00      0.00         2\n",
      "          72       0.00      0.00      0.00         4\n",
      "          73       0.00      0.00      0.00        67\n",
      "          74       0.00      0.00      0.00         2\n",
      "          75       0.00      0.00      0.00         8\n",
      "          76       0.00      0.00      0.00         2\n",
      "          77       0.00      0.00      0.00        34\n",
      "          78       0.00      0.00      0.00        20\n",
      "          79       0.00      0.00      0.00         2\n",
      "          80       0.00      0.00      0.00         4\n",
      "          81       0.00      0.00      0.00        18\n",
      "          83       0.00      0.00      0.00         3\n",
      "          84       0.00      0.00      0.00        44\n",
      "          85       0.00      0.00      0.00         2\n",
      "          86       0.00      0.00      0.00         2\n",
      "          87       0.00      0.00      0.00         3\n",
      "          88       0.00      0.00      0.00        24\n",
      "          89       0.00      0.00      0.00         6\n",
      "          90       0.00      0.00      0.00        20\n",
      "          91       0.00      0.00      0.00         3\n",
      "          92       0.00      0.00      0.00         4\n",
      "          93       0.00      0.00      0.00         2\n",
      "          94       0.00      0.00      0.00         2\n",
      "          95       0.00      0.00      0.00        12\n",
      "          96       0.00      0.00      0.00         2\n",
      "          97       0.00      0.00      0.00         9\n",
      "          98       0.00      0.00      0.00         1\n",
      "          99       0.00      0.00      0.00         1\n",
      "         100       0.00      0.00      0.00         2\n",
      "         101       0.00      0.00      0.00         2\n",
      "         102       0.00      0.00      0.00         3\n",
      "         103       0.00      0.00      0.00         4\n",
      "         104       0.00      0.00      0.00         2\n",
      "         105       0.00      0.00      0.00         1\n",
      "         106       0.00      0.00      0.00         1\n",
      "         107       0.00      0.00      0.00         2\n",
      "         108       0.00      0.00      0.00         5\n",
      "         109       0.00      0.00      0.00         6\n",
      "         110       0.00      0.00      0.00         3\n",
      "         112       0.00      0.00      0.00         3\n",
      "         113       0.00      0.00      0.00        10\n",
      "         114       0.00      0.00      0.00         3\n",
      "         115       0.00      0.00      0.00         4\n",
      "         117       0.00      0.00      0.00         1\n",
      "         118       0.00      0.00      0.00         4\n",
      "         119       0.00      0.00      0.00         9\n",
      "         120       0.00      0.00      0.00         1\n",
      "         121       0.00      0.00      0.00         1\n",
      "         122       0.00      0.00      0.00         4\n",
      "         124       0.00      0.00      0.00         8\n",
      "         125       0.00      0.00      0.00         1\n",
      "         126       0.00      0.00      0.00        27\n",
      "         127       0.00      0.00      0.00         7\n",
      "         128       0.00      0.00      0.00         2\n",
      "         129       0.00      0.00      0.00         5\n",
      "         130       0.00      0.00      0.00         5\n",
      "         131       0.00      0.00      0.00         2\n",
      "         132       0.00      0.00      0.00         9\n",
      "         133       0.00      0.00      0.00         9\n",
      "         134       0.00      0.00      0.00         1\n",
      "         135       0.00      0.00      0.00        12\n",
      "         137       0.00      0.00      0.00         2\n",
      "         138       0.00      0.00      0.00         4\n",
      "         139       0.00      0.00      0.00         8\n",
      "         140       0.00      0.00      0.00         6\n",
      "         141       0.00      0.00      0.00         2\n",
      "         142       0.00      0.00      0.00         2\n",
      "         143       0.00      0.00      0.00         8\n",
      "         144       0.00      0.00      0.00         4\n",
      "         145       0.00      0.00      0.00         3\n",
      "         146       0.00      0.00      0.00         5\n",
      "         147       0.00      0.00      0.00         4\n",
      "         148       0.00      0.00      0.00         3\n",
      "         149       0.00      0.00      0.00         2\n",
      "         150       0.00      0.00      0.00         3\n",
      "         151       0.00      0.00      0.00         3\n",
      "         152       0.00      0.00      0.00         4\n",
      "         153       0.00      0.00      0.00        48\n",
      "         154       0.00      0.00      0.00         2\n",
      "         155       0.00      0.00      0.00         2\n",
      "         156       0.00      0.00      0.00         4\n",
      "         157       0.00      0.00      0.00         3\n",
      "         158       0.00      0.00      0.00         2\n",
      "         159       0.00      0.00      0.00         6\n",
      "         161       0.00      0.00      0.00         6\n",
      "         162       0.00      0.00      0.00         9\n",
      "         163       0.00      0.00      0.00         1\n",
      "         164       0.00      0.00      0.00         3\n",
      "         165       0.00      0.00      0.00         5\n",
      "         166       0.00      0.00      0.00         3\n",
      "         167       0.00      0.00      0.00         3\n",
      "         168       0.00      0.00      0.00         5\n",
      "         169       0.00      0.00      0.00         1\n",
      "         171       0.00      0.00      0.00         6\n",
      "         174       0.00      0.00      0.00         2\n",
      "         175       0.00      0.00      0.00         2\n",
      "         176       0.00      0.00      0.00        41\n",
      "         177       0.00      0.00      0.00         1\n",
      "         178       0.00      0.00      0.00        12\n",
      "         179       0.00      0.00      0.00        11\n",
      "         180       0.00      0.00      0.00         8\n",
      "         181       0.00      0.00      0.00         2\n",
      "         182       0.00      0.00      0.00         4\n",
      "         183       0.00      0.00      0.00         1\n",
      "         184       0.00      0.00      0.00         1\n",
      "         186       0.00      0.00      0.00         1\n",
      "         187       0.00      0.00      0.00         5\n",
      "         189       0.00      0.00      0.00         1\n",
      "         195       0.00      0.00      0.00         5\n",
      "         196       0.00      0.00      0.00         1\n",
      "         197       0.00      0.00      0.00         1\n",
      "         198       0.00      0.00      0.00         1\n",
      "         199       0.00      0.00      0.00         1\n",
      "         201       0.00      0.00      0.00         2\n",
      "         202       0.00      0.00      0.00         1\n",
      "         203       0.00      0.00      0.00         1\n",
      "         204       0.00      0.00      0.00         1\n",
      "         208       0.00      0.00      0.00         2\n",
      "         209       0.00      0.00      0.00         3\n",
      "         210       0.00      0.00      0.00         2\n",
      "         213       0.00      0.00      0.00         1\n",
      "         218       0.00      0.00      0.00         3\n",
      "         222       0.00      0.00      0.00         3\n",
      "         224       0.00      0.00      0.00         1\n",
      "         225       0.00      0.00      0.00         2\n",
      "         226       0.00      0.00      0.00         1\n",
      "         227       0.00      0.00      0.00         1\n",
      "         229       0.00      0.00      0.00         3\n",
      "         231       0.00      0.00      0.00         2\n",
      "         233       0.00      0.00      0.00         2\n",
      "         234       0.00      0.00      0.00         2\n",
      "         235       0.00      0.00      0.00         2\n",
      "         237       0.00      0.00      0.00         1\n",
      "         238       0.00      0.00      0.00         1\n",
      "         241       0.00      0.00      0.00         1\n",
      "         242       0.00      0.00      0.00         1\n",
      "         246       0.00      0.00      0.00         6\n",
      "         247       0.00      0.00      0.00         2\n",
      "         249       0.00      0.00      0.00         1\n",
      "         250       0.00      0.00      0.00         1\n",
      "         251       0.00      0.00      0.00         1\n",
      "         252       0.00      0.00      0.00         2\n",
      "         255       0.00      0.00      0.00         4\n",
      "         256       0.00      0.00      0.00         1\n",
      "         257       0.00      0.00      0.00         2\n",
      "         258       0.00      0.00      0.00         7\n",
      "         259       0.00      0.00      0.00         1\n",
      "         261       0.00      0.00      0.00         1\n",
      "         262       0.00      0.00      0.00         1\n",
      "         263       0.00      0.00      0.00         1\n",
      "         264       0.00      0.00      0.00         3\n",
      "         268       0.00      0.00      0.00         1\n",
      "         269       0.00      0.00      0.00         4\n",
      "         272       0.00      0.00      0.00         7\n",
      "         276       0.00      0.00      0.00         1\n",
      "         279       0.00      0.00      0.00         1\n",
      "         280       0.00      0.00      0.00         1\n",
      "         281       0.00      0.00      0.00         1\n",
      "         282       0.00      0.00      0.00         3\n",
      "         284       0.00      0.00      0.00         2\n",
      "         288       0.00      0.00      0.00         1\n",
      "         289       0.00      0.00      0.00         3\n",
      "         292       0.00      0.00      0.00         1\n",
      "         293       0.00      0.00      0.00         2\n",
      "         294       0.00      0.00      0.00         2\n",
      "         295       0.00      0.00      0.00        17\n",
      "         296       0.00      0.00      0.00         3\n",
      "         297       0.00      0.00      0.00         1\n",
      "         298       0.00      0.00      0.00         1\n",
      "         299       0.00      0.00      0.00         2\n",
      "         302       0.00      0.00      0.00         5\n",
      "         303       0.00      0.00      0.00         1\n",
      "         304       0.00      0.00      0.00         3\n",
      "         306       0.00      0.00      0.00         1\n",
      "         307       0.00      0.00      0.00         1\n",
      "         310       0.00      0.00      0.00         2\n",
      "         311       0.00      0.00      0.00         1\n",
      "         312       0.00      0.00      0.00         9\n",
      "         314       0.00      0.00      0.00         2\n",
      "         315       0.00      0.00      0.00         1\n",
      "         316       0.00      0.00      0.00         2\n",
      "         317       0.00      0.00      0.00         1\n",
      "         318       0.00      0.00      0.00         2\n",
      "         321       0.00      0.00      0.00         2\n",
      "         324       0.00      0.00      0.00         1\n",
      "         326       0.00      0.00      0.00         1\n",
      "         328       0.00      0.00      0.00         4\n",
      "         329       0.00      0.00      0.00         5\n",
      "         330       0.00      0.00      0.00         5\n",
      "         332       0.00      0.00      0.00         2\n",
      "         335       0.00      0.00      0.00         2\n",
      "         337       0.00      0.00      0.00         1\n",
      "         338       0.00      0.00      0.00         2\n",
      "         341       0.00      0.00      0.00         4\n",
      "         342       0.00      0.00      0.00         1\n",
      "         347       0.00      0.00      0.00         4\n",
      "         349       0.00      0.00      0.00         2\n",
      "         351       0.00      0.00      0.00         1\n",
      "         355       0.00      0.00      0.00         1\n",
      "         356       0.00      0.00      0.00         2\n",
      "         357       0.00      0.00      0.00         1\n",
      "         358       0.00      0.00      0.00         1\n",
      "         361       0.00      0.00      0.00         1\n",
      "         365       0.00      0.00      0.00         1\n",
      "         366       0.00      0.00      0.00         2\n",
      "         369       0.00      0.00      0.00         1\n",
      "         370       0.00      0.00      0.00         1\n",
      "         371       0.00      0.00      0.00         2\n",
      "         374       0.00      0.00      0.00         1\n",
      "         375       0.00      0.00      0.00         2\n",
      "         379       0.00      0.00      0.00         1\n",
      "         380       0.00      0.00      0.00         1\n",
      "         381       0.00      0.00      0.00         1\n",
      "         382       0.00      0.00      0.00         1\n",
      "         383       0.00      0.00      0.00         2\n",
      "         384       0.00      0.00      0.00         6\n",
      "         387       0.00      0.00      0.00         1\n",
      "         390       0.00      0.00      0.00         1\n",
      "         392       0.00      0.00      0.00         1\n",
      "         394       0.00      0.00      0.00         1\n",
      "         396       0.00      0.00      0.00         2\n",
      "         401       0.00      0.00      0.00         2\n",
      "         402       0.00      0.00      0.00         1\n",
      "         404       0.00      0.00      0.00         5\n",
      "         405       0.00      0.00      0.00         1\n",
      "         407       0.00      0.00      0.00         1\n",
      "         408       0.00      0.00      0.00         2\n",
      "         409       0.00      0.00      0.00         1\n",
      "         411       0.00      0.00      0.00         1\n",
      "         416       0.00      0.00      0.00         1\n",
      "         418       0.00      0.00      0.00         1\n",
      "         419       0.00      0.00      0.00         3\n",
      "         426       0.00      0.00      0.00         1\n",
      "         428       0.00      0.00      0.00         2\n",
      "         430       0.00      0.00      0.00         3\n",
      "         433       0.00      0.00      0.00         1\n",
      "         434       0.00      0.00      0.00         1\n",
      "         435       0.00      0.00      0.00         2\n",
      "         436       0.00      0.00      0.00         1\n",
      "         437       0.00      0.00      0.00         1\n",
      "         438       0.00      0.00      0.00         3\n",
      "         439       0.00      0.00      0.00         1\n",
      "         440       0.00      0.00      0.00         1\n",
      "         442       0.00      0.00      0.00         1\n",
      "         444       0.00      0.00      0.00         1\n",
      "         445       0.00      0.00      0.00         1\n",
      "         446       0.00      0.00      0.00         1\n",
      "         453       0.00      0.00      0.00         1\n",
      "         454       0.00      0.00      0.00         7\n",
      "         455       0.00      0.00      0.00         1\n",
      "         457       0.00      0.00      0.00         2\n",
      "         458       0.00      0.00      0.00         2\n",
      "         459       0.00      0.00      0.00         1\n",
      "         467       0.00      0.00      0.00         2\n",
      "         473       0.00      0.00      0.00         1\n",
      "         479       0.00      0.00      0.00         1\n",
      "         487       0.00      0.00      0.00         1\n",
      "         492       0.00      0.00      0.00         1\n",
      "         497       0.00      0.00      0.00         1\n",
      "         498       0.00      0.00      0.00         1\n",
      "         501       0.00      0.00      0.00         1\n",
      "         502       0.00      0.00      0.00         1\n",
      "         504       0.00      0.00      0.00         2\n",
      "         505       0.00      0.00      0.00         1\n",
      "         507       0.00      0.00      0.00         1\n",
      "         511       0.00      0.00      0.00         1\n",
      "         517       0.00      0.00      0.00         1\n",
      "         521       0.00      0.00      0.00         1\n",
      "         522       0.00      0.00      0.00         2\n",
      "         526       0.00      0.00      0.00         1\n",
      "         527       0.00      0.00      0.00         1\n",
      "         532       0.00      0.00      0.00         1\n",
      "         538       0.00      0.00      0.00         1\n",
      "         539       0.00      0.00      0.00         1\n",
      "         542       0.00      0.00      0.00         1\n",
      "         543       0.00      0.00      0.00         1\n",
      "         555       0.00      0.00      0.00         1\n",
      "         557       0.00      0.00      0.00         2\n",
      "         562       0.00      0.00      0.00         1\n",
      "         572       0.00      0.00      0.00         1\n",
      "         573       0.00      0.00      0.00         1\n",
      "         576       0.00      0.00      0.00         1\n",
      "         577       0.00      0.00      0.00         1\n",
      "         579       0.00      0.00      0.00         1\n",
      "         586       0.00      0.00      0.00         2\n",
      "         588       0.00      0.00      0.00         1\n",
      "         589       0.00      0.00      0.00         1\n",
      "         590       0.00      0.00      0.00         1\n",
      "         596       0.00      0.00      0.00         1\n",
      "         597       0.00      0.00      0.00         1\n",
      "         602       0.00      0.00      0.00         1\n",
      "         606       0.00      0.00      0.00         1\n",
      "         614       0.00      0.00      0.00         1\n",
      "         620       0.00      0.00      0.00         2\n",
      "         626       0.00      0.00      0.00         1\n",
      "         629       0.00      0.00      0.00         1\n",
      "         631       0.00      0.00      0.00         1\n",
      "         633       0.00      0.00      0.00         1\n",
      "         634       0.00      0.00      0.00         1\n",
      "         637       0.00      0.00      0.00         1\n",
      "         638       0.00      0.00      0.00         1\n",
      "         643       0.00      0.00      0.00         1\n",
      "         646       0.00      0.00      0.00         1\n",
      "         648       0.00      0.00      0.00         1\n",
      "         650       0.00      0.00      0.00         1\n",
      "         651       0.00      0.00      0.00         2\n",
      "         655       0.00      0.00      0.00         2\n",
      "         656       0.00      0.00      0.00         1\n",
      "         658       0.00      0.00      0.00         1\n",
      "         660       0.00      0.00      0.00         1\n",
      "         663       0.00      0.00      0.00         1\n",
      "         664       0.00      0.00      0.00         1\n",
      "         668       0.00      0.00      0.00         1\n",
      "         677       0.00      0.00      0.00         1\n",
      "         687       0.00      0.00      0.00         1\n",
      "         689       0.00      0.00      0.00         2\n",
      "         699       0.00      0.00      0.00         1\n",
      "         700       0.00      0.00      0.00         1\n",
      "         710       0.00      0.00      0.00         1\n",
      "         714       0.00      0.00      0.00         2\n",
      "         715       0.00      0.00      0.00         1\n",
      "         717       0.00      0.00      0.00         1\n",
      "         719       0.00      0.00      0.00         1\n",
      "         727       0.00      0.00      0.00         2\n",
      "         731       0.00      0.00      0.00         1\n",
      "         736       0.00      0.00      0.00         1\n",
      "         743       0.00      0.00      0.00         1\n",
      "         747       0.00      0.00      0.00         3\n",
      "         750       0.00      0.00      0.00         1\n",
      "         751       0.00      0.00      0.00         1\n",
      "         752       0.00      0.00      0.00         2\n",
      "         754       0.00      0.00      0.00         1\n",
      "         756       0.00      0.00      0.00         1\n",
      "         760       0.00      0.00      0.00         1\n",
      "         762       0.00      0.00      0.00         1\n",
      "         767       0.00      0.00      0.00         2\n",
      "         768       0.00      0.00      0.00         1\n",
      "         770       0.00      0.00      0.00         2\n",
      "         771       0.00      0.00      0.00         1\n",
      "         779       0.00      0.00      0.00         1\n",
      "         783       0.00      0.00      0.00         2\n",
      "         786       0.00      0.00      0.00         2\n",
      "         787       0.00      0.00      0.00         1\n",
      "         789       0.00      0.00      0.00         1\n",
      "         790       0.00      0.00      0.00         1\n",
      "         791       0.00      0.00      0.00         3\n",
      "         793       0.00      0.00      0.00         1\n",
      "         794       0.00      0.00      0.00         1\n",
      "         796       0.00      0.00      0.00         1\n",
      "         802       0.00      0.00      0.00         2\n",
      "         804       0.00      0.00      0.00         1\n",
      "         806       0.00      0.00      0.00         2\n",
      "         807       0.00      0.00      0.00         1\n",
      "         814       0.00      0.00      0.00         1\n",
      "         818       0.00      0.00      0.00         4\n",
      "         822       0.00      0.00      0.00         3\n",
      "         823       0.00      0.00      0.00         1\n",
      "         824       0.00      0.00      0.00         2\n",
      "         833       0.00      0.00      0.00         1\n",
      "         834       0.00      0.00      0.00         2\n",
      "         835       0.00      0.00      0.00         1\n",
      "         836       0.00      0.00      0.00         2\n",
      "         837       0.00      0.00      0.00         1\n",
      "         839       0.00      0.00      0.00         1\n",
      "         842       0.00      0.00      0.00         1\n",
      "         848       0.00      0.00      0.00         1\n",
      "         849       0.00      0.00      0.00         2\n",
      "         850       0.00      0.00      0.00         1\n",
      "         854       0.00      0.00      0.00         1\n",
      "         857       0.00      0.00      0.00         1\n",
      "         860       0.00      0.00      0.00         1\n",
      "         863       0.00      0.00      0.00         1\n",
      "         864       0.00      0.00      0.00         1\n",
      "         866       0.00      0.00      0.00         1\n",
      "         869       0.00      0.00      0.00         1\n",
      "         874       0.00      0.00      0.00         1\n",
      "         876       0.00      0.00      0.00         1\n",
      "         882       0.00      0.00      0.00         1\n",
      "         885       0.00      0.00      0.00         1\n",
      "         893       0.00      0.00      0.00         1\n",
      "         895       0.00      0.00      0.00         1\n",
      "         897       0.00      0.00      0.00         1\n",
      "         902       0.00      0.00      0.00         2\n",
      "         903       0.00      0.00      0.00         1\n",
      "         908       0.00      0.00      0.00         1\n",
      "         913       0.00      0.00      0.00         1\n",
      "         915       0.00      0.00      0.00         1\n",
      "         916       0.00      0.00      0.00         1\n",
      "         924       0.00      0.00      0.00         1\n",
      "         925       0.00      0.00      0.00         1\n",
      "         939       0.00      0.00      0.00         1\n",
      "         940       0.00      0.00      0.00         1\n",
      "         942       0.00      0.00      0.00         1\n",
      "         944       0.00      0.00      0.00         1\n",
      "         945       0.00      0.00      0.00         1\n",
      "         946       0.00      0.00      0.00         2\n",
      "         951       0.00      0.00      0.00         1\n",
      "         956       0.00      0.00      0.00         1\n",
      "         959       0.00      0.00      0.00         1\n",
      "         961       0.00      0.00      0.00         1\n",
      "         962       0.00      0.00      0.00         1\n",
      "         968       0.00      0.00      0.00         1\n",
      "         969       0.00      0.00      0.00         1\n",
      "         970       0.00      0.00      0.00         1\n",
      "         975       0.00      0.00      0.00         1\n",
      "         981       0.00      0.00      0.00         1\n",
      "         994       0.00      0.00      0.00         1\n",
      "         996       0.00      0.00      0.00         1\n",
      "         998       0.00      0.00      0.00         1\n",
      "        1000       0.00      0.00      0.00         1\n",
      "        1003       0.00      0.00      0.00         1\n",
      "        1007       0.00      0.00      0.00         1\n",
      "        1019       0.00      0.00      0.00         1\n",
      "        1022       0.00      0.00      0.00         1\n",
      "        1035       0.00      0.00      0.00         1\n",
      "        1036       0.00      0.00      0.00         1\n",
      "        1038       0.00      0.00      0.00         1\n",
      "        1040       0.00      0.00      0.00         1\n",
      "        1041       0.00      0.00      0.00         1\n",
      "        1043       0.00      0.00      0.00         2\n",
      "        1045       0.00      0.00      0.00         1\n",
      "        1050       0.00      0.00      0.00         1\n",
      "        1053       0.00      0.00      0.00         1\n",
      "        1064       0.00      0.00      0.00         1\n",
      "        1065       0.00      0.00      0.00         1\n",
      "        1094       0.00      0.00      0.00         1\n",
      "        1096       0.00      0.00      0.00         1\n",
      "        1097       0.00      0.00      0.00         2\n",
      "        1100       0.00      0.00      0.00         1\n",
      "        1102       0.00      0.00      0.00         2\n",
      "        1109       0.00      0.00      0.00         1\n",
      "        1114       0.00      0.00      0.00         1\n",
      "        1122       0.00      0.00      0.00         1\n",
      "        1123       0.00      0.00      0.00         1\n",
      "        1126       0.00      0.00      0.00         1\n",
      "        1127       0.00      0.00      0.00         1\n",
      "        1130       0.00      0.00      0.00         1\n",
      "        1139       0.00      0.00      0.00         1\n",
      "        1146       0.00      0.00      0.00         1\n",
      "        1147       0.00      0.00      0.00         1\n",
      "        1148       0.00      0.00      0.00         1\n",
      "        1150       0.00      0.00      0.00         1\n",
      "        1151       0.00      0.00      0.00         1\n",
      "        1153       0.00      0.00      0.00         1\n",
      "        1156       0.00      0.00      0.00         1\n",
      "        1161       0.00      0.00      0.00         1\n",
      "        1163       0.00      0.00      0.00         1\n",
      "        1164       0.00      0.00      0.00         1\n",
      "        1167       0.00      0.00      0.00         1\n",
      "        1169       0.00      0.00      0.00         1\n",
      "        1170       0.00      0.00      0.00         2\n",
      "        1171       0.00      0.00      0.00         1\n",
      "        1173       0.00      0.00      0.00         1\n",
      "        1187       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.01      1847\n",
      "   macro avg       0.00      0.00      0.00      1847\n",
      "weighted avg       0.00      0.01      0.00      1847\n",
      "\n",
      "spearman's rho (correlation)           true      pred\n",
      "true  1.000000 -0.040495\n",
      "pred -0.040495  1.000000\n",
      "R^2 score: -0.66\n",
      "mean absolute error score 212.3\n",
      "*** validation performance: ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           3       0.00      0.00      0.00       1.0\n",
      "           8       0.00      0.00      0.00       1.0\n",
      "          13       0.00      0.00      0.00       2.0\n",
      "          14       0.00      0.00      0.00       1.0\n",
      "          15       0.00      0.00      0.00       1.0\n",
      "          21       0.00      0.00      0.00       1.0\n",
      "          26       0.00      0.00      0.00       1.0\n",
      "          35       0.00      0.00      0.00       1.0\n",
      "          36       0.00      0.00      0.00       1.0\n",
      "          38       0.00      0.00      0.00       1.0\n",
      "          39       0.00      0.00      0.00       1.0\n",
      "          42       0.00      0.00      0.00       2.0\n",
      "          45       0.00      0.00      0.00       2.0\n",
      "          47       0.00      0.00      0.00       1.0\n",
      "          50       0.00      0.00      0.00       1.0\n",
      "          51       0.00      0.00      0.00       4.0\n",
      "          52       0.00      0.00      0.00       1.0\n",
      "          53       0.00      0.00      0.00       1.0\n",
      "          56       0.00      0.00      0.00       1.0\n",
      "          57       0.00      0.00      0.00       1.0\n",
      "          58       0.00      0.00      0.00       2.0\n",
      "          60       0.00      0.00      0.00       1.0\n",
      "          63       0.00      0.00      0.00       1.0\n",
      "          64       0.00      0.00      0.00       2.0\n",
      "          71       0.00      0.00      0.00       2.0\n",
      "          72       0.00      0.00      0.00       1.0\n",
      "          73       0.00      0.00      0.00       6.0\n",
      "          77       0.00      0.00      0.00       1.0\n",
      "          78       0.00      0.00      0.00       1.0\n",
      "          81       0.00      0.00      0.00       1.0\n",
      "          84       0.00      0.00      0.00       1.0\n",
      "          88       0.00      0.00      0.00       2.0\n",
      "          91       0.00      0.00      0.00       2.0\n",
      "          93       0.00      0.00      0.00       1.0\n",
      "          94       0.00      0.00      0.00       1.0\n",
      "          97       0.00      0.00      0.00       1.0\n",
      "          99       0.00      0.00      0.00       1.0\n",
      "         100       0.00      0.00      0.00       1.0\n",
      "         102       0.00      0.00      0.00       1.0\n",
      "         110       0.00      0.00      0.00       1.0\n",
      "         113       0.00      0.00      0.00       1.0\n",
      "         114       0.00      0.00      0.00       1.0\n",
      "         125       0.00      0.00      0.00       1.0\n",
      "         133       0.00      0.00      0.00       1.0\n",
      "         137       0.00      0.00      0.00       1.0\n",
      "         153       0.00      0.00      0.00       1.0\n",
      "         156       0.00      0.00      0.00       1.0\n",
      "         168       0.00      0.00      0.00       1.0\n",
      "         169       0.00      0.00      0.00       1.0\n",
      "         173       0.00      0.00      0.00       1.0\n",
      "         176       0.00      0.00      0.00       1.0\n",
      "         179       0.00      0.00      0.00       1.0\n",
      "         188       0.00      0.00      0.00       1.0\n",
      "         198       0.00      0.00      0.00       1.0\n",
      "         202       0.00      0.00      0.00       1.0\n",
      "         229       0.00      0.00      0.00       1.0\n",
      "         237       0.00      0.00      0.00       1.0\n",
      "         248       0.00      0.00      0.00       1.0\n",
      "         252       0.00      0.00      0.00       3.0\n",
      "         255       0.00      0.00      0.00       1.0\n",
      "         263       0.00      0.00      0.00       1.0\n",
      "         272       0.00      0.00      0.00       1.0\n",
      "         286       0.00      0.00      0.00       1.0\n",
      "         289       0.00      0.00      0.00       1.0\n",
      "         291       0.00      0.00      0.00       1.0\n",
      "         295       0.00      0.00      0.00       2.0\n",
      "         296       0.00      0.00      0.00       1.0\n",
      "         297       0.00      0.00      0.00       1.0\n",
      "         299       0.00      0.00      0.00       1.0\n",
      "         303       0.00      0.00      0.00       1.0\n",
      "         308       0.00      0.00      0.00       1.0\n",
      "         311       0.00      0.00      0.00       1.0\n",
      "         317       0.00      0.00      0.00       1.0\n",
      "         321       0.00      0.00      0.00       1.0\n",
      "         332       0.00      0.00      0.00       1.0\n",
      "         339       0.00      0.00      0.00       1.0\n",
      "         368       0.00      0.00      0.00       1.0\n",
      "         391       0.00      0.00      0.00       1.0\n",
      "         402       0.00      0.00      0.00       1.0\n",
      "         418       0.00      0.00      0.00       1.0\n",
      "         419       0.00      0.00      0.00       2.0\n",
      "         423       0.00      0.00      0.00       1.0\n",
      "         428       0.00      0.00      0.00       1.0\n",
      "         442       0.00      0.00      0.00       2.0\n",
      "         445       0.00      0.00      0.00       1.0\n",
      "         449       0.00      0.00      0.00       1.0\n",
      "         453       0.00      0.00      0.00       1.0\n",
      "         454       0.00      0.00      0.00       2.0\n",
      "\n",
      "    accuracy                           0.00     110.0\n",
      "   macro avg       0.00      0.00      0.00     110.0\n",
      "weighted avg       0.00      0.00      0.00     110.0\n",
      "\n",
      "spearman's rho (correlation)           true      pred\n",
      "true  1.000000  0.092538\n",
      "pred  0.092538  1.000000\n",
      "R^2 score: -1.6\n",
      "mean absolute error score 171.5\n",
      "*** Test-set performance: ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       0.0\n",
      "           3       0.00      0.00      0.00       1.0\n",
      "           9       0.00      0.00      0.00       1.0\n",
      "          13       0.00      0.00      0.00       3.0\n",
      "          14       0.00      0.00      0.00       2.0\n",
      "          22       0.00      0.00      0.00       1.0\n",
      "          26       0.00      0.00      0.00       3.0\n",
      "          36       0.00      0.00      0.00       5.0\n",
      "          42       0.00      0.00      0.00       1.0\n",
      "          47       0.00      0.00      0.00       2.0\n",
      "          51       0.00      0.00      0.00       5.0\n",
      "          52       0.00      0.00      0.00       1.0\n",
      "          53       0.00      0.00      0.00       2.0\n",
      "          63       0.00      0.00      0.00       2.0\n",
      "          64       0.00      0.00      0.00       2.0\n",
      "          65       0.00      0.00      0.00       2.0\n",
      "          71       0.00      0.00      0.00       2.0\n",
      "          72       0.00      0.00      0.00       4.0\n",
      "          73       0.00      0.00      0.00       1.0\n",
      "          74       0.00      0.00      0.00       2.0\n",
      "          78       0.00      0.00      0.00       1.0\n",
      "          80       0.00      0.00      0.00       5.0\n",
      "          89       0.00      0.00      0.00       1.0\n",
      "          90       0.00      0.00      0.00       1.0\n",
      "          95       0.00      0.00      0.00       1.0\n",
      "          99       0.00      0.00      0.00       1.0\n",
      "         102       0.00      0.00      0.00       1.0\n",
      "         119       0.00      0.00      0.00       1.0\n",
      "         126       0.00      0.00      0.00       3.0\n",
      "         137       0.00      0.00      0.00       2.0\n",
      "         140       0.00      0.00      0.00       1.0\n",
      "         151       0.00      0.00      0.00       1.0\n",
      "         153       0.00      0.00      0.00       7.0\n",
      "         156       0.00      0.00      0.00       1.0\n",
      "         157       0.00      0.00      0.00       1.0\n",
      "         176       0.00      0.00      0.00       1.0\n",
      "         180       0.00      0.00      0.00       1.0\n",
      "         218       0.00      0.00      0.00       2.0\n",
      "         229       0.00      0.00      0.00       1.0\n",
      "         248       0.00      0.00      0.00       1.0\n",
      "         252       0.00      0.00      0.00       1.0\n",
      "         255       0.00      0.00      0.00       1.0\n",
      "         274       0.00      0.00      0.00       1.0\n",
      "         289       0.00      0.00      0.00       1.0\n",
      "         304       0.00      0.00      0.00       1.0\n",
      "         308       0.00      0.00      0.00       1.0\n",
      "         312       0.00      0.00      0.00       2.0\n",
      "         321       0.00      0.00      0.00       1.0\n",
      "         380       0.00      0.00      0.00       1.0\n",
      "         382       0.00      0.00      0.00       1.0\n",
      "         402       0.00      0.00      0.00       2.0\n",
      "         404       0.00      0.00      0.00       1.0\n",
      "         420       0.00      0.00      0.00       1.0\n",
      "         421       0.00      0.00      0.00       1.0\n",
      "         424       0.00      0.00      0.00       1.0\n",
      "         425       0.00      0.00      0.00       2.0\n",
      "         429       0.00      0.00      0.00       3.0\n",
      "         430       0.00      0.00      0.00       1.0\n",
      "         435       0.00      0.00      0.00       1.0\n",
      "         442       0.00      0.00      0.00       1.0\n",
      "         444       0.00      0.00      0.00       1.0\n",
      "         447       0.00      0.00      0.00       1.0\n",
      "         450       0.00      0.00      0.00       1.0\n",
      "         453       0.00      0.00      0.00       1.0\n",
      "         454       0.00      0.00      0.00       3.0\n",
      "\n",
      "    accuracy                           0.00     108.0\n",
      "   macro avg       0.00      0.00      0.00     108.0\n",
      "weighted avg       0.00      0.00      0.00     108.0\n",
      "\n",
      "spearman's rho (correlation)           true      pred\n",
      "true  1.000000 -0.202638\n",
      "pred -0.202638  1.000000\n",
      "R^2 score: -1.4\n",
      "mean absolute error score 175.6\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "def chunk_string(string:str, chunk_size:int):\n",
    "    return [string[i:(chunk_size + i)] for i in range(0, len(string), chunk_size)]\n",
    "\n",
    "def chunk_df(df, chunk_size:int):\n",
    "    return pd.DataFrame({col: df[col].apply(chunk_string, chunk_size = chunk_size).explode().values for col in df.columns})\n",
    "\n",
    "def load_benchmark_dataset(benchmark_name):\n",
    "    \n",
    "    text_file_path = os.path.join(BENCHMARKS_DIR, '%s.benchmark.txt' % benchmark_name)    \n",
    "    train_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.train.csv' % benchmark_name)\n",
    "    valid_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.valid.csv' % benchmark_name)\n",
    "    test_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.test.csv' % benchmark_name)\n",
    "    \n",
    "    train_set = pd.read_csv(train_set_file_path).dropna().drop_duplicates()\n",
    "          \n",
    "    if os.path.exists(valid_set_file_path):\n",
    "        valid_set = pd.read_csv(valid_set_file_path).dropna().drop_duplicates()\n",
    "    else:\n",
    "        print(f\"validation set {valid_set_file_path} missing\")\n",
    "        print(\"splitting train to train and random validation set\")\n",
    "        try: \n",
    "            train_set, valid_set = train_test_split(train_set, stratify=train_set['labels'],test_size=0.1,random_state=42)\n",
    "            print(\"Stratified sampling of validation set\")\n",
    "        except:\n",
    "            print(\"randomly sampling validation set\")\n",
    "            train_set, valid_set = train_test_split(train_set,test_size=0.1,random_state=42)    \n",
    "    \n",
    "    test_set = pd.read_csv(test_set_file_path).dropna().drop_duplicates()\n",
    "    \n",
    "    return text_file_path, train_set, valid_set, test_set\n",
    "\n",
    "\n",
    "## new\n",
    "def fast_run(train_set, valid_set, test_set, is_y_discrete):\n",
    "#     if \"remote_homology\" in benchmark_name:\n",
    "#         train_set = pd.concat([train_set.sample(frac=FAST_SAMPLE_RATIO,random_state=42),train_set.drop_duplicates('labels')]).drop_duplicates()\n",
    "    if is_y_discrete:\n",
    "        print(\"discrete_sampling\")\n",
    "#         t1,_ = train_test_split(train_set,stratify=train_set['labels'],train_size=FAST_SAMPLE_RATIO,random_state=42) # stratified sampling 0 for multiclass\n",
    "        t1,_ = train_test_split(train_set,train_size=FAST_SAMPLE_RATIO,random_state=42) # random, unstratified sampling - avoids errors with to ofew smaples per class (we combine it with at least 1 example per class below)\n",
    "        t2 = train_set.drop_duplicates('labels')\n",
    "#         t2 = train_set.groupby(['labels']).apply(lambda x: x.sample(2))\n",
    "        print(\"t2 shape\",t2.shape)\n",
    "        train_set = pd.concat([t1,t2]).drop_duplicates()\n",
    "                \n",
    "    else:\n",
    "        train_set = train_set.sample(frac=FAST_SAMPLE_RATIO,random_state=42)\n",
    "    valid_set = valid_set.sample(frac=FAST_SAMPLE_RATIO)\n",
    "    test_set = test_set.sample(frac=FAST_SAMPLE_RATIO)   \n",
    "    return train_set, valid_set, test_set\n",
    "\n",
    "def get_y_type(y):\n",
    "    '''\n",
    "    Determining which of the following y is:\n",
    "    1. Numeric (could be either probabalistic (i.e. in the range 0-1) or not)\n",
    "    2. Sequence\n",
    "    3. Categorical ### confusing - we return is_y_probability ??? # DAN\n",
    "    '''\n",
    "#     is_y_numeric = np.issubdtype(y.dtype, np.floating)\n",
    "    is_y_numeric = np.issubdtype(y.dtype, np.number)\n",
    "    if is_y_numeric:\n",
    "        is_y_probability =  not isinstance(y, int)# ORIG - #y.min() >= 0 and y.max() <= 1\n",
    "        is_y_seq = False\n",
    "    else: \n",
    "        is_y_probability = False\n",
    "        is_y_seq = y.astype(str).str.len().max()>14  # duuuuuudeeeeee\n",
    "        print('Numeric (%sprobabilistic) label' % ('' if is_y_probability else 'not '))\n",
    "    is_y_discrete = (not (is_y_seq or is_y_probability)) # may want to add condition based on cardinality? Otherwise fails for fluorescence\n",
    "    return is_y_numeric, is_y_seq, is_y_probability,is_y_discrete\n",
    "\n",
    "def build_fine_tuning_model(is_y_numeric, is_y_probability, is_y_seq,max_seq_len, n_labels,learning_rate= 2e-04):\n",
    "    model = create_model(max_seq_len)\n",
    "    load_model_weights(model, PRETRAINED_MODEL_WEIGHTS_FILE_PATH)\n",
    "    input_seq_layer, input_annoatations_layer = model.input\n",
    "    output_seq_layer, output_annoatations_layer = model.output    \n",
    "    if is_y_numeric:\n",
    "        output_layer = keras.layers.Dense(1, activation = ('sigmoid' if is_y_probability else None))(output_annoatations_layer)\n",
    "        loss = 'binary_crossentropy' if not is_y_probability else 'mse' # DAN fix\n",
    "    elif is_y_seq:\n",
    "        output_layer = keras.layers.Dense(n_labels + 1, activation = 'softmax')(output_seq_layer)\n",
    "        loss = 'categorical_crossentropy'\n",
    "    else: # non-seq categorical\n",
    "        print(\"build_fine_tuning_model - n_labels\",n_labels)\n",
    "        if n_labels ==2:\n",
    "            loss = 'binary_crossentropy' \n",
    "        else:\n",
    "            loss = 'categorical_crossentropy' \n",
    "        output_layer = keras.layers.Dense(n_labels, activation = 'softmax')(output_annoatations_layer)\n",
    "          \n",
    "    model = keras.models.Model(inputs = [input_seq_layer, input_annoatations_layer], outputs = output_layer)\n",
    "    model.compile(optimizer = keras.optimizers.Adam(lr = learning_rate), loss = loss) \n",
    "    return model\n",
    "\n",
    "def encode_seq_Y(raw_Y, max_seq_len, n_labels, label_to_index):\n",
    "    # +1 for padding\n",
    "    Y = np.zeros((len(raw_Y), max_seq_len, n_labels + 1), dtype = np.int8)\n",
    "    for i, seq in enumerate(raw_Y):\n",
    "        for j, token in enumerate(seq):\n",
    "            Y[i, j, label_to_index[token]] = 1\n",
    "        Y[i, np.arange(len(seq), max_seq_len), n_labels] = 1\n",
    "    return Y\n",
    "\n",
    "\n",
    "def preproc_benchmark_dataset(train_set, valid_set, test_set, n_labels, label_to_index, max_seq_len, is_y_numeric, is_y_seq):\n",
    "    train_X = [\n",
    "        tokenize_seqs(train_set['text'].values, max_seq_len).astype(np.int32),\n",
    "        np.zeros((len(train_set), N_ANNOTATIONS), dtype = np.int8)\n",
    "    ]    \n",
    "    valid_X = [\n",
    "        tokenize_seqs(valid_set['text'].values, max_seq_len).astype(np.int32),\n",
    "        np.zeros((len(valid_set), N_ANNOTATIONS), dtype = np.int8)\n",
    "    ]    \n",
    "    test_X = [\n",
    "        tokenize_seqs(test_set['text'].values, max_seq_len).astype(np.int32),\n",
    "        np.zeros((len(test_set), N_ANNOTATIONS), dtype = np.int8)\n",
    "    ]    \n",
    "    if is_y_numeric:\n",
    "        train_Y = train_set['labels'].values\n",
    "        valid_Y = valid_set['labels'].values\n",
    "    elif is_y_seq:  \n",
    "        print(\"y-seq y encoding\")\n",
    "        train_Y = encode_seq_Y(train_set['labels'], max_seq_len, n_labels, label_to_index)\n",
    "        valid_Y = encode_seq_Y(valid_set['labels'], max_seq_len, n_labels, label_to_index)\n",
    "    else: # non-seq categorical       \n",
    "        #### i replaced our encode_categorical_y with TF's to_categoircal . TF's expects an integer input. \n",
    "        \n",
    "#         train_Y = encode_categorical_y(train_set['labels'], n_labels, label_to_index)\n",
    "#         valid_Y = encode_categorical_y(valid_set['labels'], n_labels, label_to_index)\n",
    "        train_Y = to_categorical(train_set['labels'])\n",
    "        valid_Y = to_categorical(valid_set['labels'])\n",
    "        \n",
    "    return train_X, valid_X, test_X, train_Y, valid_Y\n",
    "\n",
    "def train_and_eval(train_set, valid_set, test_set, is_y_numeric, is_y_probability, is_y_seq,\n",
    "                   n_labels, unique_labels, label_to_index):\n",
    "    train_X, valid_X, test_X, train_Y, valid_Y = preproc_benchmark_dataset(train_set, valid_set, test_set, \n",
    "                                                                           n_labels, label_to_index, MAX_GLOBAL_SEQ_LEN, is_y_numeric, is_y_seq)\n",
    "    model = build_fine_tuning_model(is_y_numeric, is_y_probability, is_y_seq, MAX_GLOBAL_SEQ_LEN, n_labels)\n",
    "    ### Train model, with early stopping on validation set\n",
    "    model.fit(train_X, train_Y,\n",
    "              batch_size = BATCH_SIZE,\n",
    "              validation_data=(valid_X,valid_Y),\n",
    "              callbacks = [ReduceLROnPlateau(patience=2,factor=0.35), EarlyStopping(patience=4)],\n",
    "              epochs = MAX_EPOCHS,\n",
    "              validation_batch_size=BATCH_SIZE,\n",
    "              verbose=1)\n",
    "    \n",
    "    #### DAN: NOTE - keras already keeps evaluation, train error data.\n",
    "    print('\\n*** Training-set performance: ***')\n",
    "    train_Y_pred = model.predict(train_X)\n",
    "    evaluate(train_Y_pred, train_set['labels'].values, is_y_numeric, is_y_seq, unique_labels)\n",
    "    print('*** validation performance: ***')\n",
    "    valid_Y_pred = model.predict(valid_X)\n",
    "    evaluate(valid_Y_pred, valid_set['labels'].values, is_y_numeric, is_y_seq, unique_labels)\n",
    "    print('*** Test-set performance: ***')\n",
    "    test_Y_pred = model.predict(test_X)\n",
    "    evaluate(test_Y_pred, test_set['labels'].values, is_y_numeric, is_y_seq, unique_labels)    \n",
    "    print('\\n' * 3)\n",
    "    \n",
    "def train_and_eval_after_removing_too_long_seqs(train_set, valid_set, test_set, is_y_numeric, is_y_probability, is_y_seq,\n",
    "                                                n_labels, unique_labels, label_to_index):\n",
    "    filtered_train_set = train_set[train_set['text'].str.len() <= MAX_ALLOWED_INPUT_SEQ]\n",
    "    filtered_valid_set = valid_set[valid_set['text'].str.len() <= MAX_ALLOWED_INPUT_SEQ]\n",
    "    filtered_test_set = test_set[test_set['text'].str.len() <= MAX_ALLOWED_INPUT_SEQ]\n",
    "    n_removed_train_set = len(train_set) - len(filtered_train_set)\n",
    "    ptg_removed_train_set = 100 * n_removed_train_set / len(train_set)\n",
    "    n_removed_valid_set = len(valid_set) - len(filtered_valid_set)\n",
    "    ptg_removed_valid_set = 100 * n_removed_valid_set / len(valid_set)\n",
    "    n_removed_test_set = len(test_set) - len(filtered_test_set)\n",
    "    ptg_removed_test_set = 100 * n_removed_test_set / len(test_set)\n",
    "    print('Trying to remove too long sequences. Removed %d of %d (%.1g%%) of the training set, %d of %d (%.1g%%) of the validation set and %d of %d (%.1g%%) of the test set' %\n",
    "            (n_removed_train_set, len(train_set), ptg_removed_train_set, n_removed_valid_set, len(valid_set), ptg_removed_valid_set, n_removed_test_set, len(test_set), ptg_removed_test_set))\n",
    "    train_and_eval(filtered_train_set, filtered_valid_set, filtered_test_set, is_y_numeric, is_y_probability, is_y_seq, n_labels, unique_labels, label_to_index)\n",
    "    \n",
    "def truncate_dataset(dataset, is_y_seq):\n",
    "    if is_y_seq:\n",
    "        return chunk_df(dataset, MAX_ALLOWED_INPUT_SEQ)\n",
    "    else:\n",
    "        dataset = dataset.copy()\n",
    "        dataset['text'] = dataset['text'].apply(lambda seq: seq[:MAX_ALLOWED_INPUT_SEQ])\n",
    "        return dataset\n",
    "    \n",
    "def train_and_eval_after_trancating_too_long_seqs(train_set, valid_set, test_set, is_y_numeric, is_y_probability,\n",
    "                                                  is_y_seq, n_labels, unique_labels, label_to_index):\n",
    "    print('Will now truncate too-long sequences.')\n",
    "    train_and_eval(truncate_dataset(train_set, is_y_seq), truncate_dataset(valid_set, is_y_seq), truncate_dataset(test_set, is_y_seq), is_y_numeric, is_y_probability, is_y_seq,\n",
    "                   n_labels, unique_labels, label_to_index)\n",
    "\n",
    "    \n",
    "def run_benchmark(benchmark_name):\n",
    "    \n",
    "    print('========== %s ==========' % benchmark_name)   \n",
    "    print('\\n')\n",
    "    text_file_path, train_set, valid_set, test_set = load_benchmark_dataset(benchmark_name)\n",
    "    is_y_numeric, is_y_seq, is_y_probability,is_y_discrete = get_y_type(train_set['labels'])\n",
    "    if is_y_numeric: print(\"y_numeric\")\n",
    "    if is_y_seq: print(\"y_seq\")\n",
    "    if is_y_probability: print(\"y_probability\")\n",
    "    if is_y_discrete: print(\"y_discrete\")\n",
    "\n",
    "    if FAST_RUN: train_set, valid_set, test_set = fast_run(train_set, valid_set, test_set,is_y_discrete) # dan change - use new is_y_discrete\n",
    "    print(f'{len(train_set)} training-set records, {len(valid_set)} valid-set records, {len(test_set)} test-set records')     \n",
    "    \n",
    "    if DEBUG_MODE:\n",
    "        print(train_set.dtypes)\n",
    "        print(valid_set.dtypes)\n",
    "    if is_y_numeric:\n",
    "        ### stupid ugly hack - the numeric/continous targets don't have n_labels/unique_labels, but their surroudning functions expect them - I hack in a default for now, expect a refactor - DAN\n",
    "        n_labels=2 # default hack\n",
    "        unique_labels=(\"0\",\"1\") # default hack\n",
    "        label_to_index = {}\n",
    "\n",
    "    if not is_y_numeric:\n",
    "        train_set['labels'] = train_set['labels'].astype(str)\n",
    "        if is_y_seq:            \n",
    "            unique_labels = sorted(set.union(*train_set['labels'].apply(set))) \n",
    "        else:\n",
    "            unique_labels = sorted(train_set['labels'].unique())                               \n",
    "        n_labels = len(unique_labels)\n",
    "        label_to_index = {label: i for i, label in enumerate(unique_labels)}        \n",
    "        print('Sequence output with %d tokens.' % n_labels if is_y_seq else 'Categorical output with %d labels.' % n_labels)\n",
    "     \n",
    "    \n",
    "    if not is_y_numeric:\n",
    "        if is_y_discrete:\n",
    "            le = preprocessing.LabelEncoder()\n",
    "#         if len(unique_labels)>2:\n",
    "            print(\"\\n label encoding class labels\")\n",
    "#                 ##i.e y is discrete classes\n",
    "#                 print(\"unique train labels\",train_set['labels'].head())            \n",
    "#             le = le.fit(train_set['labels'])\n",
    "\n",
    "            train_set['labels'] = le.fit_transform(train_set['labels'].values)\n",
    "            print(\"le\",le)\n",
    "            print(\"le.classes_\",le.classes_)\n",
    "            print(len(list(le.classes_)))\n",
    "            valid_set['labels'] = le.transform(valid_set['labels'])\n",
    "            test_set['labels'] = le.transform(test_set['labels'])\n",
    "    \n",
    "    if DEBUG_MODE:\n",
    "        print(\"n_labels\",n_labels)\n",
    "    \n",
    "    if max(train_set[\"text\"].str.len().max(), valid_set[\"text\"].str.len().max(), test_set[\"text\"].str.len().max()) <= MAX_ALLOWED_INPUT_SEQ:\n",
    "        train_and_eval(train_set, valid_set, test_set, is_y_numeric, is_y_probability, is_y_seq, n_labels, unique_labels, label_to_index)\n",
    "    else:\n",
    "        train_and_eval_after_removing_too_long_seqs(train_set, valid_set, test_set, is_y_numeric, is_y_probability, is_y_seq, n_labels, unique_labels, label_to_index)\n",
    "        train_and_eval_after_trancating_too_long_seqs(train_set, valid_set, test_set, is_y_numeric, is_y_probability, is_y_seq, n_labels, unique_labels, label_to_index)\n",
    "\n",
    "        \n",
    "for benchmark_name in BENCHMARKS:\n",
    "    run_benchmark(benchmark_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train # unique labels 1195\n",
      "valid # unique labels 319\n",
      "test # unique labels 136\n",
      "0    382\n",
      "1     67\n",
      "2     25\n",
      "3     20\n",
      "4     65\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "text_file_path, train_set, valid_set, test_set = load_benchmark_dataset('remote_homology')\n",
    "\n",
    "print(\"train # unique labels\",train_set.labels.nunique())\n",
    "print(\"valid # unique labels\",valid_set.labels.nunique())\n",
    "print(\"test # unique labels\",test_set.labels.nunique())\n",
    "print(test_set['labels'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le LabelEncoder()\n",
      "le.classes_ [   0    1    2 ... 1192 1193 1194]\n",
      "1195\n",
      "0      382\n",
      "1       67\n",
      "2       25\n",
      "3       20\n",
      "4       65\n",
      "      ... \n",
      "713    380\n",
      "714    255\n",
      "715     36\n",
      "716    423\n",
      "717      9\n",
      "Name: labels, Length: 718, dtype: int64\n",
      "0      382\n",
      "1       67\n",
      "2       25\n",
      "3       20\n",
      "4       65\n",
      "      ... \n",
      "713    380\n",
      "714    255\n",
      "715     36\n",
      "716    423\n",
      "717      9\n",
      "Name: labels, Length: 718, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_set['labels'])\n",
    "print(\"le\",le)\n",
    "print(\"le.classes_\",le.classes_)\n",
    "print(len(list(le.classes_)))\n",
    "train_set['labels'] = le.transform(train_set['labels'])\n",
    "valid_set['labels'] = le.transform(valid_set['labels'])\n",
    "print(test_set['labels'] )\n",
    "test_set['labels'] = le.transform(test_set['labels'])\n",
    "print(test_set['labels'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.issubdtype(test_set['labels'].dtype, np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
