{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Test GPU\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "#!pip install bert-for-tf2\n",
    "import bert\n",
    "from bert import BertModelLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 16000\n",
    "N_RESERVED_SYMBOLS = 2 # We want to reserve two symbols: 1) for PADDING, 2) for MASKING.\n",
    "\n",
    "PAD_TOKEN = VOCAB_SIZE - 1\n",
    "MASK_TOKEN = VOCAB_SIZE - 2\n",
    "\n",
    "MAX_LEN = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 947596 proteins of relevant length.\n",
      "(947596, 250)\n"
     ]
    }
   ],
   "source": [
    "SAMPLES_SLICE = slice(None)\n",
    "\n",
    "DATASET_H5_FILE_PATH = '/cs/phd/nadavb/cafa_project/data/protein_tokens.h5'\n",
    "\n",
    "def pad_tokens(tokens):\n",
    "    return np.concatenate([tokens, PAD_TOKEN * np.ones(MAX_LEN - len(tokens), dtype = tokens.dtype)])\n",
    "\n",
    "with h5py.File(DATASET_H5_FILE_PATH, 'r') as h5f:\n",
    "    \n",
    "    h5f_group = h5f['protein_tokens']\n",
    "    relevant_seqs_tokens = h5f_group['tokens'][SAMPLES_SLICE][h5f_group['n_tokens'][SAMPLES_SLICE] <= MAX_LEN]\n",
    "    print('Selected %d proteins of relevant length.' % len(relevant_seqs_tokens))\n",
    "    \n",
    "    dataset_tokens = np.array(list(map(pad_tokens, relevant_seqs_tokens)))\n",
    "    del relevant_seqs_tokens\n",
    "    print(dataset_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True ...  True False False]\n",
      " [ True  True  True ... False  True  True]\n",
      " [ True False  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ... False  True  True]\n",
      " [ True  True False ...  True  True  True]\n",
      " [False  True False ...  True  True  True]]\n",
      "[[  316  2194 12416 ... 15999 15998 15998]\n",
      " [  313   570  1641 ... 15998 15999 15999]\n",
      " [ 4901 15998  2842 ... 15999 15999 15999]\n",
      " ...\n",
      " [ 8871  7011  2903 ... 15998 15999 15999]\n",
      " [  329  5480 15998 ... 15999 15999 15999]\n",
      " [15998  1129 15998 ... 15999 15999 15999]]\n"
     ]
    }
   ],
   "source": [
    "MASK_OUT_FREQ = 0.2\n",
    "\n",
    "mask = np.ones_like(dataset_tokens, dtype = bool).flatten()\n",
    "mask[:int(MASK_OUT_FREQ * mask.size)] = False\n",
    "np.random.shuffle(mask)\n",
    "mask = mask.reshape(dataset_tokens.shape)\n",
    "print(mask)\n",
    "\n",
    "masked_dataset_tokens = np.where(mask, dataset_tokens, MASK_TOKEN)\n",
    "print(masked_dataset_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 250)]             0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 250, 768)          97737216  \n",
      "_________________________________________________________________\n",
      "token_guess (Dense)          (None, 250, 16000)        12304000  \n",
      "=================================================================\n",
      "Total params: 110,041,216\n",
      "Trainable params: 110,041,216\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "l_bert = BertModelLayer(**BertModelLayer.Params(\n",
    "    \n",
    "    # embedding params  \n",
    "    vocab_size               = VOCAB_SIZE,        \n",
    "    use_token_type           = True,\n",
    "    use_position_embeddings  = True,\n",
    "    token_type_vocab_size    = 2,\n",
    "\n",
    "    # transformer encoder params\n",
    "    num_heads                = 12,\n",
    "    num_layers               = 12,           \n",
    "    hidden_size              = 768,\n",
    "    hidden_dropout           = 0.1,\n",
    "    intermediate_size        = 4 * 768,\n",
    "    intermediate_activation  = \"gelu\",\n",
    "\n",
    "    # see arXiv:1902.00751 (adapter-BERT)\n",
    "    adapter_size             = None,         \n",
    "\n",
    "    # True for ALBERT (arXiv:1909.11942)\n",
    "    shared_layer             = False,\n",
    "    # None for BERT, wordpiece embedding size for ALBERT\n",
    "    embedding_size           = None,   \n",
    "\n",
    "    # any other Keras layer params\n",
    "    name                     = \"bert\",    \n",
    "))\n",
    "\n",
    "l_input_ids = keras.layers.Input(shape = (MAX_LEN,), dtype = np.int32)\n",
    "# shape: (batch_size, max_len, hidden_size)\n",
    "bert_output = l_bert(l_input_ids)\n",
    "# shape: (batch_size, max_len, vocab_size)\n",
    "token_guess_output = keras.layers.Dense(VOCAB_SIZE, activation = 'softmax', name = 'token_guess')(bert_output)\n",
    "\n",
    "model = keras.Model(inputs = l_input_ids, outputs = token_guess_output)\n",
    "model.build(input_shape = (None, MAX_LEN))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some pre-trained weights\n",
    "bert.load_bert_weights(l_bert, \\\n",
    "        '/cs/phd/nadavb/cafa_project/data/bret_pretrained_model/multi_cased_L-12_H-768_A-12/bert_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_FILE = '/cs/phd/nadavb/cafa_project/data/protobret_weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously trained weights\n",
    "model.load_weights(WEIGHTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_bert.trainable = True\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr = 1e-06, amsgrad = True)\n",
    "model.compile(optimizer = optimizer, loss = 'sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 292s 146ms/sample - loss: 0.6268\n",
      "Epoch 2:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 290s 145ms/sample - loss: 0.6311\n",
      "Epoch 3:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 290s 145ms/sample - loss: 0.6274\n",
      "Epoch 4:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 290s 145ms/sample - loss: 0.6467\n",
      "Epoch 5:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 291s 145ms/sample - loss: 0.6283\n",
      "Epoch 6:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 290s 145ms/sample - loss: 0.6224\n",
      "Epoch 7:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 291s 146ms/sample - loss: 0.6411\n",
      "Epoch 8:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 290s 145ms/sample - loss: 0.6185\n",
      "Epoch 9:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 317s 158ms/sample - loss: 0.6372\n",
      "Epoch 10:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 290s 145ms/sample - loss: 0.6262\n",
      "Epoch 11:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 291s 145ms/sample - loss: 0.6362\n",
      "Epoch 12:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 291s 145ms/sample - loss: 0.6302\n",
      "Epoch 13:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 290s 145ms/sample - loss: 0.6268\n",
      "Epoch 14:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 291s 145ms/sample - loss: 0.6304\n",
      "Epoch 15:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 290s 145ms/sample - loss: 0.6179\n",
      "Epoch 16:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 290s 145ms/sample - loss: 0.6385\n",
      "Epoch 17:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 291s 146ms/sample - loss: 0.6333\n",
      "Epoch 18:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 291s 145ms/sample - loss: 0.6152\n",
      "Epoch 19:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 291s 146ms/sample - loss: 0.6211\n",
      "Epoch 20:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 290s 145ms/sample - loss: 0.6372\n",
      "Epoch 21:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 315s 157ms/sample - loss: 0.6411\n",
      "Epoch 22:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 290s 145ms/sample - loss: 0.6314\n",
      "Epoch 23:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 290s 145ms/sample - loss: 0.6242\n",
      "Epoch 24:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 291s 146ms/sample - loss: 0.6350\n",
      "Epoch 25:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 291s 145ms/sample - loss: 0.6267\n",
      "Epoch 26:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 291s 145ms/sample - loss: 0.6285\n",
      "Epoch 27:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 292s 146ms/sample - loss: 0.6210\n",
      "Epoch 28:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 290s 145ms/sample - loss: 0.6410\n",
      "Epoch 29:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 290s 145ms/sample - loss: 0.6285\n",
      "Epoch 30:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 291s 145ms/sample - loss: 0.6277\n",
      "Epoch 31:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 291s 145ms/sample - loss: 0.6424\n",
      "Epoch 32:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 291s 146ms/sample - loss: 0.6398\n",
      "Epoch 33:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 313s 157ms/sample - loss: 0.6403\n",
      "Epoch 34:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 294s 147ms/sample - loss: 0.6378\n",
      "Epoch 35:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 290s 145ms/sample - loss: 0.6105\n",
      "Epoch 36:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 290s 145ms/sample - loss: 0.6155\n",
      "Epoch 37:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 290s 145ms/sample - loss: 0.6266\n",
      "Epoch 38:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 291s 145ms/sample - loss: 0.6173\n",
      "Epoch 39:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 291s 145ms/sample - loss: 0.6047\n",
      "Epoch 40:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 290s 145ms/sample - loss: 0.6239\n",
      "Epoch 41:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 290s 145ms/sample - loss: 0.6265\n",
      "Epoch 42:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 290s 145ms/sample - loss: 0.6240\n",
      "Epoch 43:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 290s 145ms/sample - loss: 0.6321\n",
      "Epoch 44:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 292s 146ms/sample - loss: 0.6244\n",
      "Epoch 45:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 290s 145ms/sample - loss: 0.6360\n",
      "Epoch 46:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 314s 157ms/sample - loss: 0.6378\n",
      "Epoch 47:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 295s 148ms/sample - loss: 0.6441\n",
      "Epoch 48:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 291s 145ms/sample - loss: 0.6271\n",
      "Epoch 49:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 293s 146ms/sample - loss: 0.6222\n",
      "Epoch 50:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 292s 146ms/sample - loss: 0.6206\n",
      "Epoch 51:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 296s 148ms/sample - loss: 0.6336\n",
      "Epoch 52:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 294s 147ms/sample - loss: 0.6248\n",
      "Epoch 53:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 291s 146ms/sample - loss: 0.6512\n",
      "Epoch 54:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 292s 146ms/sample - loss: 0.6413\n",
      "Epoch 55:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 296s 148ms/sample - loss: 0.6261\n",
      "Epoch 56:\n",
      "Train on 2000 samples\n",
      "2000/2000 [==============================] - 297s 148ms/sample - loss: 0.6308\n",
      "Epoch 57:\n",
      "Train on 2000 samples\n",
      " 378/2000 [====>.........................] - ETA: 4:00 - loss: 0.6063"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-fc000bd48cf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mepoch_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasked_dataset_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mepoch_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cs/labs/michall/nadavb/my_python3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/cs/labs/michall/nadavb/my_python3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cs/labs/michall/nadavb/my_python3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cs/labs/michall/nadavb/my_python3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cs/labs/michall/nadavb/my_python3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cs/labs/michall/nadavb/my_python3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cs/labs/michall/nadavb/my_python3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cs/labs/michall/nadavb/my_python3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cs/labs/michall/nadavb/my_python3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cs/labs/michall/nadavb/my_python3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/cs/labs/michall/nadavb/my_python3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1000\n",
    "EPOCH_SIZE = 2000\n",
    "\n",
    "for i in range(N_EPOCHS):\n",
    "    \n",
    "    print('Epoch %d:' % (i + 1))\n",
    "    \n",
    "    epoch_mask = np.random.randint(0, len(dataset_tokens), EPOCH_SIZE)\n",
    "    epoch_X = masked_dataset_tokens[epoch_mask, :]\n",
    "    epoch_Y = dataset_tokens[epoch_mask, :]\n",
    "    model.fit(epoch_X, epoch_Y, batch_size = 3)\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        model.save_weights(WEIGHTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cs/labs/michall/nadavb/cafa_project/data\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "%cd /cs/phd/nadavb/cafa_project/data\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('protopiece.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁MSK GLY ?DIP? ?SWA? ?TTE? TRT LAKL AGVE RLFE PQY ?MAL? ?QAG? VEK GEN LVV AAP ?TGSGKT? FI ALVA IVN SLAR AGGR AFY LVP LKS ?VAY? ?EKY? ?TSF? SILS RMG ?LKLK? ISVG DFR ?EGP? PEAP VVIA TYE ?KFD? SLLR VSP SLA RNV ?SVL? IVDE IHS ?VS? DPK RGP ?ILES? ?IVS? ?RML? ASAG EAQ ?LVGL? SA TVP ?NAG? EIAE WIG GKIV ES SWR ?PVP? LRE YVF KEY KLY SPTG GLR EVP RVY GLY DLDL AAEA IED GGQ ALV ?FTY? SRRR AVT LAKR AAK RLGR RLS SRE ARV YSA EASR AEGA PRS VAEE LASL IAAG ?IA? ?YHH? AGL ?PPS? LRK TVE EAFR AGAV ?KVV? YST PTL AAGV ?NLP? ARR VV ?IDS? ?YYR? ?YEA? GFR EPI ?RVAE? YKQ MAG RAGR PGL DEF GEAI IVA ERLD ?RPED? ?LIS? GYI RAP PERV ESR ?LAGL? ?RGLR? HF ILGI VA PEGE VS IGSI EKV SGLT LYSL QRG LPR ETI ARA VEDL SAW GLV ?EVK? GWR IAA TSL GREV ?AAV? YLD PESV ?PVF? ?REEV? KHL SFD NEF DIL ?YL? IST MPD MVR LPA ?TRR? ?EEE? RLLE ?AI? LDA SPR ?MLS? SVD WLG PEE ?MAA? ?VKT? AVVL KLW IDEA SED TIY GEW GVH TGDL LNM VST AEW IASG LSR IAP YLG ?LNS? KVS HIL SVI ARR ?IKH? ?GVK? PELL QLVE IPGV GRV RAR IL FEA ?GYR? SIED LATA ?RAE? DLM RLP LIG PST ?AR? QILE FLGR VDEA REA EARE MLA RKG LL SYL EGD AVAG EEGE / / / ?/? ?/? ?/? / ?/? / / ?/? / / / / / / ?/? / / ?/? ?/? /\n",
      "▁MSK GLY VA    VA    VA    TRT LAKL AGVE RLFE PQY LLA   AAA   VEK GEN LVV AAP GAGKST   FI ALVA IVN SLAR AGGR AFY LVP LKS AAA   AAA   AAA   SILS RMG AQA    ISVG DFR VA    PEAP VVIA TYE AAA   SLLR VSP SLA RNV VA    IVDE IHS VA   DPK RGP VA     AIA   AIA   ASAG EAQ EALR   SA TVP VA    EIAE WIG GKIV ES SWR AQA   LRE YVF KEY KLY SPTG GLR EVP RVY GLY DLDL AAEA IED GGQ ALV LAH   SRRR AVT LAKR AAK RLGR RLS SRE ARV YSA EASR AEGA PRS VAEE LASL IAAG VA   VA    AGL VA    LRK TVE EAFR AGAV VA    YST PTL AAGV VA    ARR VV EALR  AAA   VA    GFR EPI VA     YKQ MAG RAGR PGL DEF GEAI IVA ERLD AVA    AAA   GYI RAP PERV ESR AAA    AGAD   HF ILGI VA PEGE VS IGSI EKV SGLT LYSL QRG LPR ETI ARA VEDL SAW GLV AAE   GWR IAA TSL GREV VA    YLD PESV VA    AA     KHL SFD NEF DIL AAA  IST MPD MVR LPA VA    VA    RLLE VA   LDA SPR VA    SVD WLG PEE VA    AAR   AVVL KLW IDEA SED TIY GEW GVH TGDL LNM VST AEW IASG LSR IAP YLG AGAD  KVS HIL SVI ARR VA    VA    PELL QLVE IPGV GRV RAR IL FEA VA    SIED LATA AAA   DLM RLP LIG PST VA   QILE FLGR VDEA REA EARE MLA RKG LL SYL EGD AVAG EEGE / / / /   /   /   / /   / / /   / / / / / / /   / / /   /   /\n"
     ]
    }
   ],
   "source": [
    "def format_token_id(token_id):\n",
    "    if token_id == PAD_TOKEN:\n",
    "        return '/'\n",
    "    elif token_id == MASK_TOKEN:\n",
    "        return '?'\n",
    "    else:\n",
    "        return sp.id_to_piece(int(token_id))\n",
    "\n",
    "def pad_to_max_len(*strings):\n",
    "    max_len = max(map(len, strings))\n",
    "    return [string + (max_len - len(string)) * ' ' for string in strings]\n",
    "\n",
    "def display_model_result(i = 0):\n",
    "    \n",
    "    original_token_ids = dataset_tokens[i, :]\n",
    "    used_mask = mask[i, :]\n",
    "    masked_totken_ids = masked_dataset_tokens[i, :]\n",
    "    predicted_token_ids = model.predict(masked_totken_ids.reshape(1, -1))[0, :, :].argmax(axis = -1)\n",
    "    \n",
    "#     print(np.concatenate([original_token_ids.reshape(-1, 1), masked_totken_ids.reshape(-1, 1), \\\n",
    "#             predicted_token_ids.reshape(-1, 1)], axis = 1))\n",
    "    \n",
    "    original_formatted_tokens = []\n",
    "    predicted_formatted_tokens = []\n",
    "    \n",
    "    for original_token_id, mask_bit, predicted_token_id in zip(original_token_ids, used_mask, predicted_token_ids):\n",
    "        mask_surrounding = '' if mask_bit else '?'\n",
    "        original_formatted_token, predicted_formatted_token = pad_to_max_len(mask_surrounding + \\\n",
    "                format_token_id(original_token_id) + mask_surrounding, format_token_id(predicted_token_id))\n",
    "        original_formatted_tokens.append(original_formatted_token)\n",
    "        predicted_formatted_tokens.append(predicted_formatted_token)\n",
    "        \n",
    "    print(' '.join(original_formatted_tokens) + '\\n' + ' '.join(predicted_formatted_tokens))\n",
    "\n",
    "display_model_result(i = 99999)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
