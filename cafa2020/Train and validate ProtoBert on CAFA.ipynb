{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from pwas.shared_utils.util import start_log, log, get_chunk_intervals\n",
    "    \n",
    "def mkdir_if_not_exists(directory):\n",
    "    if not os.path.isdir(directory):\n",
    "        os.mkdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating log file: protobert_pretraining__22630__2020_08_30-15:11:18.txt\n"
     ]
    }
   ],
   "source": [
    "start_log('/cs/phd/nadavb/logs', 'protobert_pretraining')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/nadavb/protobert\" target=\"_blank\">https://app.wandb.ai/nadavb/protobert</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/nadavb/protobert/runs/2t9mt9hp\" target=\"_blank\">https://app.wandb.ai/nadavb/protobert/runs/2t9mt9hp</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "wandb: Wandb version 0.9.6 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "W&B Run: https://app.wandb.ai/nadavb/protobert/runs/2t9mt9hp"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project = 'protobert', dir = '/cs/phd/nadavb/my_storage/wandb')#, resume = 'expert-dragon-23')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8943 unique annotations.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "H5_FILE_PATH = '/cs/phd/nadavb/cafa_project/data/dataset.h5'\n",
    "BASE_WEIGHTS_DIR = '/cs/phd/nadavb/cafa_project/data/model_weights'\n",
    "\n",
    "run_weights_dir = os.path.join(BASE_WEIGHTS_DIR, wandb.run.name)\n",
    "mkdir_if_not_exists(run_weights_dir)\n",
    "auto_save_weights_dir = os.path.join(run_weights_dir, 'autosave')\n",
    "mkdir_if_not_exists(auto_save_weights_dir)\n",
    "\n",
    "with h5py.File(H5_FILE_PATH, 'r') as h5f:\n",
    "    \n",
    "    included_annotation_indices = h5f['included_annotation_indices'][:]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(np.log10(h5f['seq_lengths'][:]), bins = 100)\n",
    "    ax.set_xlabel('Log10 sequence length')\n",
    "    ax.set_ylabel('# samples')\n",
    "    ax.set_title('Total: %d samples' % len(h5f['seq_lengths']))\n",
    "    \n",
    "n_annotations = len(included_annotation_indices)\n",
    "print('%d unique annotations.' % n_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class OneHotEncoding(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, vocab_size, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        super(OneHotEncoding, self).__init__(**kwargs)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape + (self.vocab_size,)\n",
    "\n",
    "    def call(self, X):\n",
    "        return K.one_hot(X, self.vocab_size)\n",
    "\n",
    "class GlobalAttention(keras.layers.Layer):\n",
    "    \n",
    "    '''\n",
    "    Recevies two inputs:\n",
    "    1. A global representation (of some fixed dimension)\n",
    "    2. A sequence (of any length, and some fixed dimension)\n",
    "    The global representation is used to construct a global query that attends to all the positions in the sequence (independently\n",
    "    for any of the heads).\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_heads, d_key, d_value, **kwargs):\n",
    "        self.n_heads = n_heads\n",
    "        self.d_key = d_key\n",
    "        self.sqrt_d_key = np.sqrt(self.d_key)\n",
    "        self.d_value = d_value\n",
    "        self.d_output = n_heads * d_value\n",
    "        super(GlobalAttention, self).__init__(**kwargs)\n",
    "        \n",
    "    def compute_output_shape(self, input_shapes):\n",
    "        # input_shapes: (batch_size, d_global_input), (batch_size, length, d_seq_input)\n",
    "        print('compute_output_shape', input_shapes) # XXX\n",
    "        (batch_size, _), _ = input_shapes\n",
    "        return (batch_size, self.d_output)\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        # input_shapes: (batch_size, d_global_input), (batch_size, length, d_seq_input)\n",
    "        (_, self.d_global_input), (_, _, self.d_seq_input) = input_shapes\n",
    "        # Wq: (n_heads, d_global_input, d_key)\n",
    "        self.Wq = self.add_weight(name = 'Wq', shape = (self.n_heads, self.d_global_input, self.d_key), \\\n",
    "                initializer = 'glorot_uniform', trainable = True)\n",
    "        # Wk: (n_heads, d_seq_input, d_key)\n",
    "        self.Wk = self.add_weight(name = 'Wk', shape = (self.n_heads, self.d_seq_input, self.d_key), \\\n",
    "                initializer = 'glorot_uniform', trainable = True)\n",
    "        # Wv: (n_heads, d_seq_input, d_value)\n",
    "        self.Wv = self.add_weight(name = 'Wv', shape = (self.n_heads, self.d_seq_input, self.d_value), \\\n",
    "                initializer = 'glorot_uniform', trainable = True)\n",
    "        super(GlobalAttention, self).build(input_shapes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # X: (batch_size, d_global_input)\n",
    "        # S: (batch_size, length, d_seq_input)\n",
    "        X, S = inputs\n",
    "        _, length, _ = K.int_shape(S)\n",
    "                \n",
    "        # (batch_size, n_heads, d_key)\n",
    "        QX = K.tanh(K.dot(X, self.Wq))\n",
    "        # (batch_size * n_heads, d_key)\n",
    "        QX_batched_heads = K.reshape(QX, (-1, self.d_key))\n",
    "        \n",
    "        # (batch_size, n_heads, d_key, length)\n",
    "        KS = K.permute_dimensions(K.tanh(K.dot(S, self.Wk)), (0, 2, 3, 1))\n",
    "        # (batch_size * n_heads, d_key, length)\n",
    "        KS_batched_heads = K.reshape(KS, (-1, self.d_key, length))\n",
    "\n",
    "        # (batch_size, n_heads, length, d_value)\n",
    "        VS = K.permute_dimensions(K.relu(K.dot(S, self.Wv)), (0, 2, 1, 3))\n",
    "        # (batch_size * n_heads, length, d_value)\n",
    "        VS_batched_heads = K.reshape(VS, (-1, length, self.d_value))\n",
    "                \n",
    "        # (batch_size * n_heads, length)\n",
    "        Z_batched_heads = K.softmax(K.batch_dot(QX_batched_heads, KS_batched_heads) / self.sqrt_d_key)\n",
    "        # (batch_size * n_heads, d_value)\n",
    "        Y_batched_heads = K.batch_dot(Z_batched_heads, VS_batched_heads)\n",
    "        # (batch_size, n_heads * d_value)\n",
    "        Y = K.reshape(Y_batched_heads, (-1, self.d_output))\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "def _create_model(seq_len, vocab_size, n_annotations, d_hidden_seq = 512, d_hidden_global = 2048, n_blocks = 6, n_heads = 8, \\\n",
    "         d_key = 64, conv_kernel_size = 9, wide_conv_dilation_rate = 5):\n",
    "    \n",
    "    '''\n",
    "    seq_len is required to create the model, but all the weights are independent of the length and can be re-used with\n",
    "    different lengths.\n",
    "    '''\n",
    "    \n",
    "    assert d_hidden_global % n_heads == 0\n",
    "    d_value = d_hidden_global // n_heads\n",
    "    \n",
    "    input_seq = keras.layers.Input(shape = (seq_len,), dtype = np.int32, name = 'input-seq')\n",
    "    input_annoatations = keras.layers.Input(shape = (n_annotations,), dtype = np.float32, name = 'input-annotations')\n",
    "    \n",
    "    encoded_input_seq = OneHotEncoding(vocab_size, name = 'input-seq-encoding')(input_seq)\n",
    "    \n",
    "    hidden_seq = keras.layers.Dense(d_hidden_seq, activation = 'relu', name = 'dense-seq-input')(encoded_input_seq)\n",
    "    hidden_global = keras.layers.Dense(d_hidden_global, activation = 'relu', name = 'dense-global-input')(input_annoatations)\n",
    "    \n",
    "    for block_index in range(1, n_blocks + 1):\n",
    "        \n",
    "        seqed_global = keras.layers.Dense(d_hidden_seq, activation = 'relu', name = 'global-to-seq-dense-block%d' % block_index)(hidden_global)\n",
    "        seqed_global = keras.layers.Reshape((1, d_hidden_seq), name = 'global-to-seq-reshape-block%d' % block_index)(seqed_global)\n",
    "        \n",
    "        narrow_conv_seq = keras.layers.Conv1D(filters = d_hidden_seq, kernel_size = conv_kernel_size, strides = 1, \\\n",
    "                padding = 'same', dilation_rate = 1, activation = 'relu', name = 'narrow-conv-block%d' % block_index)(hidden_seq)\n",
    "        wide_conv_seq = keras.layers.Conv1D(filters = d_hidden_seq, kernel_size = conv_kernel_size, strides = 1, \\\n",
    "                padding = 'same', dilation_rate = wide_conv_dilation_rate, activation = 'relu', name = 'wide-conv-block%d' % \\\n",
    "                block_index)(hidden_seq)\n",
    "        \n",
    "        hidden_seq = keras.layers.Add(name = 'seq-merge1-block%d' % block_index)([hidden_seq, seqed_global, narrow_conv_seq, wide_conv_seq])\n",
    "        hidden_seq = keras.layers.LayerNormalization(name = 'seq-merge1-norm-block%d' % block_index)(hidden_seq)\n",
    "        \n",
    "        dense_seq = keras.layers.Dense(d_hidden_seq, activation = 'relu', name = 'seq-dense-block%d' % block_index)(hidden_seq)\n",
    "        hidden_seq = keras.layers.Add(name = 'seq-merge2-block%d' % block_index)([hidden_seq, dense_seq])\n",
    "        hidden_seq = keras.layers.LayerNormalization(name = 'seq-merge2-norm-block%d' % block_index)(hidden_seq)\n",
    "        \n",
    "        dense_global = keras.layers.Dense(d_hidden_global, activation = 'relu', name = 'global-dense1-block%d' % block_index)(hidden_global)\n",
    "        attention = GlobalAttention(n_heads, d_key, d_value, name = 'global-attention-block%d' % block_index)([hidden_global, hidden_seq])\n",
    "        hidden_global = keras.layers.Add(name = 'global-merge1-block%d' % block_index)([hidden_global, dense_global, attention])\n",
    "        hidden_global = keras.layers.LayerNormalization(name = 'global-merge1-norm-block%d' % block_index)(hidden_global)\n",
    "        \n",
    "        dense_global = keras.layers.Dense(d_hidden_global, activation = 'relu', name = 'global-dense2-block%d' % block_index)(hidden_global)\n",
    "        hidden_global = keras.layers.Add(name = 'global-merge2-block%d' % block_index)([hidden_global, dense_global])\n",
    "        hidden_global = keras.layers.LayerNormalization(name = 'global-merge2-norm-block%d' % block_index)(hidden_global)\n",
    "        \n",
    "    output_seq = keras.layers.Dense(vocab_size, activation = 'softmax', name = 'output-seq')(hidden_seq)\n",
    "    output_annotations = keras.layers.Dense(n_annotations, activation = 'sigmoid', name = 'output-annotations')(hidden_global)\n",
    "\n",
    "    return keras.models.Model(inputs = [input_seq, input_annoatations], outputs = [output_seq, output_annotations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020_08_30-17:56:17] Starting with episode with max_seq_len = 450.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input-seq (InputLayer)          [(None, 450)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input-annotations (InputLayer)  [(None, 8943)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input-encoding (OneHotEncoding) (None, 450, 26)      0           input-seq[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense-global-input (Dense)      (None, 2048)         18317312    input-annotations[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense-seq-input (Dense)         (None, 450, 512)     13824       input-encoding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global-to-seq-dense-block1 (Den (None, 512)          1049088     dense-global-input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "global-to-seq-reshape-block1 (R (None, 1, 512)       0           global-to-seq-dense-block1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "narrow-conv-block1 (Conv1D)     (None, 450, 512)     2359808     dense-seq-input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "wide-conv-block1 (Conv1D)       (None, 450, 512)     2359808     dense-seq-input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge1-block1 (Add)         (None, 450, 512)     0           dense-seq-input[0][0]            \n",
      "                                                                 global-to-seq-reshape-block1[0][0\n",
      "                                                                 narrow-conv-block1[0][0]         \n",
      "                                                                 wide-conv-block1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge1-norm-block1 (LayerNo (None, 450, 512)     1024        seq-merge1-block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "seq-dense-block1 (Dense)        (None, 450, 512)     262656      seq-merge1-norm-block1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge2-block1 (Add)         (None, 450, 512)     0           seq-merge1-norm-block1[0][0]     \n",
      "                                                                 seq-dense-block1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge2-norm-block1 (LayerNo (None, 450, 512)     1024        seq-merge2-block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global-dense1-block1 (Dense)    (None, 2048)         4196352     dense-global-input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "global-attention-block1 (Global (None, 2048)         2359296     dense-global-input[0][0]         \n",
      "                                                                 seq-merge2-norm-block1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global-merge1-block1 (Add)      (None, 2048)         0           dense-global-input[0][0]         \n",
      "                                                                 global-dense1-block1[0][0]       \n",
      "                                                                 global-attention-block1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "global-merge1-norm-block1 (Laye (None, 2048)         4096        global-merge1-block1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global-dense2-block1 (Dense)    (None, 2048)         4196352     global-merge1-norm-block1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "global-merge2-block1 (Add)      (None, 2048)         0           global-merge1-norm-block1[0][0]  \n",
      "                                                                 global-dense2-block1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global-merge2-norm-block1 (Laye (None, 2048)         4096        global-merge2-block1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global-to-seq-dense-block2 (Den (None, 512)          1049088     global-merge2-norm-block1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "global-to-seq-reshape-block2 (R (None, 1, 512)       0           global-to-seq-dense-block2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "narrow-conv-block2 (Conv1D)     (None, 450, 512)     2359808     seq-merge2-norm-block1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "wide-conv-block2 (Conv1D)       (None, 450, 512)     2359808     seq-merge2-norm-block1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge1-block2 (Add)         (None, 450, 512)     0           seq-merge2-norm-block1[0][0]     \n",
      "                                                                 global-to-seq-reshape-block2[0][0\n",
      "                                                                 narrow-conv-block2[0][0]         \n",
      "                                                                 wide-conv-block2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge1-norm-block2 (LayerNo (None, 450, 512)     1024        seq-merge1-block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "seq-dense-block2 (Dense)        (None, 450, 512)     262656      seq-merge1-norm-block2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge2-block2 (Add)         (None, 450, 512)     0           seq-merge1-norm-block2[0][0]     \n",
      "                                                                 seq-dense-block2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge2-norm-block2 (LayerNo (None, 450, 512)     1024        seq-merge2-block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global-dense1-block2 (Dense)    (None, 2048)         4196352     global-merge2-norm-block1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "global-attention-block2 (Global (None, 2048)         2359296     global-merge2-norm-block1[0][0]  \n",
      "                                                                 seq-merge2-norm-block2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global-merge1-block2 (Add)      (None, 2048)         0           global-merge2-norm-block1[0][0]  \n",
      "                                                                 global-dense1-block2[0][0]       \n",
      "                                                                 global-attention-block2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "global-merge1-norm-block2 (Laye (None, 2048)         4096        global-merge1-block2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global-dense2-block2 (Dense)    (None, 2048)         4196352     global-merge1-norm-block2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "global-merge2-block2 (Add)      (None, 2048)         0           global-merge1-norm-block2[0][0]  \n",
      "                                                                 global-dense2-block2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global-merge2-norm-block2 (Laye (None, 2048)         4096        global-merge2-block2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global-to-seq-dense-block3 (Den (None, 512)          1049088     global-merge2-norm-block2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "global-to-seq-reshape-block3 (R (None, 1, 512)       0           global-to-seq-dense-block3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "narrow-conv-block3 (Conv1D)     (None, 450, 512)     2359808     seq-merge2-norm-block2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "wide-conv-block3 (Conv1D)       (None, 450, 512)     2359808     seq-merge2-norm-block2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge1-block3 (Add)         (None, 450, 512)     0           seq-merge2-norm-block2[0][0]     \n",
      "                                                                 global-to-seq-reshape-block3[0][0\n",
      "                                                                 narrow-conv-block3[0][0]         \n",
      "                                                                 wide-conv-block3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge1-norm-block3 (LayerNo (None, 450, 512)     1024        seq-merge1-block3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "seq-dense-block3 (Dense)        (None, 450, 512)     262656      seq-merge1-norm-block3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge2-block3 (Add)         (None, 450, 512)     0           seq-merge1-norm-block3[0][0]     \n",
      "                                                                 seq-dense-block3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge2-norm-block3 (LayerNo (None, 450, 512)     1024        seq-merge2-block3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global-dense1-block3 (Dense)    (None, 2048)         4196352     global-merge2-norm-block2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "global-attention-block3 (Global (None, 2048)         2359296     global-merge2-norm-block2[0][0]  \n",
      "                                                                 seq-merge2-norm-block3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global-merge1-block3 (Add)      (None, 2048)         0           global-merge2-norm-block2[0][0]  \n",
      "                                                                 global-dense1-block3[0][0]       \n",
      "                                                                 global-attention-block3[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "global-merge1-norm-block3 (Laye (None, 2048)         4096        global-merge1-block3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global-dense2-block3 (Dense)    (None, 2048)         4196352     global-merge1-norm-block3[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "global-merge2-block3 (Add)      (None, 2048)         0           global-merge1-norm-block3[0][0]  \n",
      "                                                                 global-dense2-block3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global-merge2-norm-block3 (Laye (None, 2048)         4096        global-merge2-block3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global-to-seq-dense-block4 (Den (None, 512)          1049088     global-merge2-norm-block3[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "global-to-seq-reshape-block4 (R (None, 1, 512)       0           global-to-seq-dense-block4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "narrow-conv-block4 (Conv1D)     (None, 450, 512)     2359808     seq-merge2-norm-block3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "wide-conv-block4 (Conv1D)       (None, 450, 512)     2359808     seq-merge2-norm-block3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge1-block4 (Add)         (None, 450, 512)     0           seq-merge2-norm-block3[0][0]     \n",
      "                                                                 global-to-seq-reshape-block4[0][0\n",
      "                                                                 narrow-conv-block4[0][0]         \n",
      "                                                                 wide-conv-block4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge1-norm-block4 (LayerNo (None, 450, 512)     1024        seq-merge1-block4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "seq-dense-block4 (Dense)        (None, 450, 512)     262656      seq-merge1-norm-block4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge2-block4 (Add)         (None, 450, 512)     0           seq-merge1-norm-block4[0][0]     \n",
      "                                                                 seq-dense-block4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge2-norm-block4 (LayerNo (None, 450, 512)     1024        seq-merge2-block4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global-dense1-block4 (Dense)    (None, 2048)         4196352     global-merge2-norm-block3[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "global-attention-block4 (Global (None, 2048)         2359296     global-merge2-norm-block3[0][0]  \n",
      "                                                                 seq-merge2-norm-block4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global-merge1-block4 (Add)      (None, 2048)         0           global-merge2-norm-block3[0][0]  \n",
      "                                                                 global-dense1-block4[0][0]       \n",
      "                                                                 global-attention-block4[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "global-merge1-norm-block4 (Laye (None, 2048)         4096        global-merge1-block4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global-dense2-block4 (Dense)    (None, 2048)         4196352     global-merge1-norm-block4[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "global-merge2-block4 (Add)      (None, 2048)         0           global-merge1-norm-block4[0][0]  \n",
      "                                                                 global-dense2-block4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global-merge2-norm-block4 (Laye (None, 2048)         4096        global-merge2-block4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global-to-seq-dense-block5 (Den (None, 512)          1049088     global-merge2-norm-block4[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "global-to-seq-reshape-block5 (R (None, 1, 512)       0           global-to-seq-dense-block5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "narrow-conv-block5 (Conv1D)     (None, 450, 512)     2359808     seq-merge2-norm-block4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "wide-conv-block5 (Conv1D)       (None, 450, 512)     2359808     seq-merge2-norm-block4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge1-block5 (Add)         (None, 450, 512)     0           seq-merge2-norm-block4[0][0]     \n",
      "                                                                 global-to-seq-reshape-block5[0][0\n",
      "                                                                 narrow-conv-block5[0][0]         \n",
      "                                                                 wide-conv-block5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge1-norm-block5 (LayerNo (None, 450, 512)     1024        seq-merge1-block5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "seq-dense-block5 (Dense)        (None, 450, 512)     262656      seq-merge1-norm-block5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge2-block5 (Add)         (None, 450, 512)     0           seq-merge1-norm-block5[0][0]     \n",
      "                                                                 seq-dense-block5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge2-norm-block5 (LayerNo (None, 450, 512)     1024        seq-merge2-block5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global-dense1-block5 (Dense)    (None, 2048)         4196352     global-merge2-norm-block4[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "global-attention-block5 (Global (None, 2048)         2359296     global-merge2-norm-block4[0][0]  \n",
      "                                                                 seq-merge2-norm-block5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global-merge1-block5 (Add)      (None, 2048)         0           global-merge2-norm-block4[0][0]  \n",
      "                                                                 global-dense1-block5[0][0]       \n",
      "                                                                 global-attention-block5[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "global-merge1-norm-block5 (Laye (None, 2048)         4096        global-merge1-block5[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global-dense2-block5 (Dense)    (None, 2048)         4196352     global-merge1-norm-block5[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "global-merge2-block5 (Add)      (None, 2048)         0           global-merge1-norm-block5[0][0]  \n",
      "                                                                 global-dense2-block5[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global-merge2-norm-block5 (Laye (None, 2048)         4096        global-merge2-block5[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global-to-seq-dense-block6 (Den (None, 512)          1049088     global-merge2-norm-block5[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "global-to-seq-reshape-block6 (R (None, 1, 512)       0           global-to-seq-dense-block6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "narrow-conv-block6 (Conv1D)     (None, 450, 512)     2359808     seq-merge2-norm-block5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "wide-conv-block6 (Conv1D)       (None, 450, 512)     2359808     seq-merge2-norm-block5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge1-block6 (Add)         (None, 450, 512)     0           seq-merge2-norm-block5[0][0]     \n",
      "                                                                 global-to-seq-reshape-block6[0][0\n",
      "                                                                 narrow-conv-block6[0][0]         \n",
      "                                                                 wide-conv-block6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge1-norm-block6 (LayerNo (None, 450, 512)     1024        seq-merge1-block6[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "seq-dense-block6 (Dense)        (None, 450, 512)     262656      seq-merge1-norm-block6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge2-block6 (Add)         (None, 450, 512)     0           seq-merge1-norm-block6[0][0]     \n",
      "                                                                 seq-dense-block6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "seq-merge2-norm-block6 (LayerNo (None, 450, 512)     1024        seq-merge2-block6[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global-dense1-block6 (Dense)    (None, 2048)         4196352     global-merge2-norm-block5[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "global-attention-block6 (Global (None, 2048)         2359296     global-merge2-norm-block5[0][0]  \n",
      "                                                                 seq-merge2-norm-block6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global-merge1-block6 (Add)      (None, 2048)         0           global-merge2-norm-block5[0][0]  \n",
      "                                                                 global-dense1-block6[0][0]       \n",
      "                                                                 global-attention-block6[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "global-merge1-norm-block6 (Laye (None, 2048)         4096        global-merge1-block6[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global-dense2-block6 (Dense)    (None, 2048)         4196352     global-merge1-norm-block6[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "global-merge2-block6 (Add)      (None, 2048)         0           global-merge1-norm-block6[0][0]  \n",
      "                                                                 global-dense2-block6[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "global-merge2-norm-block6 (Laye (None, 2048)         4096        global-merge2-block6[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "output-seq (Dense)              (None, 450, 26)      13338       seq-merge2-norm-block6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "output-annotations (Dense)      (None, 8943)         18324207    global-merge2-norm-block6[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 137,430,281\n",
      "Trainable params: 137,430,281\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "[2020_08_30-17:56:24] Loaded weights from /cs/phd/nadavb/cafa_project/data/model_weights/fresh-paper-42/epoch_50_sample_600000.pkl.\n",
      "[2020_08_30-17:56:24] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 373.9709375, std: 179.72734667472602, min: 70.0, 25%: 257.0, 50%: 337.0, 75%: 447.0, max: 2367.0\n",
      "[2020_08_30-17:56:26] Epoch 51 (current sample 700000):\n",
      "Train on 6400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "wandb: Wandb version 0.9.6 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400/6400 [==============================] - 81s 13ms/sample - loss: 0.6886 - output-seq_loss: 0.2157 - output-annotations_loss: 4.7287e-04\n",
      "[2020_08_30-17:57:47] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 377.8171875, std: 182.24120862176338, min: 62.0, 25%: 256.0, 50%: 339.0, 75%: 455.0, max: 2321.0\n",
      "[2020_08_30-17:57:49] Epoch 52 (current sample 700000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.7039 - output-seq_loss: 0.2156 - output-annotations_loss: 4.8829e-04\n",
      "[2020_08_30-17:59:08] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 374.47359375, std: 174.7462731171398, min: 65.0, 25%: 258.0, 50%: 338.5, 75%: 450.0, max: 2085.0\n",
      "[2020_08_30-17:59:09] Epoch 53 (current sample 700000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.6925 - output-seq_loss: 0.2145 - output-annotations_loss: 4.7794e-04\n",
      "[2020_08_30-18:00:28] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 374.37609375, std: 175.55300953411313, min: 53.0, 25%: 257.0, 50%: 340.0, 75%: 448.0, max: 2043.0\n",
      "[2020_08_30-18:00:30] Epoch 54 (current sample 700000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.6965 - output-seq_loss: 0.2145 - output-annotations_loss: 4.8195e-04\n",
      "[2020_08_30-18:01:49] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 376.26390625, std: 177.54299513492202, min: 61.0, 25%: 258.0, 50%: 342.0, 75%: 452.0, max: 2373.0\n",
      "[2020_08_30-18:01:50] Epoch 55 (current sample 700000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.7216 - output-seq_loss: 0.2170 - output-annotations_loss: 5.0455e-04\n",
      "[2020_08_30-18:03:09] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 376.12, std: 179.20371430442842, min: 62.0, 25%: 255.0, 50%: 339.0, 75%: 455.0, max: 2368.0\n",
      "[2020_08_30-18:03:10] Epoch 56 (current sample 700000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.6680 - output-seq_loss: 0.2145 - output-annotations_loss: 4.5346e-04\n",
      "[2020_08_30-18:04:30] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 375.7240625, std: 177.02612666633135, min: 65.0, 25%: 257.0, 50%: 338.0, 75%: 455.0, max: 2406.0\n",
      "[2020_08_30-18:04:31] Epoch 57 (current sample 700000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.6863 - output-seq_loss: 0.2151 - output-annotations_loss: 4.7120e-04\n",
      "[2020_08_30-18:06:03] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 376.14171875, std: 176.9475473549686, min: 63.0, 25%: 257.0, 50%: 340.0, 75%: 451.0, max: 2155.0\n",
      "[2020_08_30-18:06:05] Epoch 58 (current sample 800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.6709 - output-seq_loss: 0.2145 - output-annotations_loss: 4.5639e-04\n",
      "[2020_08_30-18:07:23] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 379.33421875, std: 180.12833546674602, min: 58.0, 25%: 262.0, 50%: 343.0, 75%: 454.0, max: 2527.0\n",
      "[2020_08_30-18:07:25] Epoch 59 (current sample 800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.6588 - output-seq_loss: 0.2158 - output-annotations_loss: 4.4299e-04\n",
      "[2020_08_30-18:08:44] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 374.0190625, std: 178.42635982749312, min: 46.0, 25%: 256.0, 50%: 338.0, 75%: 447.0, max: 2059.0\n",
      "[2020_08_30-18:08:45] Epoch 60 (current sample 800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.6587 - output-seq_loss: 0.2147 - output-annotations_loss: 4.4404e-04\n",
      "[2020_08_30-18:11:00] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 378.98765625, std: 176.07245913246763, min: 61.0, 25%: 261.0, 50%: 343.0, 75%: 456.0, max: 1939.0\n",
      "[2020_08_30-18:11:01] Epoch 61 (current sample 800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.6689 - output-seq_loss: 0.2171 - output-annotations_loss: 4.5180e-04\n",
      "[2020_08_30-18:12:19] Starting a new episode with max_seq_len = 100.\n",
      "[2020_08_30-18:12:26] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 149.493828125, std: 79.81798748928554, min: 13.0, 25%: 89.0, 50%: 134.0, 75%: 190.0, max: 779.0\n",
      "[2020_08_30-18:12:29] Epoch 62 (current sample 800000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 42s 3ms/sample - loss: 0.5431 - output-seq_loss: 0.2632 - output-annotations_loss: 2.7988e-04\n",
      "[2020_08_30-18:13:11] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 149.4125, std: 78.65799440858883, min: 13.0, 25%: 90.0, 50%: 134.0, 75%: 191.0, max: 806.0\n",
      "[2020_08_30-18:13:14] Epoch 63 (current sample 800000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.5272 - output-seq_loss: 0.2622 - output-annotations_loss: 2.6501e-04\n",
      "[2020_08_30-18:13:54] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.306796875, std: 77.93615588910471, min: 13.0, 25%: 90.0, 50%: 133.0, 75%: 189.0, max: 711.0\n",
      "[2020_08_30-18:13:57] Epoch 64 (current sample 800000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 40s 3ms/sample - loss: 0.5061 - output-seq_loss: 0.2626 - output-annotations_loss: 2.4346e-04\n",
      "[2020_08_30-18:14:37] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 149.08484375, std: 77.68738897321762, min: 14.0, 25%: 90.0, 50%: 135.0, 75%: 190.0, max: 723.0\n",
      "[2020_08_30-18:14:40] Epoch 65 (current sample 800000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 40s 3ms/sample - loss: 0.5186 - output-seq_loss: 0.2610 - output-annotations_loss: 2.5767e-04\n",
      "[2020_08_30-18:15:20] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 147.560546875, std: 77.84865800953042, min: 13.0, 25%: 89.0, 50%: 132.0, 75%: 188.0, max: 829.0\n",
      "[2020_08_30-18:15:22] Epoch 66 (current sample 800000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 40s 3ms/sample - loss: 0.5123 - output-seq_loss: 0.2611 - output-annotations_loss: 2.5123e-04\n",
      "[2020_08_30-18:16:02] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.94953125, std: 78.42251126084786, min: 13.0, 25%: 90.0, 50%: 134.0, 75%: 189.0, max: 719.0\n",
      "[2020_08_30-18:16:05] Epoch 67 (current sample 800000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 40s 3ms/sample - loss: 0.5149 - output-seq_loss: 0.2600 - output-annotations_loss: 2.5483e-04\n",
      "[2020_08_30-18:16:56] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 147.6990625, std: 76.92796584949198, min: 13.0, 25%: 90.0, 50%: 132.0, 75%: 187.0, max: 870.0\n",
      "[2020_08_30-18:16:59] Epoch 68 (current sample 900000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.5007 - output-seq_loss: 0.2595 - output-annotations_loss: 2.4125e-04\n",
      "[2020_08_30-18:17:39] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 149.7075, std: 78.30002283563287, min: 13.0, 25%: 90.0, 50%: 135.0, 75%: 191.0, max: 752.0\n",
      "[2020_08_30-18:17:42] Epoch 69 (current sample 900000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.5054 - output-seq_loss: 0.2607 - output-annotations_loss: 2.4475e-04\n",
      "[2020_08_30-18:18:22] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.34109375, std: 76.41776035937355, min: 14.0, 25%: 91.0, 50%: 134.5, 75%: 191.0, max: 1086.0\n",
      "[2020_08_30-18:18:25] Epoch 70 (current sample 900000):\n",
      "Train on 12800 samples\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.661629). Check your callbacks.\n",
      "12800/12800 [==============================] - 40s 3ms/sample - loss: 0.5094 - output-seq_loss: 0.2617 - output-annotations_loss: 2.4765e-04\n",
      "[2020_08_30-18:20:14] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 147.015234375, std: 76.67434330101558, min: 13.0, 25%: 89.0, 50%: 131.0, 75%: 188.0, max: 732.0\n",
      "[2020_08_30-18:20:17] Epoch 71 (current sample 1000000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.5080 - output-seq_loss: 0.2600 - output-annotations_loss: 2.4800e-04\n",
      "[2020_08_30-18:20:56] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.90359375, std: 78.44088140448662, min: 13.0, 25%: 91.0, 50%: 134.0, 75%: 189.0, max: 834.0\n",
      "[2020_08_30-18:20:59] Epoch 72 (current sample 1000000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4985 - output-seq_loss: 0.2622 - output-annotations_loss: 2.3627e-04\n",
      "[2020_08_30-18:21:39] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 149.145625, std: 76.76830193387815, min: 13.0, 25%: 91.0, 50%: 135.0, 75%: 190.0, max: 671.0\n",
      "[2020_08_30-18:21:41] Epoch 73 (current sample 1000000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 40s 3ms/sample - loss: 0.5010 - output-seq_loss: 0.2637 - output-annotations_loss: 2.3737e-04\n",
      "[2020_08_30-18:22:34] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.74453125, std: 77.5184643979612, min: 14.0, 25%: 91.0, 50%: 134.0, 75%: 190.0, max: 808.0\n",
      "[2020_08_30-18:22:37] Epoch 74 (current sample 1100000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4904 - output-seq_loss: 0.2619 - output-annotations_loss: 2.2850e-04\n",
      "[2020_08_30-18:23:17] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.788984375, std: 77.30594114020505, min: 13.0, 25%: 90.0, 50%: 134.0, 75%: 191.0, max: 709.0\n",
      "[2020_08_30-18:23:20] Epoch 75 (current sample 1100000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4941 - output-seq_loss: 0.2584 - output-annotations_loss: 2.3562e-04\n",
      "[2020_08_30-18:24:00] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.21765625, std: 76.34427474132661, min: 13.0, 25%: 90.0, 50%: 134.0, 75%: 190.0, max: 786.0\n",
      "[2020_08_30-18:24:02] Epoch 76 (current sample 1100000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 40s 3ms/sample - loss: 0.4937 - output-seq_loss: 0.2601 - output-annotations_loss: 2.3360e-04\n",
      "[2020_08_30-18:24:42] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 149.19703125, std: 78.12950452347161, min: 13.0, 25%: 90.0, 50%: 134.0, 75%: 190.0, max: 1026.0\n",
      "[2020_08_30-18:24:45] Epoch 77 (current sample 1100000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 40s 3ms/sample - loss: 0.4965 - output-seq_loss: 0.2618 - output-annotations_loss: 2.3468e-04\n",
      "[2020_08_30-18:25:37] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.65078125, std: 79.6667089429756, min: 14.0, 25%: 89.0, 50%: 133.0, 75%: 190.0, max: 801.0\n",
      "[2020_08_30-18:25:40] Epoch 78 (current sample 1200000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4869 - output-seq_loss: 0.2608 - output-annotations_loss: 2.2609e-04\n",
      "[2020_08_30-18:26:20] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.752265625, std: 78.47714341625989, min: 13.0, 25%: 90.0, 50%: 132.0, 75%: 190.0, max: 888.0\n",
      "[2020_08_30-18:26:23] Epoch 79 (current sample 1200000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4921 - output-seq_loss: 0.2599 - output-annotations_loss: 2.3216e-04\n",
      "[2020_08_30-18:27:02] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 150.0475, std: 77.55759568532385, min: 13.0, 25%: 91.0, 50%: 135.0, 75%: 191.0, max: 697.0\n",
      "[2020_08_30-18:27:05] Epoch 80 (current sample 1200000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 40s 3ms/sample - loss: 0.4956 - output-seq_loss: 0.2608 - output-annotations_loss: 2.3482e-04\n",
      "[2020_08_30-18:29:02] Starting a new episode with max_seq_len = 450.\n",
      "[2020_08_30-18:29:08] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 373.600625, std: 178.25777289815207, min: 53.0, 25%: 258.0, 50%: 337.0, 75%: 448.0, max: 2442.0\n",
      "[2020_08_30-18:29:10] Epoch 81 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 81s 13ms/sample - loss: 0.6579 - output-seq_loss: 0.2125 - output-annotations_loss: 4.4539e-04\n",
      "[2020_08_30-18:30:31] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 375.81703125, std: 171.25285320767537, min: 68.0, 25%: 258.0, 50%: 340.5, 75%: 455.0, max: 2485.0\n",
      "[2020_08_30-18:30:32] Epoch 82 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.6413 - output-seq_loss: 0.2149 - output-annotations_loss: 4.2640e-04\n",
      "[2020_08_30-18:31:51] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 373.50984375, std: 175.30087964083643, min: 67.0, 25%: 257.0, 50%: 339.0, 75%: 445.0, max: 1802.0\n",
      "[2020_08_30-18:31:53] Epoch 83 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.6514 - output-seq_loss: 0.2136 - output-annotations_loss: 4.3782e-04\n",
      "[2020_08_30-18:33:12] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 375.8025, std: 180.21672908501122, min: 54.0, 25%: 257.0, 50%: 342.0, 75%: 451.0, max: 3429.0\n",
      "[2020_08_30-18:33:14] Epoch 84 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.6470 - output-seq_loss: 0.2136 - output-annotations_loss: 4.3337e-04\n",
      "[2020_08_30-18:34:33] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 374.2378125, std: 174.14339366728294, min: 51.0, 25%: 257.0, 50%: 339.5, 75%: 450.25, max: 2088.0\n",
      "[2020_08_30-18:34:34] Epoch 85 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.6137 - output-seq_loss: 0.2128 - output-annotations_loss: 4.0093e-04\n",
      "[2020_08_30-18:35:53] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 374.2234375, std: 172.91390637006288, min: 77.0, 25%: 255.0, 50%: 338.0, 75%: 450.0, max: 1902.0\n",
      "[2020_08_30-18:35:55] Epoch 86 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.6228 - output-seq_loss: 0.2119 - output-annotations_loss: 4.1094e-04\n",
      "[2020_08_30-18:37:14] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 375.92140625, std: 174.48551993289175, min: 64.0, 25%: 258.0, 50%: 341.0, 75%: 453.0, max: 2148.0\n",
      "[2020_08_30-18:37:15] Epoch 87 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.6304 - output-seq_loss: 0.2133 - output-annotations_loss: 4.1710e-04\n",
      "[2020_08_30-18:38:34] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 379.05796875, std: 188.74619392657445, min: 63.0, 25%: 259.0, 50%: 341.0, 75%: 451.25, max: 3669.0\n",
      "[2020_08_30-18:38:36] Epoch 88 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.6088 - output-seq_loss: 0.2124 - output-annotations_loss: 3.9646e-04\n",
      "[2020_08_30-18:39:55] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 375.8428125, std: 182.1463410450042, min: 61.0, 25%: 257.0, 50%: 336.0, 75%: 448.0, max: 2585.0\n",
      "[2020_08_30-18:39:56] Epoch 89 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.6118 - output-seq_loss: 0.2108 - output-annotations_loss: 4.0104e-04\n",
      "[2020_08_30-18:41:15] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 377.334375, std: 181.92617216951817, min: 66.0, 25%: 260.0, 50%: 340.0, 75%: 447.0, max: 2516.0\n",
      "[2020_08_30-18:41:17] Epoch 90 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.6028 - output-seq_loss: 0.2123 - output-annotations_loss: 3.9050e-04\n",
      "[2020_08_30-18:43:54] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 376.101875, std: 177.7886765196072, min: 67.0, 25%: 258.0, 50%: 341.0, 75%: 450.0, max: 2330.0\n",
      "[2020_08_30-18:43:55] Epoch 91 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.5840 - output-seq_loss: 0.2113 - output-annotations_loss: 3.7276e-04\n",
      "[2020_08_30-18:45:14] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 370.44375, std: 176.010634219462, min: 58.0, 25%: 255.0, 50%: 337.0, 75%: 446.0, max: 2467.0\n",
      "[2020_08_30-18:45:15] Epoch 92 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.5864 - output-seq_loss: 0.2090 - output-annotations_loss: 3.7740e-04\n",
      "[2020_08_30-18:46:34] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 377.26984375, std: 177.56148013569822, min: 51.0, 25%: 259.0, 50%: 341.0, 75%: 453.0, max: 2188.0\n",
      "[2020_08_30-18:46:36] Epoch 93 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.6028 - output-seq_loss: 0.2125 - output-annotations_loss: 3.9026e-04\n",
      "[2020_08_30-18:47:55] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 377.04359375, std: 180.6657224599376, min: 66.0, 25%: 255.0, 50%: 338.0, 75%: 455.0, max: 2458.0\n",
      "[2020_08_30-18:47:56] Epoch 94 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.5828 - output-seq_loss: 0.2116 - output-annotations_loss: 3.7125e-04\n",
      "[2020_08_30-18:49:15] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 376.02390625, std: 178.48959523156984, min: 62.0, 25%: 257.0, 50%: 341.0, 75%: 451.0, max: 2625.0\n",
      "[2020_08_30-18:49:17] Epoch 95 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.5883 - output-seq_loss: 0.2126 - output-annotations_loss: 3.7568e-04\n",
      "[2020_08_30-18:50:36] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 371.288125, std: 173.67627129262925, min: 64.0, 25%: 256.0, 50%: 339.5, 75%: 444.25, max: 1903.0\n",
      "[2020_08_30-18:50:37] Epoch 96 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.5687 - output-seq_loss: 0.2114 - output-annotations_loss: 3.5722e-04\n",
      "[2020_08_30-18:51:56] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 373.75984375, std: 179.46830358721957, min: 56.0, 25%: 257.0, 50%: 339.0, 75%: 448.0, max: 2950.0\n",
      "[2020_08_30-18:51:58] Epoch 97 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.5768 - output-seq_loss: 0.2112 - output-annotations_loss: 3.6557e-04\n",
      "[2020_08_30-18:53:17] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 375.93359375, std: 179.79001781064284, min: 64.0, 25%: 257.0, 50%: 340.0, 75%: 449.25, max: 2484.0\n",
      "[2020_08_30-18:53:18] Epoch 98 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.5752 - output-seq_loss: 0.2107 - output-annotations_loss: 3.6446e-04\n",
      "[2020_08_30-18:54:37] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 375.71453125, std: 179.50384095712622, min: 81.0, 25%: 257.0, 50%: 340.0, 75%: 452.0, max: 2578.0\n",
      "[2020_08_30-18:54:39] Epoch 99 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.5766 - output-seq_loss: 0.2104 - output-annotations_loss: 3.6612e-04\n",
      "[2020_08_30-18:55:58] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 375.57671875, std: 179.4030578179028, min: 68.0, 25%: 259.0, 50%: 341.0, 75%: 450.0, max: 2527.0\n",
      "[2020_08_30-18:55:59] Epoch 100 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.5794 - output-seq_loss: 0.2117 - output-annotations_loss: 3.6777e-04\n",
      "[2020_08_30-18:58:31] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 378.19140625, std: 178.33639666272137, min: 62.0, 25%: 261.0, 50%: 341.0, 75%: 452.0, max: 2170.0\n",
      "[2020_08_30-18:58:32] Epoch 101 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.5689 - output-seq_loss: 0.2126 - output-annotations_loss: 3.5629e-04\n",
      "[2020_08_30-18:59:51] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 373.13859375, std: 176.55097985241693, min: 67.0, 25%: 254.0, 50%: 339.5, 75%: 448.0, max: 2616.0\n",
      "[2020_08_30-18:59:52] Epoch 102 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.5516 - output-seq_loss: 0.2119 - output-annotations_loss: 3.3965e-04\n",
      "[2020_08_30-19:01:11] Starting a new episode with max_seq_len = 1200.\n",
      "[2020_08_30-19:01:17] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 812.9959375, std: 774.1611465055148, min: 129.0, 25%: 449.0, 50%: 652.0, 75%: 946.25, max: 26672.0\n",
      "[2020_08_30-19:01:18] Epoch 103 (current sample 1200000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 108s 34ms/sample - loss: 0.6605 - output-seq_loss: 0.1680 - output-annotations_loss: 4.9250e-04\n",
      "[2020_08_30-19:03:07] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 809.536875, std: 709.3882988637365, min: 133.0, 25%: 455.0, 50%: 659.5, 75%: 941.0, max: 11971.0\n",
      "[2020_08_30-19:03:07] Epoch 104 (current sample 1200000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.6638 - output-seq_loss: 0.1699 - output-annotations_loss: 4.9388e-04\n",
      "[2020_08_30-19:04:53] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 793.9771875, std: 596.9600647901525, min: 118.0, 25%: 446.0, 50%: 657.0, 75%: 922.0, max: 8988.0\n",
      "[2020_08_30-19:04:53] Epoch 105 (current sample 1200000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.6822 - output-seq_loss: 0.1673 - output-annotations_loss: 5.1488e-04\n",
      "[2020_08_30-19:06:39] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 787.204375, std: 611.0206253022722, min: 134.0, 25%: 439.75, 50%: 645.0, 75%: 934.5, max: 9280.0\n",
      "[2020_08_30-19:06:40] Epoch 106 (current sample 1200000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.6644 - output-seq_loss: 0.1664 - output-annotations_loss: 4.9801e-04\n",
      "[2020_08_30-19:08:25] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 794.785, std: 574.3468274805296, min: 139.0, 25%: 450.75, 50%: 658.0, 75%: 940.0, max: 6559.0\n",
      "[2020_08_30-19:08:26] Epoch 107 (current sample 1200000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.6480 - output-seq_loss: 0.1694 - output-annotations_loss: 4.7859e-04\n",
      "[2020_08_30-19:10:11] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 807.8771875, std: 681.8913450243344, min: 111.0, 25%: 441.0, 50%: 648.5, 75%: 955.25, max: 10314.0\n",
      "[2020_08_30-19:10:12] Epoch 108 (current sample 1200000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.6167 - output-seq_loss: 0.1666 - output-annotations_loss: 4.5013e-04\n",
      "[2020_08_30-19:11:57] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 800.7703125, std: 577.7677219588944, min: 138.0, 25%: 457.0, 50%: 660.0, 75%: 961.0, max: 8756.0\n",
      "[2020_08_30-19:11:58] Epoch 109 (current sample 1200000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.6068 - output-seq_loss: 0.1689 - output-annotations_loss: 4.3789e-04\n",
      "[2020_08_30-19:13:43] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 799.7415625, std: 624.5155830185556, min: 155.0, 25%: 448.0, 50%: 650.0, 75%: 947.25, max: 9335.0\n",
      "[2020_08_30-19:13:44] Epoch 110 (current sample 1200000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.6006 - output-seq_loss: 0.1671 - output-annotations_loss: 4.3346e-04\n",
      "[2020_08_30-19:16:44] Starting a new episode with max_seq_len = 450.\n",
      "[2020_08_30-19:16:51] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 375.2721875, std: 173.5186339803098, min: 65.0, 25%: 256.0, 50%: 341.0, 75%: 454.0, max: 1778.0\n",
      "[2020_08_30-19:16:52] Epoch 111 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 81s 13ms/sample - loss: 0.5736 - output-seq_loss: 0.2112 - output-annotations_loss: 3.6248e-04\n",
      "[2020_08_30-19:18:13] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 377.210625, std: 181.02837025084617, min: 65.0, 25%: 260.0, 50%: 341.0, 75%: 450.0, max: 2596.0\n",
      "[2020_08_30-19:18:15] Epoch 112 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.5489 - output-seq_loss: 0.2108 - output-annotations_loss: 3.3806e-04\n",
      "[2020_08_30-19:19:34] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 373.520625, std: 179.79230315395066, min: 60.0, 25%: 258.0, 50%: 336.0, 75%: 447.0, max: 2047.0\n",
      "[2020_08_30-19:19:35] Epoch 113 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.5378 - output-seq_loss: 0.2104 - output-annotations_loss: 3.2743e-04\n",
      "[2020_08_30-19:20:54] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 380.1609375, std: 187.20086154187808, min: 64.0, 25%: 257.0, 50%: 342.0, 75%: 452.0, max: 2807.0\n",
      "[2020_08_30-19:20:56] Epoch 114 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.5445 - output-seq_loss: 0.2115 - output-annotations_loss: 3.3298e-04\n",
      "[2020_08_30-19:22:15] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 375.29140625, std: 174.197533487497, min: 73.0, 25%: 259.0, 50%: 339.0, 75%: 451.0, max: 1964.0\n",
      "[2020_08_30-19:22:16] Epoch 115 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.5455 - output-seq_loss: 0.2117 - output-annotations_loss: 3.3372e-04\n",
      "[2020_08_30-19:23:35] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 373.4109375, std: 169.65048745613598, min: 60.0, 25%: 255.0, 50%: 340.0, 75%: 451.0, max: 1572.0\n",
      "[2020_08_30-19:23:36] Epoch 116 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.5520 - output-seq_loss: 0.2100 - output-annotations_loss: 3.4199e-04\n",
      "[2020_08_30-19:24:55] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 377.1409375, std: 175.4760964556508, min: 47.0, 25%: 259.0, 50%: 343.0, 75%: 453.0, max: 2173.0\n",
      "[2020_08_30-19:24:57] Epoch 117 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.5463 - output-seq_loss: 0.2124 - output-annotations_loss: 3.3390e-04\n",
      "[2020_08_30-19:26:15] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 377.5471875, std: 177.56136605942564, min: 39.0, 25%: 259.0, 50%: 344.0, 75%: 450.0, max: 2016.0\n",
      "[2020_08_30-19:26:17] Epoch 118 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.5467 - output-seq_loss: 0.2119 - output-annotations_loss: 3.3477e-04\n",
      "[2020_08_30-19:27:36] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 373.91, std: 179.09559397139176, min: 60.0, 25%: 255.75, 50%: 337.0, 75%: 446.0, max: 2844.0\n",
      "[2020_08_30-19:27:37] Epoch 119 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.5394 - output-seq_loss: 0.2092 - output-annotations_loss: 3.3015e-04\n",
      "[2020_08_30-19:28:56] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 376.78640625, std: 176.83432721257086, min: 81.0, 25%: 259.0, 50%: 339.5, 75%: 452.0, max: 1875.0\n",
      "[2020_08_30-19:28:57] Epoch 120 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.5436 - output-seq_loss: 0.2112 - output-annotations_loss: 3.3245e-04\n",
      "[2020_08_30-19:31:33] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 375.16828125, std: 177.37436021059466, min: 49.0, 25%: 256.75, 50%: 339.0, 75%: 451.0, max: 2163.0\n",
      "[2020_08_30-19:31:35] Epoch 121 (current sample 1200000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.5285 - output-seq_loss: 0.2101 - output-annotations_loss: 3.1844e-04\n",
      "[2020_08_30-19:32:53] Starting a new episode with max_seq_len = 1200.\n",
      "[2020_08_30-19:33:00] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 829.011875, std: 872.3629961822547, min: 155.0, 25%: 456.0, 50%: 660.0, 75%: 959.0, max: 33552.0\n",
      "[2020_08_30-19:33:00] Epoch 122 (current sample 1200000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 108s 34ms/sample - loss: 0.5652 - output-seq_loss: 0.1685 - output-annotations_loss: 3.9674e-04\n",
      "[2020_08_30-19:34:48] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 802.770625, std: 632.6290172546384, min: 131.0, 25%: 445.0, 50%: 651.0, 75%: 940.0, max: 12301.0\n",
      "[2020_08_30-19:34:49] Epoch 123 (current sample 1200000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.6336 - output-seq_loss: 0.1666 - output-annotations_loss: 4.6700e-04\n",
      "[2020_08_30-19:36:35] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 799.55375, std: 857.2734497599541, min: 125.0, 25%: 450.0, 50%: 648.0, 75%: 926.0, max: 35422.0\n",
      "[2020_08_30-19:36:35] Epoch 124 (current sample 1200000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.5950 - output-seq_loss: 0.1653 - output-annotations_loss: 4.2968e-04\n",
      "[2020_08_30-19:38:21] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 806.1453125, std: 654.6701021249901, min: 117.0, 25%: 450.0, 50%: 653.0, 75%: 948.25, max: 10046.0\n",
      "[2020_08_30-19:38:21] Epoch 125 (current sample 1200000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.5478 - output-seq_loss: 0.1677 - output-annotations_loss: 3.8019e-04\n",
      "[2020_08_30-19:40:07] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 788.5771875, std: 631.8514004314312, min: 140.0, 25%: 448.0, 50%: 648.5, 75%: 934.25, max: 11141.0\n",
      "[2020_08_30-19:40:08] Epoch 126 (current sample 1200000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.5853 - output-seq_loss: 0.1659 - output-annotations_loss: 4.1943e-04\n",
      "[2020_08_30-19:41:53] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 810.094375, std: 627.6466197675586, min: 143.0, 25%: 454.75, 50%: 642.0, 75%: 954.0, max: 10785.0\n",
      "[2020_08_30-19:41:54] Epoch 127 (current sample 1200000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.6178 - output-seq_loss: 0.1678 - output-annotations_loss: 4.4995e-04\n",
      "[2020_08_30-19:43:39] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 793.779375, std: 603.3145978526492, min: 129.0, 25%: 454.0, 50%: 652.5, 75%: 932.0, max: 9931.0\n",
      "[2020_08_30-19:43:40] Epoch 128 (current sample 1200000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.5509 - output-seq_loss: 0.1658 - output-annotations_loss: 3.8508e-04\n",
      "[2020_08_30-19:45:25] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 796.6578125, std: 566.430273610406, min: 119.0, 25%: 449.0, 50%: 657.0, 75%: 938.0, max: 8809.0\n",
      "[2020_08_30-19:45:26] Epoch 129 (current sample 1200000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.5722 - output-seq_loss: 0.1668 - output-annotations_loss: 4.0539e-04\n",
      "[2020_08_30-19:47:12] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 812.3709375, std: 602.274234918995, min: 131.0, 25%: 453.0, 50%: 661.5, 75%: 963.0, max: 9244.0\n",
      "[2020_08_30-19:47:12] Epoch 130 (current sample 1200000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.5764 - output-seq_loss: 0.1688 - output-annotations_loss: 4.0756e-04\n",
      "[2020_08_30-19:49:54] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 813.764375, std: 643.5813175240115, min: 167.0, 25%: 464.0, 50%: 666.0, 75%: 966.25, max: 13288.0\n",
      "[2020_08_30-19:49:55] Epoch 131 (current sample 1200000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 104s 33ms/sample - loss: 0.5767 - output-seq_loss: 0.1691 - output-annotations_loss: 4.0760e-04\n",
      "[2020_08_30-19:51:40] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 808.27625, std: 590.234320616671, min: 155.0, 25%: 451.0, 50%: 663.0, 75%: 968.0, max: 9059.0\n",
      "[2020_08_30-19:51:40] Epoch 132 (current sample 1200000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.5711 - output-seq_loss: 0.1687 - output-annotations_loss: 4.0238e-04\n",
      "[2020_08_30-19:53:26] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 808.580625, std: 808.6149573012024, min: 124.0, 25%: 453.0, 50%: 660.5, 75%: 959.0, max: 29552.0\n",
      "[2020_08_30-19:53:27] Epoch 133 (current sample 1200000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.5433 - output-seq_loss: 0.1676 - output-annotations_loss: 3.7567e-04\n",
      "[2020_08_30-19:55:12] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 802.599375, std: 789.7718325460828, min: 160.0, 25%: 446.75, 50%: 649.0, 75%: 936.0, max: 29486.0\n",
      "[2020_08_30-19:55:13] Epoch 134 (current sample 1200000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.5345 - output-seq_loss: 0.1670 - output-annotations_loss: 3.6753e-04\n",
      "[2020_08_30-19:56:58] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 801.9396875, std: 715.7489281095226, min: 142.0, 25%: 445.0, 50%: 648.0, 75%: 944.0, max: 21288.0\n",
      "[2020_08_30-19:56:59] Epoch 135 (current sample 1200000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.5671 - output-seq_loss: 0.1658 - output-annotations_loss: 4.0129e-04\n",
      "[2020_08_30-19:58:56] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 809.150625, std: 575.982272670235, min: 131.0, 25%: 455.0, 50%: 654.5, 75%: 960.0, max: 6005.0\n",
      "[2020_08_30-19:58:57] Epoch 136 (current sample 1300000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.5251 - output-seq_loss: 0.1684 - output-annotations_loss: 3.5672e-04\n",
      "[2020_08_30-20:00:42] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 800.709375, std: 619.7279737110847, min: 150.0, 25%: 445.0, 50%: 650.0, 75%: 952.25, max: 9859.0\n",
      "[2020_08_30-20:00:43] Epoch 137 (current sample 1300000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.5663 - output-seq_loss: 0.1659 - output-annotations_loss: 4.0047e-04\n",
      "[2020_08_30-20:02:28] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 801.0796875, std: 608.6656791161665, min: 146.0, 25%: 458.0, 50%: 652.5, 75%: 948.25, max: 9847.0\n",
      "[2020_08_30-20:02:29] Epoch 138 (current sample 1300000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.5272 - output-seq_loss: 0.1668 - output-annotations_loss: 3.6045e-04\n",
      "[2020_08_30-20:04:14] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 810.193125, std: 678.8206662444228, min: 145.0, 25%: 451.0, 50%: 654.0, 75%: 975.0, max: 14103.0\n",
      "[2020_08_30-20:04:15] Epoch 139 (current sample 1300000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.5418 - output-seq_loss: 0.1681 - output-annotations_loss: 3.7371e-04\n",
      "[2020_08_30-20:06:01] Starting a new episode with max_seq_len = 450.\n",
      "[2020_08_30-20:06:07] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 378.31203125, std: 181.10086338329185, min: 68.0, 25%: 260.0, 50%: 342.0, 75%: 454.0, max: 2785.0\n",
      "[2020_08_30-20:06:09] Epoch 140 (current sample 1300000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 81s 13ms/sample - loss: 0.5505 - output-seq_loss: 0.2109 - output-annotations_loss: 3.3957e-04\n",
      "[2020_08_30-20:08:46] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 374.31796875, std: 176.50712746747323, min: 62.0, 25%: 257.0, 50%: 338.0, 75%: 450.0, max: 2026.0\n",
      "[2020_08_30-20:08:48] Epoch 141 (current sample 1300000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.5453 - output-seq_loss: 0.2110 - output-annotations_loss: 3.3433e-04\n",
      "[2020_08_30-20:10:06] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 377.91390625, std: 179.01981806213644, min: 55.0, 25%: 261.0, 50%: 342.0, 75%: 452.0, max: 2450.0\n",
      "[2020_08_30-20:10:08] Epoch 142 (current sample 1300000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.5151 - output-seq_loss: 0.2117 - output-annotations_loss: 3.0334e-04\n",
      "[2020_08_30-20:11:26] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 374.08546875, std: 178.0447153145652, min: 58.0, 25%: 255.0, 50%: 337.0, 75%: 447.0, max: 1905.0\n",
      "[2020_08_30-20:11:28] Epoch 143 (current sample 1300000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.5185 - output-seq_loss: 0.2098 - output-annotations_loss: 3.0866e-04\n",
      "[2020_08_30-20:12:47] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 376.908125, std: 178.73777369429934, min: 67.0, 25%: 262.0, 50%: 340.0, 75%: 452.0, max: 3280.0\n",
      "[2020_08_30-20:12:48] Epoch 144 (current sample 1300000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.5112 - output-seq_loss: 0.2109 - output-annotations_loss: 3.0037e-04\n",
      "[2020_08_30-20:14:07] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 375.613125, std: 177.38913307068003, min: 55.0, 25%: 256.0, 50%: 339.0, 75%: 452.0, max: 2041.0\n",
      "[2020_08_30-20:14:09] Epoch 145 (current sample 1300000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.5057 - output-seq_loss: 0.2092 - output-annotations_loss: 2.9655e-04\n",
      "[2020_08_30-20:15:28] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 374.29546875, std: 176.7986885047735, min: 62.0, 25%: 257.0, 50%: 339.0, 75%: 448.0, max: 2094.0\n",
      "[2020_08_30-20:15:29] Epoch 146 (current sample 1300000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.4982 - output-seq_loss: 0.2092 - output-annotations_loss: 2.8899e-04\n",
      "[2020_08_30-20:17:02] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 381.76421875, std: 177.92674279561993, min: 68.0, 25%: 262.0, 50%: 345.0, 75%: 458.0, max: 1845.0\n",
      "[2020_08_30-20:17:03] Epoch 147 (current sample 1400000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.5139 - output-seq_loss: 0.2127 - output-annotations_loss: 3.0128e-04\n",
      "[2020_08_30-20:18:22] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 374.49046875, std: 174.65409630178158, min: 71.0, 25%: 258.0, 50%: 336.0, 75%: 448.0, max: 1922.0\n",
      "[2020_08_30-20:18:23] Epoch 148 (current sample 1400000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.5040 - output-seq_loss: 0.2096 - output-annotations_loss: 2.9441e-04\n",
      "[2020_08_30-20:19:42] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 377.1521875, std: 180.24002843310456, min: 62.0, 25%: 259.0, 50%: 340.0, 75%: 454.0, max: 2303.0\n",
      "[2020_08_30-20:19:44] Epoch 149 (current sample 1400000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.4959 - output-seq_loss: 0.2099 - output-annotations_loss: 2.8599e-04\n",
      "[2020_08_30-20:21:02] Starting a new episode with max_seq_len = 100.\n",
      "[2020_08_30-20:21:09] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.626640625, std: 78.22444559738618, min: 14.0, 25%: 90.0, 50%: 133.0, 75%: 190.0, max: 865.0\n",
      "[2020_08_30-20:21:12] Epoch 150 (current sample 1400000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 43s 3ms/sample - loss: 0.4913 - output-seq_loss: 0.2601 - output-annotations_loss: 2.3124e-04\n",
      "[2020_08_30-20:23:06] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 149.02171875, std: 78.22898366737685, min: 13.0, 25%: 90.0, 50%: 135.0, 75%: 190.0, max: 690.0\n",
      "[2020_08_30-20:23:09] Epoch 151 (current sample 1400000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4734 - output-seq_loss: 0.2590 - output-annotations_loss: 2.1437e-04\n",
      "[2020_08_30-20:23:48] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 149.241484375, std: 78.57292705101571, min: 13.0, 25%: 90.0, 50%: 135.0, 75%: 190.0, max: 1141.0\n",
      "[2020_08_30-20:23:51] Epoch 152 (current sample 1400000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4655 - output-seq_loss: 0.2572 - output-annotations_loss: 2.0829e-04\n",
      "[2020_08_30-20:24:31] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.20546875, std: 77.73882704206667, min: 13.0, 25%: 90.0, 50%: 133.0, 75%: 189.0, max: 762.0\n",
      "[2020_08_30-20:24:33] Epoch 153 (current sample 1400000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 40s 3ms/sample - loss: 0.4635 - output-seq_loss: 0.2591 - output-annotations_loss: 2.0443e-04\n",
      "[2020_08_30-20:25:13] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.77828125, std: 78.32057065055936, min: 13.0, 25%: 89.0, 50%: 134.0, 75%: 190.0, max: 664.0\n",
      "[2020_08_30-20:25:16] Epoch 154 (current sample 1400000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 40s 3ms/sample - loss: 0.4755 - output-seq_loss: 0.2592 - output-annotations_loss: 2.1623e-04\n",
      "[2020_08_30-20:25:56] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 147.78734375, std: 77.28414697818332, min: 13.0, 25%: 89.0, 50%: 133.0, 75%: 189.0, max: 790.0\n",
      "[2020_08_30-20:25:59] Epoch 155 (current sample 1400000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4537 - output-seq_loss: 0.2572 - output-annotations_loss: 1.9648e-04\n",
      "[2020_08_30-20:26:52] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.431875, std: 78.34852976400356, min: 13.0, 25%: 90.0, 50%: 133.0, 75%: 190.0, max: 677.0\n",
      "[2020_08_30-20:26:55] Epoch 156 (current sample 1500000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4649 - output-seq_loss: 0.2585 - output-annotations_loss: 2.0642e-04\n",
      "[2020_08_30-20:27:35] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.549296875, std: 76.11205449169111, min: 13.0, 25%: 91.0, 50%: 134.0, 75%: 190.25, max: 612.0\n",
      "[2020_08_30-20:27:38] Epoch 157 (current sample 1500000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4653 - output-seq_loss: 0.2588 - output-annotations_loss: 2.0652e-04\n",
      "[2020_08_30-20:28:18] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.574140625, std: 77.74743881673321, min: 14.0, 25%: 90.0, 50%: 134.0, 75%: 189.0, max: 667.0\n",
      "[2020_08_30-20:28:21] Epoch 158 (current sample 1500000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4674 - output-seq_loss: 0.2599 - output-annotations_loss: 2.0742e-04\n",
      "[2020_08_30-20:29:14] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.946328125, std: 78.19718811263422, min: 13.0, 25%: 90.0, 50%: 133.5, 75%: 190.0, max: 699.0\n",
      "[2020_08_30-20:29:17] Epoch 159 (current sample 1600000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4632 - output-seq_loss: 0.2585 - output-annotations_loss: 2.0466e-04\n",
      "[2020_08_30-20:29:57] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 147.893515625, std: 78.05632213906046, min: 13.0, 25%: 89.0, 50%: 132.0, 75%: 189.0, max: 814.0\n",
      "[2020_08_30-20:30:00] Epoch 160 (current sample 1600000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4524 - output-seq_loss: 0.2563 - output-annotations_loss: 1.9615e-04\n",
      "[2020_08_30-20:31:59] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 149.7915625, std: 78.68118551793792, min: 13.0, 25%: 90.0, 50%: 135.0, 75%: 193.0, max: 918.0\n",
      "[2020_08_30-20:32:02] Epoch 161 (current sample 1600000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4619 - output-seq_loss: 0.2573 - output-annotations_loss: 2.0463e-04\n",
      "[2020_08_30-20:32:42] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 149.00796875, std: 78.15737417340512, min: 13.0, 25%: 90.0, 50%: 134.0, 75%: 189.0, max: 712.0\n",
      "[2020_08_30-20:32:45] Epoch 162 (current sample 1600000):\n",
      "Train on 12800 samples\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.408828). Check your callbacks.\n",
      "12800/12800 [==============================] - 40s 3ms/sample - loss: 0.4625 - output-seq_loss: 0.2584 - output-annotations_loss: 2.0411e-04\n",
      "[2020_08_30-20:33:36] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.94875, std: 78.57242877124733, min: 13.0, 25%: 90.0, 50%: 135.0, 75%: 190.0, max: 794.0\n",
      "[2020_08_30-20:33:39] Epoch 163 (current sample 1700000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4566 - output-seq_loss: 0.2569 - output-annotations_loss: 1.9969e-04\n",
      "[2020_08_30-20:34:19] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 147.843515625, std: 77.26682852934644, min: 14.0, 25%: 90.0, 50%: 133.0, 75%: 188.0, max: 733.0\n",
      "[2020_08_30-20:34:22] Epoch 164 (current sample 1700000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4579 - output-seq_loss: 0.2577 - output-annotations_loss: 2.0029e-04\n",
      "[2020_08_30-20:35:02] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.00375, std: 77.9006339877, min: 14.0, 25%: 89.0, 50%: 133.0, 75%: 191.0, max: 711.0\n",
      "[2020_08_30-20:35:05] Epoch 165 (current sample 1700000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4490 - output-seq_loss: 0.2564 - output-annotations_loss: 1.9256e-04\n",
      "[2020_08_30-20:35:58] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.96125, std: 78.55750683311152, min: 14.0, 25%: 89.0, 50%: 134.0, 75%: 190.0, max: 693.0\n",
      "[2020_08_30-20:36:01] Epoch 166 (current sample 1800000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4502 - output-seq_loss: 0.2562 - output-annotations_loss: 1.9402e-04\n",
      "[2020_08_30-20:36:40] Starting a new episode with max_seq_len = 450.\n",
      "[2020_08_30-20:36:47] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 374.143125, std: 176.45276478232927, min: 52.0, 25%: 255.0, 50%: 339.0, 75%: 453.0, max: 2314.0\n",
      "[2020_08_30-20:36:49] Epoch 167 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 80s 13ms/sample - loss: 0.5138 - output-seq_loss: 0.2082 - output-annotations_loss: 3.0555e-04\n",
      "[2020_08_30-20:38:09] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 381.66828125, std: 182.89382717214286, min: 69.0, 25%: 260.0, 50%: 345.0, 75%: 460.0, max: 3023.0\n",
      "[2020_08_30-20:38:11] Epoch 168 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.5038 - output-seq_loss: 0.2105 - output-annotations_loss: 2.9330e-04\n",
      "[2020_08_30-20:39:30] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 380.17953125, std: 185.09068017396947, min: 62.0, 25%: 259.0, 50%: 342.0, 75%: 456.0, max: 2104.0\n",
      "[2020_08_30-20:39:31] Epoch 169 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.5183 - output-seq_loss: 0.2098 - output-annotations_loss: 3.0857e-04\n",
      "[2020_08_30-20:40:50] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 373.59390625, std: 183.31339684566106, min: 62.0, 25%: 257.0, 50%: 337.0, 75%: 447.0, max: 4949.0\n",
      "[2020_08_30-20:40:52] Epoch 170 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.5023 - output-seq_loss: 0.2097 - output-annotations_loss: 2.9262e-04\n",
      "[2020_08_30-20:43:16] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 379.64140625, std: 184.83123493674574, min: 44.0, 25%: 257.0, 50%: 341.0, 75%: 453.0, max: 2132.0\n",
      "[2020_08_30-20:43:18] Epoch 171 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.5173 - output-seq_loss: 0.2101 - output-annotations_loss: 3.0718e-04\n",
      "[2020_08_30-20:44:36] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 374.62890625, std: 178.69322740115547, min: 57.0, 25%: 256.0, 50%: 338.0, 75%: 450.0, max: 2380.0\n",
      "[2020_08_30-20:44:38] Epoch 172 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.5099 - output-seq_loss: 0.2079 - output-annotations_loss: 3.0202e-04\n",
      "[2020_08_30-20:45:57] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 377.2890625, std: 183.7177435308641, min: 57.0, 25%: 258.0, 50%: 339.0, 75%: 453.0, max: 2407.0\n",
      "[2020_08_30-20:45:58] Epoch 173 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.5179 - output-seq_loss: 0.2095 - output-annotations_loss: 3.0844e-04\n",
      "[2020_08_30-20:47:17] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 374.5596875, std: 173.60266283776411, min: 56.0, 25%: 258.0, 50%: 337.5, 75%: 449.0, max: 1973.0\n",
      "[2020_08_30-20:47:19] Epoch 174 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.4971 - output-seq_loss: 0.2089 - output-annotations_loss: 2.8820e-04\n",
      "[2020_08_30-20:48:38] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 374.20359375, std: 178.48459425278898, min: 73.0, 25%: 258.0, 50%: 338.0, 75%: 450.0, max: 2327.0\n",
      "[2020_08_30-20:48:39] Epoch 175 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.4962 - output-seq_loss: 0.2088 - output-annotations_loss: 2.8740e-04\n",
      "[2020_08_30-20:49:58] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 375.6134375, std: 181.70707157410448, min: 60.0, 25%: 257.0, 50%: 337.0, 75%: 451.25, max: 2679.0\n",
      "[2020_08_30-20:50:00] Epoch 176 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.4824 - output-seq_loss: 0.2081 - output-annotations_loss: 2.7426e-04\n",
      "[2020_08_30-20:51:18] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 376.04671875, std: 180.22174661937663, min: 61.0, 25%: 257.0, 50%: 339.0, 75%: 451.0, max: 2625.0\n",
      "[2020_08_30-20:51:20] Epoch 177 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.4986 - output-seq_loss: 0.2084 - output-annotations_loss: 2.9016e-04\n",
      "[2020_08_30-20:52:39] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 372.62203125, std: 177.3400098383942, min: 61.0, 25%: 255.0, 50%: 338.0, 75%: 444.0, max: 2378.0\n",
      "[2020_08_30-20:52:41] Epoch 178 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.4888 - output-seq_loss: 0.2087 - output-annotations_loss: 2.8011e-04\n",
      "[2020_08_30-20:54:00] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 378.19171875, std: 178.91009377213535, min: 81.0, 25%: 260.0, 50%: 342.0, 75%: 453.0, max: 2153.0\n",
      "[2020_08_30-20:54:01] Epoch 179 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.4871 - output-seq_loss: 0.2099 - output-annotations_loss: 2.7725e-04\n",
      "[2020_08_30-20:55:20] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 377.02265625, std: 186.45194003783675, min: 60.0, 25%: 257.0, 50%: 338.0, 75%: 453.0, max: 2283.0\n",
      "[2020_08_30-20:55:22] Epoch 180 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.4859 - output-seq_loss: 0.2094 - output-annotations_loss: 2.7653e-04\n",
      "[2020_08_30-20:58:00] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 378.53875, std: 178.64106515412382, min: 62.0, 25%: 259.0, 50%: 343.0, 75%: 454.0, max: 2969.0\n",
      "[2020_08_30-20:58:01] Epoch 181 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.5011 - output-seq_loss: 0.2110 - output-annotations_loss: 2.9008e-04\n",
      "[2020_08_30-20:59:19] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 374.44515625, std: 177.06127004655505, min: 62.0, 25%: 257.0, 50%: 340.0, 75%: 448.0, max: 2310.0\n",
      "[2020_08_30-20:59:21] Epoch 182 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.4811 - output-seq_loss: 0.2081 - output-annotations_loss: 2.7306e-04\n",
      "[2020_08_30-21:00:40] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 371.96609375, std: 179.4531254371236, min: 64.0, 25%: 258.0, 50%: 339.0, 75%: 447.0, max: 3025.0\n",
      "[2020_08_30-21:00:41] Epoch 183 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.4812 - output-seq_loss: 0.2083 - output-annotations_loss: 2.7286e-04\n",
      "[2020_08_30-21:02:00] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 376.25328125, std: 180.83738663484846, min: 61.0, 25%: 257.0, 50%: 339.5, 75%: 451.0, max: 2789.0\n",
      "[2020_08_30-21:02:02] Epoch 184 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.4896 - output-seq_loss: 0.2088 - output-annotations_loss: 2.8076e-04\n",
      "[2020_08_30-21:03:20] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 374.37640625, std: 179.24345631970482, min: 66.0, 25%: 257.0, 50%: 339.0, 75%: 449.0, max: 2449.0\n",
      "[2020_08_30-21:03:22] Epoch 185 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.4752 - output-seq_loss: 0.2088 - output-annotations_loss: 2.6638e-04\n",
      "[2020_08_30-21:04:41] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 374.51890625, std: 178.51759465278573, min: 64.0, 25%: 256.0, 50%: 338.0, 75%: 448.0, max: 1980.0\n",
      "[2020_08_30-21:04:42] Epoch 186 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.4698 - output-seq_loss: 0.2085 - output-annotations_loss: 2.6133e-04\n",
      "[2020_08_30-21:06:01] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 373.6128125, std: 175.68976805898853, min: 61.0, 25%: 258.0, 50%: 337.0, 75%: 447.0, max: 2308.0\n",
      "[2020_08_30-21:06:02] Epoch 187 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.4924 - output-seq_loss: 0.2078 - output-annotations_loss: 2.8463e-04\n",
      "[2020_08_30-21:07:21] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 374.9365625, std: 181.04370374781132, min: 66.0, 25%: 257.0, 50%: 338.0, 75%: 453.0, max: 2320.0\n",
      "[2020_08_30-21:07:23] Epoch 188 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.4745 - output-seq_loss: 0.2090 - output-annotations_loss: 2.6549e-04\n",
      "[2020_08_30-21:08:41] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 374.15421875, std: 178.21659659903534, min: 63.0, 25%: 256.0, 50%: 335.0, 75%: 449.0, max: 1855.0\n",
      "[2020_08_30-21:08:43] Epoch 189 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.4711 - output-seq_loss: 0.2075 - output-annotations_loss: 2.6358e-04\n",
      "[2020_08_30-21:10:02] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 374.25203125, std: 178.7424042619867, min: 60.0, 25%: 256.0, 50%: 338.0, 75%: 448.0, max: 2819.0\n",
      "[2020_08_30-21:10:04] Epoch 190 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.4618 - output-seq_loss: 0.2080 - output-annotations_loss: 2.5372e-04\n",
      "[2020_08_30-21:12:30] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 377.21921875, std: 179.07960648722042, min: 64.0, 25%: 259.75, 50%: 341.0, 75%: 454.0, max: 2489.0\n",
      "[2020_08_30-21:12:31] Epoch 191 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.4741 - output-seq_loss: 0.2095 - output-annotations_loss: 2.6469e-04\n",
      "[2020_08_30-21:13:50] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 374.33296875, std: 175.1018900327601, min: 65.0, 25%: 257.0, 50%: 338.0, 75%: 450.0, max: 2114.0\n",
      "[2020_08_30-21:13:51] Epoch 192 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.4830 - output-seq_loss: 0.2082 - output-annotations_loss: 2.7481e-04\n",
      "[2020_08_30-21:15:10] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 373.509375, std: 175.69827978446958, min: 54.0, 25%: 257.0, 50%: 340.0, 75%: 449.0, max: 2653.0\n",
      "[2020_08_30-21:15:12] Epoch 193 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.4828 - output-seq_loss: 0.2097 - output-annotations_loss: 2.7306e-04\n",
      "[2020_08_30-21:16:31] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 375.08015625, std: 176.40349613585323, min: 66.0, 25%: 259.0, 50%: 341.0, 75%: 449.0, max: 2149.0\n",
      "[2020_08_30-21:16:32] Epoch 194 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.4593 - output-seq_loss: 0.2088 - output-annotations_loss: 2.5057e-04\n",
      "[2020_08_30-21:17:51] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 376.65, std: 178.319815375947, min: 56.0, 25%: 258.0, 50%: 341.0, 75%: 454.0, max: 2527.0\n",
      "[2020_08_30-21:17:53] Epoch 195 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.4618 - output-seq_loss: 0.2093 - output-annotations_loss: 2.5253e-04\n",
      "[2020_08_30-21:19:11] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 377.18, std: 174.1911482377748, min: 64.0, 25%: 260.0, 50%: 341.0, 75%: 455.0, max: 1923.0\n",
      "[2020_08_30-21:19:13] Epoch 196 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.4742 - output-seq_loss: 0.2086 - output-annotations_loss: 2.6555e-04\n",
      "[2020_08_30-21:20:32] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 375.8740625, std: 179.44473161415146, min: 46.0, 25%: 258.0, 50%: 339.0, 75%: 449.0, max: 3347.0\n",
      "[2020_08_30-21:20:33] Epoch 197 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.4745 - output-seq_loss: 0.2084 - output-annotations_loss: 2.6605e-04\n",
      "[2020_08_30-21:21:52] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 374.98234375, std: 179.8834896687902, min: 67.0, 25%: 255.0, 50%: 338.0, 75%: 449.0, max: 2217.0\n",
      "[2020_08_30-21:21:54] Epoch 198 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 78s 12ms/sample - loss: 0.4540 - output-seq_loss: 0.2085 - output-annotations_loss: 2.4551e-04\n",
      "[2020_08_30-21:23:12] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 379.51765625, std: 179.98686031673708, min: 71.0, 25%: 259.0, 50%: 345.0, 75%: 459.0, max: 2284.0\n",
      "[2020_08_30-21:23:14] Epoch 199 (current sample 1800000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 79s 12ms/sample - loss: 0.4587 - output-seq_loss: 0.2106 - output-annotations_loss: 2.4811e-04\n",
      "[2020_08_30-21:24:33] Starting a new episode with max_seq_len = 1200.\n",
      "[2020_08_30-21:24:39] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 808.42, std: 612.0903702021087, min: 137.0, 25%: 450.75, 50%: 660.5, 75%: 965.25, max: 7952.0\n",
      "[2020_08_30-21:24:40] Epoch 200 (current sample 1800000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 107s 34ms/sample - loss: 0.5195 - output-seq_loss: 0.1661 - output-annotations_loss: 3.5346e-04\n",
      "[2020_08_30-21:27:31] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 791.4003125, std: 575.1764273144332, min: 137.0, 25%: 446.75, 50%: 649.0, 75%: 948.0, max: 6196.0\n",
      "[2020_08_30-21:27:32] Epoch 201 (current sample 1800000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 104s 33ms/sample - loss: 0.5226 - output-seq_loss: 0.1647 - output-annotations_loss: 3.5788e-04\n",
      "[2020_08_30-21:29:17] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 806.7928125, std: 852.6490822893188, min: 136.0, 25%: 451.0, 50%: 660.0, 75%: 942.0, max: 34486.0\n",
      "[2020_08_30-21:29:17] Epoch 202 (current sample 1800000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.5044 - output-seq_loss: 0.1650 - output-annotations_loss: 3.3941e-04\n",
      "[2020_08_30-21:31:03] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 800.5915625, std: 561.7434559260747, min: 163.0, 25%: 450.0, 50%: 667.0, 75%: 962.25, max: 6211.0\n",
      "[2020_08_30-21:31:04] Epoch 203 (current sample 1800000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.5015 - output-seq_loss: 0.1671 - output-annotations_loss: 3.3448e-04\n",
      "[2020_08_30-21:32:49] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 811.4428125, std: 736.4920985968688, min: 158.0, 25%: 446.75, 50%: 651.0, 75%: 946.0, max: 14709.0\n",
      "[2020_08_30-21:32:50] Epoch 204 (current sample 1800000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.5077 - output-seq_loss: 0.1643 - output-annotations_loss: 3.4334e-04\n",
      "[2020_08_30-21:34:35] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 773.526875, std: 594.2832774397751, min: 157.0, 25%: 439.0, 50%: 630.5, 75%: 911.25, max: 7786.0\n",
      "[2020_08_30-21:34:36] Epoch 205 (current sample 1800000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.4901 - output-seq_loss: 0.1613 - output-annotations_loss: 3.2879e-04\n",
      "[2020_08_30-21:36:21] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 795.76125, std: 632.4999278047571, min: 139.0, 25%: 447.0, 50%: 643.5, 75%: 953.25, max: 14658.0\n",
      "[2020_08_30-21:36:22] Epoch 206 (current sample 1800000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.5040 - output-seq_loss: 0.1648 - output-annotations_loss: 3.3927e-04\n",
      "[2020_08_30-21:38:08] Epoch sequence length distribution (for max_seq_len = 1200): count: 3200.0, mean: 802.6790625, std: 595.0344142755946, min: 134.0, 25%: 449.75, 50%: 660.0, 75%: 952.25, max: 8803.0\n",
      "[2020_08_30-21:38:09] Epoch 207 (current sample 1800000):\n",
      "Train on 3200 samples\n",
      "3200/3200 [==============================] - 105s 33ms/sample - loss: 0.4965 - output-seq_loss: 0.1657 - output-annotations_loss: 3.3081e-04\n",
      "[2020_08_30-21:39:54] Starting a new episode with max_seq_len = 100.\n",
      "[2020_08_30-21:40:00] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 149.06859375, std: 78.0633815333652, min: 13.0, 25%: 90.0, 50%: 134.0, 75%: 189.0, max: 700.0\n",
      "[2020_08_30-21:40:03] Epoch 208 (current sample 1800000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 42s 3ms/sample - loss: 0.4734 - output-seq_loss: 0.2594 - output-annotations_loss: 2.1398e-04\n",
      "[2020_08_30-21:40:45] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 149.08578125, std: 78.33274570264581, min: 13.0, 25%: 90.0, 50%: 134.0, 75%: 190.0, max: 677.0\n",
      "[2020_08_30-21:40:48] Epoch 209 (current sample 1800000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4499 - output-seq_loss: 0.2562 - output-annotations_loss: 1.9370e-04\n",
      "[2020_08_30-21:41:40] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.118359375, std: 77.4416064730186, min: 14.0, 25%: 90.0, 50%: 133.0, 75%: 189.0, max: 816.0\n",
      "[2020_08_30-21:41:43] Epoch 210 (current sample 1900000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4559 - output-seq_loss: 0.2577 - output-annotations_loss: 1.9820e-04\n",
      "[2020_08_30-21:43:04] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 147.44390625, std: 77.82268311615861, min: 13.0, 25%: 88.0, 50%: 132.0, 75%: 189.0, max: 780.0\n",
      "[2020_08_30-21:43:07] Epoch 211 (current sample 1900000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4402 - output-seq_loss: 0.2570 - output-annotations_loss: 1.8324e-04\n",
      "[2020_08_30-21:43:46] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.301796875, std: 77.71437019323872, min: 14.0, 25%: 90.0, 50%: 134.0, 75%: 189.0, max: 855.0\n",
      "[2020_08_30-21:43:49] Epoch 212 (current sample 1900000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 40s 3ms/sample - loss: 0.4463 - output-seq_loss: 0.2587 - output-annotations_loss: 1.8755e-04\n",
      "[2020_08_30-21:44:46] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 147.051953125, std: 76.38767669362772, min: 13.0, 25%: 89.0, 50%: 132.0, 75%: 187.0, max: 687.0\n",
      "[2020_08_30-21:44:49] Epoch 213 (current sample 2000000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4385 - output-seq_loss: 0.2559 - output-annotations_loss: 1.8260e-04\n",
      "[2020_08_30-21:45:29] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 147.544921875, std: 77.68423716530364, min: 13.0, 25%: 89.0, 50%: 132.0, 75%: 187.0, max: 659.0\n",
      "[2020_08_30-21:45:31] Epoch 214 (current sample 2000000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 40s 3ms/sample - loss: 0.4418 - output-seq_loss: 0.2549 - output-annotations_loss: 1.8684e-04\n",
      "[2020_08_30-21:46:11] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 147.8525, std: 77.04296444312855, min: 13.0, 25%: 90.0, 50%: 133.0, 75%: 190.0, max: 652.0\n",
      "[2020_08_30-21:46:14] Epoch 215 (current sample 2000000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4425 - output-seq_loss: 0.2578 - output-annotations_loss: 1.8474e-04\n",
      "[2020_08_30-21:46:54] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.507890625, std: 78.18361195380129, min: 13.0, 25%: 89.0, 50%: 134.0, 75%: 189.0, max: 765.0\n",
      "[2020_08_30-21:46:57] Epoch 216 (current sample 2000000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4407 - output-seq_loss: 0.2582 - output-annotations_loss: 1.8249e-04\n",
      "[2020_08_30-21:47:46] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 147.98765625, std: 76.62261572296217, min: 14.0, 25%: 90.0, 50%: 134.0, 75%: 189.0, max: 788.0\n",
      "[2020_08_30-21:47:49] Epoch 217 (current sample 2100000):\n",
      "Train on 12800 samples\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.684773). Check your callbacks.\n",
      "12800/12800 [==============================] - 40s 3ms/sample - loss: 0.4453 - output-seq_loss: 0.2560 - output-annotations_loss: 1.8933e-04\n",
      "[2020_08_30-21:48:29] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.12484375, std: 77.00456672891663, min: 14.0, 25%: 90.0, 50%: 134.0, 75%: 188.0, max: 895.0\n",
      "[2020_08_30-21:48:32] Epoch 218 (current sample 2100000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 40s 3ms/sample - loss: 0.4455 - output-seq_loss: 0.2578 - output-annotations_loss: 1.8766e-04\n",
      "[2020_08_30-21:49:12] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.698515625, std: 77.15001301052244, min: 13.0, 25%: 91.0, 50%: 134.0, 75%: 191.0, max: 678.0\n",
      "[2020_08_30-21:49:15] Epoch 219 (current sample 2100000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4415 - output-seq_loss: 0.2563 - output-annotations_loss: 1.8518e-04\n",
      "[2020_08_30-21:50:09] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.7178125, std: 78.50821328365855, min: 13.0, 25%: 90.0, 50%: 134.0, 75%: 188.0, max: 905.0\n",
      "[2020_08_30-21:50:12] Epoch 220 (current sample 2200000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 40s 3ms/sample - loss: 0.4370 - output-seq_loss: 0.2574 - output-annotations_loss: 1.7961e-04\n",
      "[2020_08_30-21:51:33] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.49734375, std: 76.69171521059445, min: 13.0, 25%: 90.0, 50%: 134.0, 75%: 190.0, max: 757.0\n",
      "[2020_08_30-21:51:36] Epoch 221 (current sample 2200000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4365 - output-seq_loss: 0.2558 - output-annotations_loss: 1.8070e-04\n",
      "[2020_08_30-21:52:16] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.6290625, std: 77.45182452956692, min: 13.0, 25%: 90.0, 50%: 134.0, 75%: 190.0, max: 821.0\n",
      "[2020_08_30-21:52:19] Epoch 222 (current sample 2200000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4415 - output-seq_loss: 0.2566 - output-annotations_loss: 1.8493e-04\n",
      "[2020_08_30-21:53:10] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.302265625, std: 78.6219787524852, min: 13.0, 25%: 89.0, 50%: 134.0, 75%: 189.0, max: 763.0\n",
      "[2020_08_30-21:53:13] Epoch 223 (current sample 2300000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4336 - output-seq_loss: 0.2550 - output-annotations_loss: 1.7860e-04\n",
      "[2020_08_30-21:53:53] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 148.97125, std: 78.3632807345496, min: 13.0, 25%: 90.0, 50%: 133.0, 75%: 191.0, max: 821.0\n",
      "[2020_08_30-21:53:56] Epoch 224 (current sample 2300000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 39s 3ms/sample - loss: 0.4356 - output-seq_loss: 0.2557 - output-annotations_loss: 1.7991e-04\n",
      "[2020_08_30-21:54:35] Epoch sequence length distribution (for max_seq_len = 100): count: 12800.0, mean: 149.22203125, std: 78.25903774893236, min: 13.0, 25%: 91.0, 50%: 134.0, 75%: 190.0, max: 739.0\n",
      "[2020_08_30-21:54:38] Epoch 225 (current sample 2300000):\n",
      "Train on 12800 samples\n",
      "12800/12800 [==============================] - 40s 3ms/sample - loss: 0.4405 - output-seq_loss: 0.2569 - output-annotations_loss: 1.8363e-04\n",
      "[2020_08_30-21:55:18] Starting a new episode with max_seq_len = 450.\n",
      "[2020_08_30-21:55:24] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 376.470625, std: 178.94988366961738, min: 71.0, 25%: 257.0, 50%: 341.0, 75%: 451.0, max: 2174.0\n",
      "[2020_08_30-21:55:26] Epoch 226 (current sample 2300000):\n",
      "Train on 6400 samples\n",
      "6400/6400 [==============================] - 81s 13ms/sample - loss: 0.4621 - output-seq_loss: 0.2080 - output-annotations_loss: 2.5402e-04\n",
      "[2020_08_30-21:56:47] Epoch sequence length distribution (for max_seq_len = 450): count: 6400.0, mean: 373.7696875, std: 175.90587566422607, min: 36.0, 25%: 257.0, 50%: 339.0, 75%: 447.0, max: 2665.0\n",
      "[2020_08_30-21:56:48] Epoch 227 (current sample 2300000):\n",
      "Train on 6400 samples\n",
      "1280/6400 [=====>........................] - ETA: 1:04 - loss: 0.4638 - output-seq_loss: 0.2028 - output-annotations_loss: 2.6102e-04"
     ]
    }
   ],
   "source": [
    "EPISODE_SETTINGS = [\n",
    "    # max_seq_len, batch_size\n",
    "    (100, 128),\n",
    "    (450, 64),\n",
    "    (1200, 32),\n",
    "]\n",
    "\n",
    "ALL_AAS = 'ACDEFGHIKLMNPQRSTUVWXY'\n",
    "ADDITIONAL_TOKENS = ['<OTHER>', '<START>', '<END>', '<PAD>']\n",
    "\n",
    "# Each sequence is added <START> and <END> tokens\n",
    "ADDED_TOKENS_PER_SEQ = 2\n",
    "\n",
    "n_aas = len(ALL_AAS)\n",
    "aa_to_token_index = {aa: i for i, aa in enumerate(ALL_AAS)}\n",
    "additional_token_to_index = {token: i + len(ALL_AAS) for i, token in enumerate(ADDITIONAL_TOKENS)}\n",
    "token_to_index = {**aa_to_token_index, **additional_token_to_index}\n",
    "index_to_token = {index: token for token, index in token_to_index.items()}\n",
    "n_tokens = len(token_to_index)\n",
    "\n",
    "def tokenize_seq(seq):\n",
    "    other_token_index = additional_token_to_index['<OTHER>']\n",
    "    return [additional_token_to_index['<START>']] + [aa_to_token_index.get(aa, other_token_index) for aa in seq] + \\\n",
    "            [additional_token_to_index['<END>']]\n",
    "\n",
    "def create_model(max_seq_len):\n",
    "    global n_tokens, n_annotations\n",
    "    K.clear_session()\n",
    "    return _create_model(seq_len = max_seq_len, vocab_size = n_tokens, n_annotations = n_annotations)\n",
    "\n",
    "def recreate_model_with_same_state(old_model, create_and_compile_model_funtion):\n",
    "    model_weights, optimizer_weights = old_model.get_weights(), old_model.optimizer.get_weights()\n",
    "    new_model = create_and_compile_model_funtion()\n",
    "    new_model.set_weights(model_weights)\n",
    "    new_model.optimizer.set_weights(optimizer_weights)\n",
    "    return new_model\n",
    "\n",
    "def save_model(model, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump((model.get_weights(), model.optimizer.get_weights()), f)\n",
    "        \n",
    "def load_model(model, path):\n",
    "    with open(path, 'rb') as f:\n",
    "        model_weights, optimizer_weights = pickle.load(f)\n",
    "        model.set_weights(model_weights)\n",
    "        model.optimizer.set_weights(optimizer_weights)\n",
    "\n",
    "class SampleCache:\n",
    "    \n",
    "    def __init__(self, seqs = [], annotation_masks = [], test_set_flags = []):\n",
    "        self.seqs = list(seqs)\n",
    "        self.annotation_masks = list(annotation_masks)\n",
    "        self.test_set_flags = list(test_set_flags)\n",
    "        \n",
    "    def extend(self, other_cache):\n",
    "        self.seqs.extend(other_cache.seqs)\n",
    "        self.annotation_masks.extend(other_cache.annotation_masks)\n",
    "        self.test_set_flags.extend(other_cache.test_set_flags)\n",
    "        \n",
    "    def pop(self, n):\n",
    "        popped_sample_cache = self.slice_first(n)\n",
    "        self.seqs = self.seqs[n:]\n",
    "        self.annotation_masks = self.annotation_masks[n:]\n",
    "        self.test_set_flags = self.test_set_flags[n:]\n",
    "        return popped_sample_cache\n",
    "    \n",
    "    def slice_first(self, n):\n",
    "        return SampleCache(self.seqs[:n], self.annotation_masks[:n], self.test_set_flags[:n])\n",
    "        \n",
    "    def slice_indices(self, indices):\n",
    "        return SampleCache([self.seqs[i] for i in indices], [self.annotation_masks[i] for i in indices], \\\n",
    "                [self.test_set_flags[i] for i in indices])\n",
    "    \n",
    "    def __len__(self):\n",
    "        assert len(self.seqs) == len(self.annotation_masks) == len(self.test_set_flags)\n",
    "        return len(self.seqs)\n",
    "    \n",
    "class DatasetHandler:\n",
    "    \n",
    "    def __init__(self, dataset_h5f):\n",
    "        self.dataset_h5f = dataset_h5f\n",
    "        self.total_size = len(h5f['seq_lengths'])\n",
    "        \n",
    "    def __getitem__(self, slicing):\n",
    "        return SampleCache(self.dataset_h5f['seqs'][slicing], self.dataset_h5f['annotation_masks'][slicing], \\\n",
    "                self.dataset_h5f['test_set_similarity'][slicing])\n",
    "\n",
    "class EpisodeDataManager:\n",
    "    \n",
    "    def __init__(self, max_seq_len, batch_size, batches_per_epoch):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.batches_per_epoch = batches_per_epoch\n",
    "        self.epoch_size = self.batches_per_epoch * self.batch_size\n",
    "        self.sample_cache = SampleCache()\n",
    "        \n",
    "    def is_epoch_ready(self, n_required_samples = None):\n",
    "        return len(self.sample_cache) >= self._resolve_epoch_size(n_required_samples)\n",
    "    \n",
    "    def get_next_raw_epoch(self, size = None):\n",
    "        return self.sample_cache.pop(self._resolve_epoch_size(size))\n",
    "    \n",
    "    def peek_raw_epoch(self, size = None):\n",
    "        return self.sample_cache.slice_first(self._resolve_epoch_size(size))\n",
    "    \n",
    "    def encode_next_epoch(self, log_length_dist = True):\n",
    "        \n",
    "        seq_lengths, encoded_seqs, encoded_annotation_masks = self._encode_epoch(self.get_next_raw_epoch())\n",
    "        \n",
    "        if log_length_dist:\n",
    "            log('Epoch sequence length distribution (for max_seq_len = %d): %s' % (self.max_seq_len, \\\n",
    "                    ', '.join('%s: %s' % item for item in pd.Series(seq_lengths).describe().iteritems())))\n",
    "        \n",
    "        return encoded_seqs, encoded_annotation_masks\n",
    "    \n",
    "    def encode_dummy_epoch(self, size = 1):\n",
    "        seq_lengths, encoded_seqs, encoded_annotation_masks = self._encode_epoch(self.peek_raw_epoch(size))\n",
    "        return encoded_seqs, encoded_annotation_masks\n",
    "    \n",
    "    def _encode_epoch(self, epoch_sample_cache):\n",
    "        \n",
    "        pad_token_index = additional_token_to_index['<PAD>']\n",
    "        tokenized_seqs = list(map(tokenize_seq, epoch_sample_cache.seqs))\n",
    "        seq_lengths = np.array(list(map(len, tokenized_seqs)))\n",
    "        max_offsets = np.maximum(seq_lengths - self.max_seq_len, 0)\n",
    "        chosen_offsets = (np.random.rand(self.epoch_size) * (max_offsets + 1)).astype(int)\n",
    "        trimmed_tokenized_seqs = [seq_tokens[chosen_offset:(chosen_offset + self.max_seq_len)] for seq_tokens, chosen_offset in \\\n",
    "                zip(tokenized_seqs, chosen_offsets)]\n",
    "        encoded_seqs = np.array([seq_tokens + max(self.max_seq_len - len(seq_tokens), 0) * [pad_token_index] for seq_tokens in \\\n",
    "                trimmed_tokenized_seqs]).astype(np.int8)\n",
    "        \n",
    "        encoded_annotation_masks = np.concatenate([annotation_mask.reshape(1, -1) for annotation_mask in \\\n",
    "                epoch_sample_cache.annotation_masks], axis = 0).astype(bool)\n",
    "        encoded_annotation_masks[epoch_sample_cache.test_set_flags, :] = False\n",
    "        \n",
    "        return seq_lengths, encoded_seqs, encoded_annotation_masks\n",
    "    \n",
    "    def _resolve_epoch_size(self, size):\n",
    "        if size is None:\n",
    "            return self.epoch_size\n",
    "        else:\n",
    "            return size\n",
    "    \n",
    "class EpochGenerator:\n",
    "    \n",
    "    def __init__(self, batches_per_epoch = 100, p_seq_noise = 0.05, p_no_input_annot = 0.5, p_annot_noise_positive = 0.25, \\\n",
    "            p_annot_noise_negative = 1e-04, load_chunk_size = 100000, min_time_per_episode = timedelta(minutes = 15)):\n",
    "        \n",
    "        self.batches_per_epoch = batches_per_epoch\n",
    "        self.p_seq_noise = p_seq_noise\n",
    "        self.p_no_input_annot = p_no_input_annot\n",
    "        self.p_annot_noise_positive = p_annot_noise_positive\n",
    "        self.p_annot_noise_negative = p_annot_noise_negative\n",
    "        self.load_chunk_size = load_chunk_size\n",
    "        self.min_time_per_episode = min_time_per_episode\n",
    "        \n",
    "        self.episode_managers = [EpisodeDataManager(max_seq_len, batch_size, self.batches_per_epoch) for max_seq_len, batch_size in \\\n",
    "                EPISODE_SETTINGS]\n",
    "        self.episode_max_seq_lens = np.array([episode_manager.max_seq_len for episode_manager in self.episode_managers])\n",
    "        \n",
    "    def setup(self, dataset_handler, starting_sample_index = 0):\n",
    "        self.dataset_handler = dataset_handler\n",
    "        self.current_sample_index = starting_sample_index % self.dataset_handler.total_size\n",
    "        self._load_chunk()\n",
    "        self._select_new_episode()\n",
    "        return self._current_episode\n",
    "    \n",
    "    def determine_episode_and_ready_next_epoch(self):\n",
    "        \n",
    "        if self._episode_selection_time + self.min_time_per_episode <= datetime.now():\n",
    "            old_episode = self._current_episode\n",
    "            self._select_new_episode()\n",
    "            changed_episode = (self._current_episode is not old_episode)\n",
    "        else:\n",
    "            changed_episode = False\n",
    "            \n",
    "        while not self._current_episode.is_epoch_ready():\n",
    "            self._load_chunk()\n",
    "\n",
    "        return changed_episode, self._current_episode\n",
    "        \n",
    "    def create_next_epoch(self):\n",
    "        return self._encode_epoch(*self.create_next_epoch_X())\n",
    "        \n",
    "    def create_dummpy_epoch(self, size = 1):\n",
    "        return self._encode_epoch(*self.create_next_dummy_epoch_X(size))\n",
    "        \n",
    "    def create_next_epoch_X(self):\n",
    "        assert self._current_episode.is_epoch_ready()\n",
    "        return self._current_episode.encode_next_epoch()\n",
    "    \n",
    "    def create_next_dummy_epoch_X(self, size = 1):\n",
    "        \n",
    "        while not self._current_episode.is_epoch_ready(size):\n",
    "            self._load_chunk()\n",
    "            \n",
    "        return self._current_episode.encode_dummy_epoch(size)\n",
    "    \n",
    "    def _select_new_episode(self):\n",
    "        self._current_episode = max(self.episode_managers, key = lambda episode_manager: len(episode_manager.sample_cache))\n",
    "        self._episode_selection_time = datetime.now()\n",
    "            \n",
    "    def _load_chunk(self):\n",
    "        \n",
    "        chunk_sample_cache = self.dataset_handler[self.current_sample_index:(self.current_sample_index + self.load_chunk_size)]\n",
    "        self.current_sample_index += self.load_chunk_size\n",
    "        \n",
    "        if self.current_sample_index >= self.dataset_handler.total_size:\n",
    "            self.current_sample_index = 0\n",
    "            \n",
    "        self._assign_samples(chunk_sample_cache)\n",
    "        \n",
    "    def _assign_samples(self, sample_cache):\n",
    "        \n",
    "        seq_lens = np.array(list(map(len, sample_cache.seqs))) + ADDED_TOKENS_PER_SEQ\n",
    "        assigned_episode_indices = self._select_episodes_to_assign(seq_lens)\n",
    "        \n",
    "        for episode_manager_index, episode_manager in enumerate(self.episode_managers):\n",
    "            sample_indices_for_episode, = np.where(assigned_episode_indices == episode_manager_index)\n",
    "            episode_manager.sample_cache.extend(sample_cache.slice_indices(sample_indices_for_episode))\n",
    "        \n",
    "    def _select_episodes_to_assign(self, seq_lens, gamma = 1):\n",
    "        # The smaller the distance between a sample's sequence length to an episode's maximum sequence length, the higher the chance\n",
    "        # that it will be assigned to that episode.\n",
    "        samples_by_episodes_seq_len_ratio = seq_lens.reshape(-1, 1) / self.episode_max_seq_lens.reshape(1, -1)\n",
    "        samples_by_episodes_seq_len_symmetric_ratio = np.maximum(samples_by_episodes_seq_len_ratio, 1 / samples_by_episodes_seq_len_ratio)\n",
    "        raw_samples_by_episodes_probs = np.exp(-gamma * samples_by_episodes_seq_len_symmetric_ratio)\n",
    "        samples_by_episodes_probs = raw_samples_by_episodes_probs / raw_samples_by_episodes_probs.sum(axis = -1).reshape(-1, 1)\n",
    "        samples_by_episodes_cum_probs = samples_by_episodes_probs.cumsum(axis = -1)\n",
    "        assigned_episode_indices = (np.random.rand(len(seq_lens), 1) <= samples_by_episodes_cum_probs).argmax(axis = 1)\n",
    "        return assigned_episode_indices\n",
    "    \n",
    "    def _encode_epoch(self, encoded_seqs, encoded_annotation_masks):\n",
    "        \n",
    "        seqs_noise_mask = np.random.choice([True, False], encoded_seqs.shape, p = [1 - self.p_seq_noise, self.p_seq_noise])\n",
    "        random_seq_tokens = np.random.randint(0, n_tokens, encoded_seqs.shape)\n",
    "        noisy_encoded_seqs = np.where(seqs_noise_mask, encoded_seqs, random_seq_tokens)\n",
    "\n",
    "        noisy_annotations_when_positive = np.random.choice([True, False], encoded_annotation_masks.shape, \\\n",
    "                p = [1 - self.p_annot_noise_positive, self.p_annot_noise_positive])\n",
    "        noisy_annotations_when_negative = np.random.choice([True, False], encoded_annotation_masks.shape, \\\n",
    "                p = [self.p_annot_noise_negative, 1 - self.p_annot_noise_negative])\n",
    "        noisy_annotation_masks = np.where(encoded_annotation_masks, noisy_annotations_when_positive, \\\n",
    "                noisy_annotations_when_negative)\n",
    "        noisy_annotation_masks[np.random.choice([True, False], len(noisy_annotation_masks), p = [self.p_no_input_annot, \\\n",
    "                1 - self.p_no_input_annot]), :] = False\n",
    "\n",
    "        # When a protein has no annotations at all, we don't know whether it's because such annotations don't exist or just not found,\n",
    "        # so it's safer to set the loss weight of those annotations to zero.\n",
    "        seq_weights = np.ones(len(encoded_seqs))\n",
    "        annotation_weights = encoded_annotation_masks.any(axis = -1).astype(float)\n",
    "        \n",
    "        X = [noisy_encoded_seqs, noisy_annotation_masks.astype(np.int8)]\n",
    "        Y = [np.expand_dims(encoded_seqs, axis = -1), encoded_annotation_masks.astype(np.int8)]\n",
    "        sample_weigths = [seq_weights, annotation_weights]\n",
    "        \n",
    "        return X, Y, sample_weigths\n",
    "    \n",
    "class AutoSaveManager:\n",
    "    \n",
    "    def __init__(self, directory, every_epochs_to_save = 1, every_saves_to_keep = 25):\n",
    "        self.directory = directory\n",
    "        self.every_epochs_to_save = every_epochs_to_save\n",
    "        self.every_saves_to_keep = every_saves_to_keep\n",
    "        self.last_saved_path_to_delete = None\n",
    "        self.n_saves = 0\n",
    "    \n",
    "    def on_epoch_end(self, model, epoch_index, sample_index):\n",
    "        \n",
    "        if epoch_index % self.every_epochs_to_save != 0:\n",
    "            return\n",
    "        \n",
    "        save_path = os.path.join(self.directory, 'epoch_%d_sample_%d.pkl' % (epoch_index, sample_index))\n",
    "        save_model(model, save_path)\n",
    "        self.n_saves += 1\n",
    "        \n",
    "        if self.last_saved_path_to_delete is not None:\n",
    "            os.remove(self.last_saved_path_to_delete)\n",
    "            \n",
    "        if self.n_saves % self.every_saves_to_keep == 0:\n",
    "            self.last_saved_path_to_delete = None\n",
    "        else:\n",
    "            self.last_saved_path_to_delete = save_path\n",
    "    \n",
    "class ModelTrainer:\n",
    "    \n",
    "    def __init__(self, epoch_generator, autosave_manager, lr = 2e-04, annots_loss_weight = 1e03):\n",
    "        self.epoch_generator = epoch_generator\n",
    "        self.autosave_manager = autosave_manager\n",
    "        self.lr = lr\n",
    "        self.annots_loss_weight = annots_loss_weight\n",
    "        \n",
    "    def setup(self, dataset_handler, resume_epoch = None):\n",
    "        \n",
    "        if resume_epoch is None:\n",
    "            self.current_epoch_index = 0\n",
    "            starting_sample_index = 0\n",
    "            resumed_weights_file_path = None\n",
    "        else:\n",
    "            self.current_epoch_index, starting_sample_index = resume_epoch\n",
    "            self.current_epoch_index += 1\n",
    "            resumed_weights_file_path = os.path.join(run_weights_dir, 'epoch_%d_sample_%d.pkl' % resume_epoch)\n",
    "        \n",
    "        starting_episode = self.epoch_generator.setup(dataset_handler, starting_sample_index)\n",
    "        log('Starting with episode with max_seq_len = %d.' % starting_episode.max_seq_len)\n",
    "        \n",
    "        self.model = self._get_create_and_compile_model_function(starting_episode.max_seq_len, will_weights_be_reloaded = \\\n",
    "                (resumed_weights_file_path is not None))()\n",
    "        self.model.summary()\n",
    "        \n",
    "        if resumed_weights_file_path is not None:\n",
    "            load_model(self.model, resumed_weights_file_path)\n",
    "            log('Loaded weights from %s.' % resumed_weights_file_path)\n",
    "        \n",
    "    def train_next_epoch(self, autosave_if_needed = True):\n",
    "        \n",
    "        changed_episode, episode = self.epoch_generator.determine_episode_and_ready_next_epoch()\n",
    "        \n",
    "        if changed_episode:\n",
    "            log('Starting a new episode with max_seq_len = %d.' % episode.max_seq_len)\n",
    "            self.model = recreate_model_with_same_state(self.model, self._get_create_and_compile_model_function(episode.max_seq_len, \\\n",
    "                    will_weights_be_reloaded = True))\n",
    "        \n",
    "        X, Y, sample_weigths = self.epoch_generator.create_next_epoch()\n",
    "        log('Epoch %d (current sample %d):' % (self.current_epoch_index, self.epoch_generator.current_sample_index))\n",
    "        self.model.fit(X, Y, sample_weight = sample_weigths, batch_size = episode.batch_size, callbacks = [WandbCallback()])\n",
    "        \n",
    "        if autosave_if_needed:\n",
    "            self.autosave_manager.on_epoch_end(self.model, self.current_epoch_index, self.epoch_generator.current_sample_index)\n",
    "            \n",
    "        self.current_epoch_index += 1\n",
    "        \n",
    "    def train_forever(self, autosave = True):\n",
    "        while True:\n",
    "            self.train_next_epoch(autosave_if_needed = autosave)\n",
    "            \n",
    "    def _get_create_and_compile_model_function(self, max_seq_len, will_weights_be_reloaded = False):\n",
    "        \n",
    "        def create_and_compile_model_function():\n",
    "            \n",
    "            model = create_model(max_seq_len)\n",
    "            self._compile_model(model)\n",
    "            \n",
    "            if will_weights_be_reloaded:\n",
    "                self._train_for_a_dummy_epoch(model)\n",
    "            \n",
    "            return model\n",
    "        \n",
    "        return create_and_compile_model_function\n",
    "    \n",
    "    def _compile_model(self, model):\n",
    "        model.compile(optimizer = keras.optimizers.Adam(lr = self.lr), loss = ['sparse_categorical_crossentropy', \\\n",
    "                'binary_crossentropy'], loss_weights = [1, self.annots_loss_weight])\n",
    "        \n",
    "    def _train_for_a_dummy_epoch(self, model):\n",
    "        '''\n",
    "        For some reason keras requires this strange little hack in order to properly initialize a new model's optimizer, so that\n",
    "        the optimizer's weights can be reloaded from an existing state.\n",
    "        '''\n",
    "        X, Y, sample_weigths = self.epoch_generator.create_dummpy_epoch(size = 1)\n",
    "        model.fit(X, Y, batch_size = 1, verbose = 0)\n",
    "        \n",
    "# RESUME_EPOCH = None\n",
    "RESUME_EPOCH = (50, 600000)\n",
    "        \n",
    "np.random.seed(0)\n",
    "epoch_generator = EpochGenerator()\n",
    "autosave_manager = AutoSaveManager(auto_save_weights_dir, every_epochs_to_save = 10)\n",
    "model_trainer = ModelTrainer(epoch_generator, autosave_manager)\n",
    "\n",
    "with h5py.File(H5_FILE_PATH, 'r') as h5f:\n",
    "    model_trainer.setup(DatasetHandler(h5f), resume_epoch = RESUME_EPOCH)\n",
    "    model_trainer.train_forever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation on test-set samples\n",
    "\n",
    "(The code is somewhat patchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "\n",
    "with h5py.File(H5_FILE_PATH, 'r') as h5f:\n",
    "    test_set_indices, = np.where(h5f['test_set_similarity'][:])\n",
    "    chosen_sample_indices = list(sorted(np.random.choice(test_set_indices, N)))\n",
    "    del test_set_indices\n",
    "    chosen_samples = DatasetHandler(h5f)[chosen_sample_indices]\n",
    "    chosen_uniprot_ids = h5f['uniprot_ids'][chosen_sample_indices]\n",
    "    \n",
    "# So that true annotations won't be masked away here.\n",
    "chosen_samples.test_set_flags = [False for _ in range(N)]\n",
    "    \n",
    "seq_lens = np.array(list(map(len, chosen_samples.seqs)))\n",
    "max_seq_len = seq_lens.max()\n",
    "model_max_seq_len = min(max_seq_len, 1000)\n",
    "model_max_seq_len = 450 # We end up just forcing 450.\n",
    "log('max_seq_len = %d, model_max_seq_len = %d' % (max_seq_len, model_max_seq_len))\n",
    "\n",
    "episode = EpisodeDataManager(model_max_seq_len, batch_size = 1, batches_per_epoch = 1)\n",
    "episode.sample_cache = chosen_samples\n",
    "epoch_generator._current_episode = episode\n",
    "\n",
    "model = recreate_model_with_same_state(model_trainer.model, model_trainer._get_create_and_compile_model_function(model_max_seq_len, \\\n",
    "        will_weights_be_reloaded = True))\n",
    "    \n",
    "for uniprot_id in chosen_uniprot_ids:\n",
    "    \n",
    "    print('UniProt ID: %s (https://www.uniprot.org/uniprot/%s)' % (uniprot_id, uniprot_id))\n",
    "\n",
    "    X, Y_true, _ = epoch_generator._encode_epoch(*episode.encode_next_epoch())\n",
    "    Y_pred = model.predict(X)\n",
    "    \n",
    "    X_seqs, X_annots = X\n",
    "    Y_true_seqs, Y_true_annots = Y_true\n",
    "    Y_pred_seqs, Y_pred_annots = Y_pred\n",
    "\n",
    "    X_seqs = X_seqs.flatten()\n",
    "    X_annots = X_annots.flatten()\n",
    "    Y_true_seqs = Y_true_seqs.flatten()\n",
    "    Y_true_annots = Y_true_annots.flatten()\n",
    "    Y_pred_seqs = Y_pred_seqs[0, :, :]\n",
    "    Y_pred_annots = Y_pred_annots.flatten()\n",
    "    Y_pred_seqs_max = Y_pred_seqs.argmax(axis = -1)\n",
    "\n",
    "    seq_result = pd.DataFrame()\n",
    "    seq_result['true'] = list(map(index_to_token.get, Y_true_seqs))\n",
    "    seq_result['input'] = list(map(index_to_token.get, X_seqs))\n",
    "    seq_result['max'] = list(map(index_to_token.get, Y_pred_seqs_max))\n",
    "    seq_result['p_true'] = Y_pred_seqs[np.arange(model_max_seq_len), Y_true_seqs]\n",
    "    seq_result['p_input'] = Y_pred_seqs[np.arange(model_max_seq_len), X_seqs]\n",
    "    seq_result['p_max'] = Y_pred_seqs[np.arange(model_max_seq_len), Y_pred_seqs_max]\n",
    "\n",
    "    print('Sequence results:')\n",
    "\n",
    "    with pd.option_context('display.max_columns', model_max_seq_len):\n",
    "        display(seq_result[(seq_result['true'] != seq_result['input']) | (seq_result['p_true'] < 0.9)].transpose())\n",
    "\n",
    "    true_annots, = np.where(Y_true_annots)\n",
    "    input_annots, = np.where(X_annots)\n",
    "    relevant_annots = sorted(set(true_annots) | set(input_annots) | set(np.where(Y_pred_annots >= 0.05)[0]))\n",
    "\n",
    "    print('Annotation results:')\n",
    "    print('True annotations: %s' % true_annots)\n",
    "    print('Input annotations: %s' % input_annots)\n",
    "    print('Predicted annotations: %s' % ', '.join('%d (%.2g)' % (annot, Y_pred_annots[annot]) for annot in relevant_annots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX Check the consistency of the model after reloading the weights.\n",
    "\n",
    "def create_and_compile_model_function():\n",
    "    global model_max_seq_len, model_trainer, X, Y_true\n",
    "    model = create_model(model_max_seq_len)\n",
    "    model_trainer._compile_model(model)\n",
    "    model.fit(X, Y_true, batch_size = 1, verbose = 0)\n",
    "    return model\n",
    "\n",
    "def train_model(model):\n",
    "    \n",
    "    global X, Y_true\n",
    "\n",
    "    for _ in range(10):\n",
    "        model.fit(X, Y_true, batch_size = 1, verbose = 0)\n",
    "\n",
    "train_model(model)\n",
    "\n",
    "model1 = recreate_model_with_same_state(model, create_and_compile_model_function)\n",
    "train_model(model1)\n",
    "Y_pred1 = model1.predict(X)\n",
    "\n",
    "model2 = recreate_model_with_same_state(model, create_and_compile_model_function)\n",
    "train_model(model2)\n",
    "Y_pred2 = model2.predict(X)\n",
    "\n",
    "for y_pred1, y_pred2 in zip(Y_pred1, Y_pred2):\n",
    "    print(np.abs(y_pred1 - y_pred2).max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_python",
   "language": "python",
   "name": "my_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
