{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from pwas.shared_utils.util import log, get_chunk_intervals\n",
    "\n",
    "with open('/cs/phd/nadavb/github_projects/keras_seq_vec_transformer/keras_seq_vec_transformer.py', 'r') as f:\n",
    "    '''\n",
    "    Some horrible hacks. To make keras_seq_vec_transformer work with tf.keras instead of just keras we run the following instead of\n",
    "    just exec(f.read()).\n",
    "    Note that also above we run from tensorflow import keras instead of just import keras\n",
    "    '''\n",
    "    import tensorflow.keras.backend as K \n",
    "    from tensorflow.keras.layers import LayerNormalization\n",
    "    exec('\\n'.join([line for line in f.read().splitlines() if not line.startswith('import keras') and not \\\n",
    "            line.startswith('from keras_layer_normalization')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_if_not_exists(directory):\n",
    "    if not os.path.isdir(directory):\n",
    "        os.mkdir(directory)\n",
    "\n",
    "def save_model(model, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump((model.get_weights(), model.optimizer.get_weights()), f)\n",
    "        \n",
    "def load_model(model, path):\n",
    "    with open(path, 'rb') as f:\n",
    "        model_weights, optimizer_weights = pickle.load(f)\n",
    "        model.set_weights(model_weights)\n",
    "        model.optimizer.set_weights(optimizer_weights)\n",
    "\n",
    "class AutoSaveCallback(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, directory, epochs_offset = 0, every_epochs_to_save = 1, every_saves_to_keep = 25):\n",
    "        self.directory = directory\n",
    "        self.epochs_offset = epochs_offset\n",
    "        self.every_epochs_to_save = every_epochs_to_save\n",
    "        self.every_saves_to_keep = every_saves_to_keep\n",
    "        self.last_saved_path_to_delete = None\n",
    "        self.n_saves = 0\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        \n",
    "        epoch += self.epochs_offset + 1\n",
    "        \n",
    "        if epoch % self.every_epochs_to_save != 0:\n",
    "            return\n",
    "        \n",
    "        save_path = os.path.join(self.directory, 'epoch-%d.pkl' % epoch)\n",
    "        save_model(self.model, save_path)\n",
    "        self.n_saves += 1\n",
    "        \n",
    "        if self.last_saved_path_to_delete is not None:\n",
    "            os.remove(self.last_saved_path_to_delete)\n",
    "            \n",
    "        if self.n_saves % self.every_saves_to_keep != 0:\n",
    "            self.last_saved_path_to_delete = save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/nadavb/protobert\" target=\"_blank\">https://app.wandb.ai/nadavb/protobert</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/nadavb/protobert/runs/2yuphi1y\" target=\"_blank\">https://app.wandb.ai/nadavb/protobert/runs/2yuphi1y</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "W&B Run: https://app.wandb.ai/nadavb/protobert/runs/2yuphi1y"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project = 'protobert', dir = '/cs/phd/nadavb/my_storage/wandb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 400\n",
    "N_TEST_SET = 1000000\n",
    "\n",
    "ALL_AAS = 'ACDEFGHIKLMNPQRSTUVWXY'\n",
    "\n",
    "H5_FILE_PATH = '/cs/phd/nadavb/cafa_project/data/dataset.h5'\n",
    "\n",
    "BASE_WEIGHTS_DIR = '/cs/phd/nadavb/cafa_project/data/model_weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_weights_dir = os.path.join(BASE_WEIGHTS_DIR, wandb.run.name)\n",
    "mkdir_if_not_exists(run_weights_dir)\n",
    "\n",
    "auto_save_weights_dir = os.path.join(run_weights_dir, 'autosave')\n",
    "mkdir_if_not_exists(auto_save_weights_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8943 unique annotations.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAD8CAYAAACyyUlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAE1JJREFUeJzt3X+s3Xd93/HnCyeBaBQSiGGR7dSR6j8a0IBwFTxFmjLSJU6ocKSBZNQ1BmWyxIJGtUlt6B+LCq0U/ildNkrlEQuHtQ0R/REvJPPcBFRNgpAbSBNCynKXMnKVCJs4hCA2KtP3/jgfh8Pl2Pec+7m+51zf50M6ut/v+/v5ns/H3zh+3c/3xzmpKiRJ6vGKaQ9AkrT+GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkrqdM+0BrJWLLrqotm/fPu1hSNK68sgjj3yvqjYv127DhMn27duZn5+f9jAkaV1J8n/GaedpLklSN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVK3scIkybeTPJ7k0STzrfa6JEeSPNV+XtjqSXJ7koUkjyW5fOh99rb2TyXZO1R/e3v/hbZvVtqHNIntt3zh5ZeklZtkZvLPq+qtVTXX1m8BHqiqHcADbR3gOmBHe+0DPgWDYABuBd4BXAHcejIcWpt9Q/vtWkkfkqTp6DnNtRs42JYPAjcM1e+sga8AFyS5GLgWOFJVx6vqBeAIsKtte01VfbmqCrhzyXtN0ockaQrGDZMC/keSR5Lsa7U3VtVzAO3nG1p9C/DM0L6LrXa6+uKI+kr6kCRNwbifGnxlVT2b5A3AkSR/e5q2GVGrFdRPZ6x9WvDtA7jkkkuWeUtJ0kqNFSZV9Wz7eTTJXzC45vHdJBdX1XPtFNPR1nwR2Da0+1bg2Va/akn9S62+dUR7VtDH0nHvB/YDzM3NLRdQ2uCGL8J/+7Z3TXEk0vqz7GmuJP8oyS+cXAauAb4BHAJO3pG1F7inLR8Cbmx3XO0EXmynqA4D1yS5sF14vwY43La9lGRnu4vrxiXvNUkfkqQpGGdm8kbgL9rduucAf1JV/z3Jw8DdSW4CvgO8t7W/D7geWAB+BHwAoKqOJ/kY8HBr99GqOt6WPwh8BjgfuL+9AG6bpA9J0nQsGyZV9TTwlhH154GrR9QLuPkU73UAODCiPg+8eTX6kCStPZ+AlyR1M0wkSd0ME0lSN8NEktTNMJEkdRv3CXhpQ/EBRmkyzkwkSd2cmWhDcKYhnVnOTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN28N1oYzfJvwpO29rVgazZmJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuvmciTQBnzmRRnNmIknqZphIkroZJpKkboaJJKmbYSJJ6ubdXDqrTPqJwJJWhzMTSVI3w0SS1M0wkSR1GztMkmxK8vUk97b1S5M8lOSpJJ9Lcl6rv7KtL7Tt24fe4yOt/q0k1w7Vd7XaQpJbhuoT9yFJWnuTzEw+DDw5tP5x4BNVtQN4Abip1W8CXqiqXwI+0dqR5DJgD/AmYBfwhy2gNgGfBK4DLgPe19pO3IckaTrGCpMkW4F3AZ9u6wHeCXy+NTkI3NCWd7d12varW/vdwF1V9eOq+jtgAbiivRaq6umq+nvgLmD3CvuQJE3BuDOTPwB+E/iHtv564PtVdaKtLwJb2vIW4BmAtv3F1v7l+pJ9TlVfSR8/I8m+JPNJ5o8dOzbmH1WSNKllwyTJrwJHq+qR4fKIprXMttWqL9f/TwtV+6tqrqrmNm/ePGIXSdJqGOehxSuBdye5HngV8BoGM5ULkpzTZgZbgWdb+0VgG7CY5BzgtcDxofpJw/uMqn9vBX1IkqZg2ZlJVX2kqrZW1XYGF9AfrKpfA74IvKc12wvc05YPtXXa9gerqlp9T7sT61JgB/BV4GFgR7tz67zWx6G2z6R9SJKmoOfjVH4LuCvJ7wJfB+5o9TuAzyZZYDBb2ANQVU8kuRv4JnACuLmqfgKQ5EPAYWATcKCqnlhJH9Ja8ouypJ/KRvmFfm5urubn56c9DJ1h0/psLsNEZ6skj1TV3HLtfAJektTNMJEkdTNMJEndDBNJUjfDRJLUzW9a1LrntytK0+fMRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR189ZgrUveDizNFmcmkqRuzky0bjgbkWaXYSKtAr8oSxudp7kkSd0ME0lSN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHXzoUXNHJ90l9YfZyaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqduyYZLkVUm+muRvkjyR5Hda/dIkDyV5KsnnkpzX6q9s6wtt+/ah9/pIq38rybVD9V2ttpDklqH6xH1IktbeODOTHwPvrKq3AG8FdiXZCXwc+ERV7QBeAG5q7W8CXqiqXwI+0dqR5DJgD/AmYBfwh0k2JdkEfBK4DrgMeF9ry6R9SJKmY9kwqYEfttVz26uAdwKfb/WDwA1teXdbp22/Okla/a6q+nFV/R2wAFzRXgtV9XRV/T1wF7C77TNpH5KkKRjrmkmbQTwKHAWOAP8b+H5VnWhNFoEtbXkL8AxA2/4i8Prh+pJ9TlV//Qr6kCRNwVhhUlU/qaq3AlsZzCR+eVSz9nPUDKFWsX66Pn5Gkn1J5pPMHzt2bMQukqTVMNHdXFX1feBLwE7ggiQnP9trK/BsW14EtgG07a8Fjg/Xl+xzqvr3VtDH0vHur6q5qprbvHnzJH9USdIExrmba3OSC9ry+cCvAE8CXwTe05rtBe5py4faOm37g1VVrb6n3Yl1KbAD+CrwMLCj3bl1HoOL9IfaPpP2oXVq+y1fePklaf0Z51ODLwYOtruuXgHcXVX3JvkmcFeS3wW+DtzR2t8BfDbJAoPZwh6Aqnoiyd3AN4ETwM1V9ROAJB8CDgObgANV9UR7r9+apA9J0nQsGyZV9RjwthH1pxlcP1la/3/Ae0/xXr8H/N6I+n3AfavRhyRp7fkEvCSpm1+OJa2y4es+377tXVMcibR2nJlIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6+U2LOqNO962Dw9skrW+GidaM4SGdvQwT6Qzy++C1UXjNRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdVs2TJJsS/LFJE8meSLJh1v9dUmOJHmq/byw1ZPk9iQLSR5LcvnQe+1t7Z9Ksneo/vYkj7d9bk+SlfYhSVp748xMTgD/vqp+GdgJ3JzkMuAW4IGq2gE80NYBrgN2tNc+4FMwCAbgVuAdwBXArSfDobXZN7TfrlafqA9J0nQsGyZV9VxVfa0tvwQ8CWwBdgMHW7ODwA1teTdwZw18BbggycXAtcCRqjpeVS8AR4BdbdtrqurLVVXAnUvea5I+JElTMNE1kyTbgbcBDwFvrKrnYBA4wBtasy3AM0O7Lbba6eqLI+qsoA9J0hSM/RH0SV4N/BnwG1X1g3ZZY2TTEbVaQf20wxlnnyT7GJwG45JLLlnmLaUzy4+j19lsrDBJci6DIPnjqvrzVv5ukour6rl2iuloqy8C24Z23wo82+pXLal/qdW3jmi/kj5+RlXtB/YDzM3NLRdQmpD/OEo6aZy7uQLcATxZVb8/tOkQcPKOrL3APUP1G9sdVzuBF9spqsPANUkubBferwEOt20vJdnZ+rpxyXtN0ockaQrGmZlcCfw68HiSR1vtt4HbgLuT3AR8B3hv23YfcD2wAPwI+ABAVR1P8jHg4dbuo1V1vC1/EPgMcD5wf3sxaR+SpOlYNkyq6n8y+hoFwNUj2hdw8yne6wBwYER9HnjziPrzk/ah6fCUl7Sx+QS8JKnb2HdzSeManqVI2hicmUiSuhkmkqRuhokkqZthIknq5gV4TcSL66vDW6l1tnFmIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSermE/Balk+9S1qOYSJN2dKw9uNVtB55mkuS1M0wkSR18zSXRvI6iaRJODORJHUzTCRJ3QwTSVI3r5lIM8ZvYdR65MxEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3ZYNkyQHkhxN8o2h2uuSHEnyVPt5Yasnye1JFpI8luTyoX32tvZPJdk7VH97ksfbPrcnyUr7kCRNxzgzk88Au5bUbgEeqKodwANtHeA6YEd77QM+BYNgAG4F3gFcAdx6Mhxam31D++1aSR/S2Wj7LV94+SXNsmXDpKr+Gji+pLwbONiWDwI3DNXvrIGvABckuRi4FjhSVcer6gXgCLCrbXtNVX25qgq4c8l7TdKHJGlKVnrN5I1V9RxA+/mGVt8CPDPUbrHVTldfHFFfSR8/J8m+JPNJ5o8dOzbRH1CSNL7VvgCfEbVaQX0lffx8sWp/Vc1V1dzmzZuXeVtJ0kqtNEy+e/LUUvt5tNUXgW1D7bYCzy5T3zqivpI+JElTstIwOQScvCNrL3DPUP3GdsfVTuDFdorqMHBNkgvbhfdrgMNt20tJdra7uG5c8l6T9CFJmpJlP4I+yZ8CVwEXJVlkcFfWbcDdSW4CvgO8tzW/D7geWAB+BHwAoKqOJ/kY8HBr99GqOnlR/4MM7hg7H7i/vZi0D0nS9CwbJlX1vlNsunpE2wJuPsX7HAAOjKjPA28eUX9+0j4kSdPhl2NJ64RfmqVZ5sepSJK6GSaSpG6GiSSpm2EiSermBXhpHfJivGaNMxNJUjdnJnqZH3MuaaWcmUiSuhkmkqRuhokkqZvXTKR1zju7NAucmUiSuhkmkqRuhokkqZvXTDY4ny2RtBoME+ks4sV4TYunuSRJ3QwTSVI3w0SS1M1rJhuQF90lrTbDZIMwQDYeL8ZrLXmaS5LUzTCRJHXzNNdZzFNbOslTXjrTnJlIkro5MznLOBvRcpyl6ExwZiJJ6ubMRNrAnKVotRgmZwFPbWk1GCzqYZisUwaIziSDRZNat2GSZBfwH4FNwKer6rYpD0k6KxksGse6DJMkm4BPAv8CWAQeTnKoqr453ZFJZzeDRaeyLsMEuAJYqKqnAZLcBewGzoow8RSW1oNx/p4aOBvHeg2TLcAzQ+uLwDvWehDj/pZmOGijWs2/+wbTbFuvYZIRtfq5Rsk+YF9b/WGSb03Qx0XA98Ye0McneOfVNdE4p8hxrq4NN84z/P/YhjueE/jFcRqt1zBZBLYNrW8Fnl3aqKr2A/tX0kGS+aqaW9nw1o7jXF2Oc3U5ztU1y+Ncr0/APwzsSHJpkvOAPcChKY9JkjasdTkzqaoTST4EHGZwa/CBqnpiysOSpA1rXYYJQFXdB9x3BrtY0emxKXCcq8txri7Hubpmdpyp+rnr1pIkTWS9XjORJM2QDR0mSQ4kOZrkG6fYniS3J1lI8liSy9d6jG0cy43zqiQvJnm0vf7DWo+xjWNbki8meTLJE0k+PKLN1I/pmOOc+jFN8qokX03yN22cvzOizSuTfK4dz4eSbJ/Rcb4/ybGh4/mv13qcQ2PZlOTrSe4dsW3qx3NoLKcb58wcz5dV1YZ9Af8MuBz4xim2Xw/cz+C5lp3AQzM6zquAe2fgeF4MXN6WfwH4X8Bls3ZMxxzn1I9pO0avbsvnAg8BO5e0+TfAH7XlPcDnZnSc7wf+8zSP59BY/h3wJ6P++87C8RxznDNzPE++NvTMpKr+Gjh+mia7gTtr4CvABUkuXpvR/dQY45wJVfVcVX2tLb8EPMng0wqGTf2YjjnOqWvH6Idt9dz2WnqRczdwsC1/Hrg6yaiHes+YMcc5E5JsBd4FfPoUTaZ+PGGscc6cDR0mYxj1sS0z949O80/baYb7k7xp2oNppwfexuC31GEzdUxPM06YgWPaTnU8ChwFjlTVKY9nVZ0AXgRev7ajHGucAP+yndr8fJJtI7avhT8AfhP4h1Nsn4njyfLjhNk4ni8zTE5vrI9tmQFfA36xqt4C/CfgL6c5mCSvBv4M+I2q+sHSzSN2mcoxXWacM3FMq+onVfVWBp/ycEWSNy9pMhPHc4xx/jdge1X9E+Cv+Olv/2smya8CR6vqkdM1G1Fb0+M55jinfjyXMkxOb6yPbZm2qvrBydMMNXj+5twkF01jLEnOZfAP9B9X1Z+PaDITx3S5cc7SMW1j+D7wJWDXkk0vH88k5wCvZYqnRE81zqp6vqp+3Fb/C/D2NR4awJXAu5N8G7gLeGeS/7qkzSwcz2XHOSPH82cYJqd3CLix3YG0E3ixqp6b9qCWSvKPT57XTXIFg/+uz09hHAHuAJ6sqt8/RbOpH9NxxjkLxzTJ5iQXtOXzgV8B/nZJs0PA3rb8HuDBaldo18o441xyXezdDK5Tramq+khVba2q7Qwurj9YVf9qSbOpH89xxjkLx3OpdfsE/GpI8qcM7tq5KMkicCuDi4dU1R8xeML+emAB+BHwgRkd53uADyY5AfxfYM9a/w/QXAn8OvB4O38O8NvAJUNjnYVjOs44Z+GYXgwczODL4F4B3F1V9yb5KDBfVYcYhOJnkyww+A16zxqPcdxx/tsk7wZOtHG+fwrjHGkGj+dIs348fQJektTN01ySpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkrr9fzCTdxtn0/IFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with h5py.File(H5_FILE_PATH, 'r') as h5f:\n",
    "    included_annotation_indices = h5f['included_annotation_indices'][:]\n",
    "    seq_lengths = h5f['seq_lengths'][:]\n",
    "    \n",
    "n_annotations = len(included_annotation_indices)\n",
    "print('%d unique annotations.' % n_annotations)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "_ = ax.hist(np.log10(seq_lengths), bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set size: 77352183 proteins, training-set size: 1000000 proteins.\n"
     ]
    }
   ],
   "source": [
    "relevant_seq_length_indice, = np.where(seq_lengths <= MAX_SEQ_LENGTH)\n",
    "\n",
    "np.random.seed(0)\n",
    "test_set_mask = np.zeros_like(relevant_seq_length_indice, dtype = bool)\n",
    "test_set_mask[:N_TEST_SET] = True\n",
    "np.random.shuffle(test_set_mask)\n",
    "\n",
    "test_set_indices = relevant_seq_length_indice[test_set_mask]\n",
    "training_set_indices = relevant_seq_length_indice[~test_set_mask]\n",
    "\n",
    "del test_set_mask, relevant_seq_length_indice\n",
    "\n",
    "print('Test-set size: %d proteins, training-set size: %d proteins.' % (len(training_set_indices), len(test_set_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_aas = len(ALL_AAS)\n",
    "other_aa_token = n_aas\n",
    "pad_token = n_aas + 1\n",
    "n_seq_tokens = n_aas + 2\n",
    "aa_to_token = {aa: i for i, aa in enumerate(ALL_AAS)}\n",
    "\n",
    "token_to_str = dict(enumerate(ALL_AAS))\n",
    "token_to_str[n_aas] = '<OTHER>'\n",
    "token_to_str[n_aas + 1] = '<PAD>'\n",
    "\n",
    "def tokenize_seqs(seqs, max_length):\n",
    "    \n",
    "    tokenized_seqs = np.zeros((len(seqs), max_length), dtype = np.int8)\n",
    "    \n",
    "    for i, seq in enumerate(seqs):\n",
    "        \n",
    "        assert len(seq) <= max_length\n",
    "        \n",
    "        for j, aa in enumerate(seq):\n",
    "            tokenized_seqs[i, j] = aa_to_token.get(aa, other_aa_token)\n",
    "            \n",
    "        tokenized_seqs[i, len(seq):] = pad_token\n",
    "        \n",
    "    return tokenized_seqs\n",
    "\n",
    "def iter_dataset(indices, chunk_size = 10000, record_offset = 0):\n",
    "    \n",
    "    indices = indices[(record_offset % len(indices)):]\n",
    "    \n",
    "    with h5py.File(H5_FILE_PATH, 'r') as h5f: \n",
    "        for i1, i2 in get_chunk_intervals(len(indices), chunk_size):\n",
    "            \n",
    "            chunk_indices = indices[i1:i2]\n",
    "            chunk_seqs = h5f['seqs'][chunk_indices]\n",
    "            chunk_annotation_masks = h5f['annotation_masks'][chunk_indices]\n",
    "            \n",
    "            for seq, annotation_mask in zip(chunk_seqs, chunk_annotation_masks):\n",
    "                yield seq, annotation_mask\n",
    "                \n",
    "def iter_dataset_forever(indices, chunk_size = 10000, record_offset = 0):\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        iteration += 1\n",
    "        log('Iteration #%d over the datasdet...' % iteration)\n",
    "        \n",
    "        if record_offset != 0:\n",
    "            log('Starting from record %d.' % record_offset)\n",
    "        \n",
    "        for seq, annotation_mask in iter_dataset(indices, chunk_size = chunk_size, record_offset = record_offset):\n",
    "            yield seq, annotation_mask\n",
    "            \n",
    "        record_offset = 0\n",
    "            \n",
    "def generate_batches(dataset_indices, p_seq_noise = 0.1, p_no_input_annot = 0.5, p_annot_noise_positive = 0.25, \\\n",
    "        p_annot_noise_negative = 1e-04, batch_size = 64, chunk_size = 10000, record_offset = 0):\n",
    "    \n",
    "    dataset_iter = iter_dataset_forever(dataset_indices, chunk_size = chunk_size, record_offset = record_offset)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        batch_data = [next(dataset_iter) for _ in range(batch_size)]\n",
    "        batch_seqs, batch_annotation_masks = map(list, zip(*batch_data))\n",
    "\n",
    "        batch_true_seqs_tokens = tokenize_seqs(batch_seqs, MAX_SEQ_LENGTH)\n",
    "        batch_true_annotation_masks = np.concatenate([annotation_mask.reshape(1, -1) for annotation_mask in batch_annotation_masks], axis = 0)\n",
    "        \n",
    "        batch_seqs_mask = np.random.choice([True, False], batch_true_seqs_tokens.shape, p = [1 - p_seq_noise, p_seq_noise])\n",
    "        batch_random_seqs_tokens = np.random.randint(0, n_seq_tokens, batch_true_seqs_tokens.shape)\n",
    "        batch_noisy_seqs_tokens = np.where(batch_seqs_mask, batch_true_seqs_tokens, batch_random_seqs_tokens)\n",
    "        \n",
    "        batch_noisy_annotations_when_positive = np.random.choice([True, False], batch_true_annotation_masks.shape, \\\n",
    "                p = [1 - p_annot_noise_positive, p_annot_noise_positive])\n",
    "        batch_noisy_annotations_when_negative = np.random.choice([True, False], batch_true_annotation_masks.shape, \\\n",
    "                p = [p_annot_noise_negative, 1 - p_annot_noise_negative])\n",
    "        batch_noisy_annotation_masks = np.where(batch_true_annotation_masks, batch_noisy_annotations_when_positive, \\\n",
    "                batch_noisy_annotations_when_negative)\n",
    "        batch_noisy_annotation_masks[np.random.choice([True, False], batch_size, p = [p_no_input_annot, 1 - p_no_input_annot]), :] = False\n",
    "        \n",
    "        yield [batch_noisy_seqs_tokens, batch_noisy_annotation_masks.astype(np.int8)], [np.expand_dims(batch_true_seqs_tokens, axis = -1), \\\n",
    "                batch_true_annotation_masks.astype(np.int8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input-seq (InputLayer)          [(None, 400)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input-annotations (InputLayer)  [(None, 8943)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "auto-encoder (TransformerAutoEn [(None, 400, 24), (N 24987399    input-seq[0][0]                  \n",
      "                                                                 input-annotations[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "output_seq_layer (Reshape)      (None, 400, 24)      0           auto-encoder[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_annoatations_layer (Resh (None, 8943)         0           auto-encoder[0][1]               \n",
      "==================================================================================================\n",
      "Total params: 24,987,399\n",
      "Trainable params: 24,987,399\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_seq_layer = keras.layers.Input(shape = (MAX_SEQ_LENGTH,), dtype = np.int32, name = 'input-seq')\n",
    "input_annoatations_layer = keras.layers.Input(shape = (n_annotations,), dtype = np.float32, name = 'input-annotations')\n",
    "output_seq_layer, output_annoatations_layer = TransformerAutoEncoder(vocab_size = n_seq_tokens, d_vec = n_annotations, \\\n",
    "        output_vec_activation = 'sigmoid', name = 'auto-encoder')([input_seq_layer, input_annoatations_layer])\n",
    "\n",
    "# An ugly hack meant to achieve nothing than renaming those layers.\n",
    "output_seq_layer = keras.layers.Reshape(output_seq_layer.shape[1:], name = 'output_seq_layer')(output_seq_layer)\n",
    "output_annoatations_layer = keras.layers.Reshape(output_annoatations_layer.shape[1:], name = 'output_annoatations_layer')\\\n",
    "        (output_annoatations_layer)\n",
    "\n",
    "model = keras.models.Model(inputs = [input_seq_layer, input_annoatations_layer], outputs = [output_seq_layer, output_annoatations_layer])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "wandb: Wandb version 0.9.1 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model /cs/phd/nadavb/cafa_project/data/model_weights/whole-donkey-13/epoch-5240.pkl...\n",
      "[2020_06_11-10:50:40] Iteration #1 over the datasdet...\n",
      "Epoch 1/100000\n",
      "[2020_06_11-10:50:40] Starting from record 16768000.\n",
      "100/100 [==============================] - 45s 448ms/step - loss: 0.2802 - output_seq_layer_loss: 0.2624 - output_annoatations_layer_loss: 1.7837e-04\n",
      "Epoch 2/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2785 - output_seq_layer_loss: 0.2604 - output_annoatations_layer_loss: 1.8106e-04\n",
      "Epoch 3/100000\n",
      "100/100 [==============================] - 42s 423ms/step - loss: 0.2793 - output_seq_layer_loss: 0.2601 - output_annoatations_layer_loss: 1.9242e-04\n",
      "Epoch 4/100000\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.2826 - output_seq_layer_loss: 0.2621 - output_annoatations_layer_loss: 2.0486e-04\n",
      "Epoch 5/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2782 - output_seq_layer_loss: 0.2581 - output_annoatations_layer_loss: 2.0101e-04\n",
      "Epoch 6/100000\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.2825 - output_seq_layer_loss: 0.2631 - output_annoatations_layer_loss: 1.9356e-04\n",
      "Epoch 7/100000\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.2840 - output_seq_layer_loss: 0.2639 - output_annoatations_layer_loss: 2.0048e-04\n",
      "Epoch 8/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2846 - output_seq_layer_loss: 0.2631 - output_annoatations_layer_loss: 2.1529e-04\n",
      "Epoch 9/100000\n",
      "100/100 [==============================] - 42s 425ms/step - loss: 0.2806 - output_seq_layer_loss: 0.2616 - output_annoatations_layer_loss: 1.8983e-04\n",
      "Epoch 10/100000\n",
      "100/100 [==============================] - 58s 582ms/step - loss: 0.2790 - output_seq_layer_loss: 0.2599 - output_annoatations_layer_loss: 1.9080e-04\n",
      "Epoch 11/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2815 - output_seq_layer_loss: 0.2614 - output_annoatations_layer_loss: 2.0053e-04\n",
      "Epoch 12/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2792 - output_seq_layer_loss: 0.2605 - output_annoatations_layer_loss: 1.8709e-04\n",
      "Epoch 13/100000\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.2867 - output_seq_layer_loss: 0.2674 - output_annoatations_layer_loss: 1.9268e-04\n",
      "Epoch 14/100000\n",
      "100/100 [==============================] - 42s 425ms/step - loss: 0.2854 - output_seq_layer_loss: 0.2632 - output_annoatations_layer_loss: 2.2241e-04\n",
      "Epoch 15/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2794 - output_seq_layer_loss: 0.2602 - output_annoatations_layer_loss: 1.9207e-04\n",
      "Epoch 16/100000\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.2848 - output_seq_layer_loss: 0.2634 - output_annoatations_layer_loss: 2.1361e-04\n",
      "Epoch 17/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2767 - output_seq_layer_loss: 0.2567 - output_annoatations_layer_loss: 1.9963e-04\n",
      "Epoch 18/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2854 - output_seq_layer_loss: 0.2647 - output_annoatations_layer_loss: 2.0657e-04\n",
      "Epoch 19/100000\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.2816 - output_seq_layer_loss: 0.2621 - output_annoatations_layer_loss: 1.9518e-04\n",
      "Epoch 20/100000\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 0.2782 - output_seq_layer_loss: 0.2583 - output_annoatations_layer_loss: 1.9923e-04\n",
      "Epoch 21/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2815 - output_seq_layer_loss: 0.2600 - output_annoatations_layer_loss: 2.1515e-04\n",
      "Epoch 22/100000\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.2824 - output_seq_layer_loss: 0.2608 - output_annoatations_layer_loss: 2.1638e-04\n",
      "Epoch 23/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2874 - output_seq_layer_loss: 0.2673 - output_annoatations_layer_loss: 2.0162e-04\n",
      "Epoch 24/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2853 - output_seq_layer_loss: 0.2642 - output_annoatations_layer_loss: 2.1046e-04\n",
      "Epoch 25/100000\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.2811 - output_seq_layer_loss: 0.2612 - output_annoatations_layer_loss: 1.9916e-04\n",
      "Epoch 26/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2861 - output_seq_layer_loss: 0.2652 - output_annoatations_layer_loss: 2.0887e-04\n",
      "Epoch 27/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2835 - output_seq_layer_loss: 0.2626 - output_annoatations_layer_loss: 2.0872e-04\n",
      "Epoch 28/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2819 - output_seq_layer_loss: 0.2622 - output_annoatations_layer_loss: 1.9653e-04\n",
      "Epoch 29/100000\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.2821 - output_seq_layer_loss: 0.2612 - output_annoatations_layer_loss: 2.0979e-04\n",
      "Epoch 30/100000\n",
      "100/100 [==============================] - 53s 532ms/step - loss: 0.2819 - output_seq_layer_loss: 0.2625 - output_annoatations_layer_loss: 1.9318e-04\n",
      "Epoch 31/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2824 - output_seq_layer_loss: 0.2638 - output_annoatations_layer_loss: 1.8623e-04\n",
      "Epoch 32/100000\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.2789 - output_seq_layer_loss: 0.2588 - output_annoatations_layer_loss: 2.0114e-04\n",
      "Epoch 33/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2841 - output_seq_layer_loss: 0.2627 - output_annoatations_layer_loss: 2.1463e-04\n",
      "Epoch 34/100000\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.2836 - output_seq_layer_loss: 0.2640 - output_annoatations_layer_loss: 1.9537e-04\n",
      "Epoch 35/100000\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.2809 - output_seq_layer_loss: 0.2612 - output_annoatations_layer_loss: 1.9619e-04\n",
      "Epoch 36/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2844 - output_seq_layer_loss: 0.2635 - output_annoatations_layer_loss: 2.0982e-04\n",
      "Epoch 37/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2822 - output_seq_layer_loss: 0.2619 - output_annoatations_layer_loss: 2.0337e-04\n",
      "Epoch 38/100000\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.2825 - output_seq_layer_loss: 0.2618 - output_annoatations_layer_loss: 2.0737e-04\n",
      "Epoch 39/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2804 - output_seq_layer_loss: 0.2606 - output_annoatations_layer_loss: 1.9823e-04\n",
      "Epoch 40/100000\n",
      "100/100 [==============================] - 54s 538ms/step - loss: 0.2868 - output_seq_layer_loss: 0.2651 - output_annoatations_layer_loss: 2.1705e-04\n",
      "Epoch 41/100000\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.2806 - output_seq_layer_loss: 0.2608 - output_annoatations_layer_loss: 1.9708e-04\n",
      "Epoch 42/100000\n",
      "100/100 [==============================] - 42s 425ms/step - loss: 0.2823 - output_seq_layer_loss: 0.2617 - output_annoatations_layer_loss: 2.0566e-04\n",
      "Epoch 43/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2778 - output_seq_layer_loss: 0.2582 - output_annoatations_layer_loss: 1.9547e-04\n",
      "Epoch 44/100000\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.2833 - output_seq_layer_loss: 0.2628 - output_annoatations_layer_loss: 2.0485e-04\n",
      "Epoch 45/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2775 - output_seq_layer_loss: 0.2565 - output_annoatations_layer_loss: 2.0937e-04\n",
      "Epoch 46/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2813 - output_seq_layer_loss: 0.2624 - output_annoatations_layer_loss: 1.8891e-04\n",
      "Epoch 47/100000\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.2825 - output_seq_layer_loss: 0.2628 - output_annoatations_layer_loss: 1.9792e-04\n",
      "Epoch 48/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2818 - output_seq_layer_loss: 0.2600 - output_annoatations_layer_loss: 2.1746e-04\n",
      "Epoch 49/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2822 - output_seq_layer_loss: 0.2623 - output_annoatations_layer_loss: 1.9865e-04\n",
      "Epoch 50/100000\n",
      "100/100 [==============================] - 58s 579ms/step - loss: 0.2842 - output_seq_layer_loss: 0.2641 - output_annoatations_layer_loss: 2.0155e-04\n",
      "Epoch 51/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2825 - output_seq_layer_loss: 0.2619 - output_annoatations_layer_loss: 2.0625e-04\n",
      "Epoch 52/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2860 - output_seq_layer_loss: 0.2658 - output_annoatations_layer_loss: 2.0141e-04\n",
      "Epoch 53/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2870 - output_seq_layer_loss: 0.2662 - output_annoatations_layer_loss: 2.0805e-04\n",
      "Epoch 54/100000\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.2806 - output_seq_layer_loss: 0.2595 - output_annoatations_layer_loss: 2.1051e-04\n",
      "Epoch 55/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2806 - output_seq_layer_loss: 0.2593 - output_annoatations_layer_loss: 2.1372e-04\n",
      "Epoch 56/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2800 - output_seq_layer_loss: 0.2599 - output_annoatations_layer_loss: 2.0109e-04\n",
      "Epoch 57/100000\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.2836 - output_seq_layer_loss: 0.2629 - output_annoatations_layer_loss: 2.0653e-04\n",
      "Epoch 58/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2801 - output_seq_layer_loss: 0.2609 - output_annoatations_layer_loss: 1.9199e-04\n",
      "Epoch 59/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2872 - output_seq_layer_loss: 0.2674 - output_annoatations_layer_loss: 1.9760e-04\n",
      "Epoch 60/100000\n",
      "100/100 [==============================] - 57s 571ms/step - loss: 0.2804 - output_seq_layer_loss: 0.2597 - output_annoatations_layer_loss: 2.0665e-04\n",
      "Epoch 61/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2810 - output_seq_layer_loss: 0.2610 - output_annoatations_layer_loss: 2.0004e-04\n",
      "Epoch 62/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2803 - output_seq_layer_loss: 0.2607 - output_annoatations_layer_loss: 1.9530e-04\n",
      "Epoch 63/100000\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.2807 - output_seq_layer_loss: 0.2608 - output_annoatations_layer_loss: 1.9960e-04\n",
      "Epoch 64/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2846 - output_seq_layer_loss: 0.2643 - output_annoatations_layer_loss: 2.0298e-04\n",
      "Epoch 65/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2846 - output_seq_layer_loss: 0.2639 - output_annoatations_layer_loss: 2.0765e-04\n",
      "Epoch 66/100000\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.2827 - output_seq_layer_loss: 0.2619 - output_annoatations_layer_loss: 2.0834e-04\n",
      "Epoch 67/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2773 - output_seq_layer_loss: 0.2564 - output_annoatations_layer_loss: 2.0834e-04\n",
      "Epoch 68/100000\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.2857 - output_seq_layer_loss: 0.2648 - output_annoatations_layer_loss: 2.0912e-04\n",
      "Epoch 69/100000\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.2806 - output_seq_layer_loss: 0.2605 - output_annoatations_layer_loss: 2.0080e-04\n",
      "Epoch 70/100000\n",
      "100/100 [==============================] - 53s 532ms/step - loss: 0.2816 - output_seq_layer_loss: 0.2607 - output_annoatations_layer_loss: 2.0881e-04\n",
      "Epoch 71/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2826 - output_seq_layer_loss: 0.2639 - output_annoatations_layer_loss: 1.8774e-04\n",
      "Epoch 72/100000\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.2859 - output_seq_layer_loss: 0.2656 - output_annoatations_layer_loss: 2.0366e-04\n",
      "Epoch 73/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2828 - output_seq_layer_loss: 0.2637 - output_annoatations_layer_loss: 1.9135e-04\n",
      "Epoch 74/100000\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.2884 - output_seq_layer_loss: 0.2668 - output_annoatations_layer_loss: 2.1553e-04\n",
      "Epoch 75/100000\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.2828 - output_seq_layer_loss: 0.2614 - output_annoatations_layer_loss: 2.1441e-04\n",
      "Epoch 76/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2814 - output_seq_layer_loss: 0.2630 - output_annoatations_layer_loss: 1.8403e-04\n",
      "Epoch 77/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2810 - output_seq_layer_loss: 0.2609 - output_annoatations_layer_loss: 2.0160e-04\n",
      "Epoch 78/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2784 - output_seq_layer_loss: 0.2579 - output_annoatations_layer_loss: 2.0536e-04\n",
      "Epoch 79/100000\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.2843 - output_seq_layer_loss: 0.2638 - output_annoatations_layer_loss: 2.0497e-04\n",
      "Epoch 80/100000\n",
      "100/100 [==============================] - 53s 529ms/step - loss: 0.2801 - output_seq_layer_loss: 0.2621 - output_annoatations_layer_loss: 1.8050e-04\n",
      "Epoch 81/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2791 - output_seq_layer_loss: 0.2593 - output_annoatations_layer_loss: 1.9804e-04\n",
      "Epoch 82/100000\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.2823 - output_seq_layer_loss: 0.2626 - output_annoatations_layer_loss: 1.9753e-04\n",
      "Epoch 83/100000\n",
      "100/100 [==============================] - 43s 425ms/step - loss: 0.2810 - output_seq_layer_loss: 0.2614 - output_annoatations_layer_loss: 1.9535e-04\n",
      "Epoch 84/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2834 - output_seq_layer_loss: 0.2639 - output_annoatations_layer_loss: 1.9430e-04\n",
      "Epoch 85/100000\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.2801 - output_seq_layer_loss: 0.2611 - output_annoatations_layer_loss: 1.8973e-04\n",
      "Epoch 86/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2778 - output_seq_layer_loss: 0.2576 - output_annoatations_layer_loss: 2.0243e-04\n",
      "Epoch 87/100000\n",
      "100/100 [==============================] - 42s 425ms/step - loss: 0.2811 - output_seq_layer_loss: 0.2626 - output_annoatations_layer_loss: 1.8503e-04\n",
      "Epoch 88/100000\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.2803 - output_seq_layer_loss: 0.2608 - output_annoatations_layer_loss: 1.9590e-04\n",
      "Epoch 89/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2860 - output_seq_layer_loss: 0.2653 - output_annoatations_layer_loss: 2.0686e-04\n",
      "Epoch 90/100000\n",
      "100/100 [==============================] - 53s 529ms/step - loss: 0.2806 - output_seq_layer_loss: 0.2600 - output_annoatations_layer_loss: 2.0646e-04\n",
      "Epoch 91/100000\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.2804 - output_seq_layer_loss: 0.2606 - output_annoatations_layer_loss: 1.9798e-04\n",
      "Epoch 92/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2832 - output_seq_layer_loss: 0.2641 - output_annoatations_layer_loss: 1.9135e-04\n",
      "Epoch 93/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2787 - output_seq_layer_loss: 0.2594 - output_annoatations_layer_loss: 1.9279e-04\n",
      "Epoch 94/100000\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.2839 - output_seq_layer_loss: 0.2624 - output_annoatations_layer_loss: 2.1444e-04\n",
      "Epoch 95/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2834 - output_seq_layer_loss: 0.2633 - output_annoatations_layer_loss: 2.0092e-04\n",
      "Epoch 96/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2796 - output_seq_layer_loss: 0.2606 - output_annoatations_layer_loss: 1.9012e-04\n",
      "Epoch 97/100000\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.2840 - output_seq_layer_loss: 0.2634 - output_annoatations_layer_loss: 2.0648e-04\n",
      "Epoch 98/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2849 - output_seq_layer_loss: 0.2631 - output_annoatations_layer_loss: 2.1796e-04\n",
      "Epoch 99/100000\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.2793 - output_seq_layer_loss: 0.2592 - output_annoatations_layer_loss: 2.0073e-04\n",
      "Epoch 100/100000\n",
      "100/100 [==============================] - 57s 568ms/step - loss: 0.2777 - output_seq_layer_loss: 0.2597 - output_annoatations_layer_loss: 1.7939e-04\n",
      "Epoch 101/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2833 - output_seq_layer_loss: 0.2625 - output_annoatations_layer_loss: 2.0824e-04\n",
      "Epoch 102/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2838 - output_seq_layer_loss: 0.2623 - output_annoatations_layer_loss: 2.1488e-04\n",
      "Epoch 103/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2844 - output_seq_layer_loss: 0.2634 - output_annoatations_layer_loss: 2.0990e-04\n",
      "Epoch 104/100000\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.2893 - output_seq_layer_loss: 0.2680 - output_annoatations_layer_loss: 2.1274e-04\n",
      "Epoch 105/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2843 - output_seq_layer_loss: 0.2642 - output_annoatations_layer_loss: 2.0106e-04\n",
      "Epoch 106/100000\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.2857 - output_seq_layer_loss: 0.2638 - output_annoatations_layer_loss: 2.1841e-04\n",
      "Epoch 107/100000\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.2757 - output_seq_layer_loss: 0.2571 - output_annoatations_layer_loss: 1.8682e-04\n",
      "Epoch 108/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2825 - output_seq_layer_loss: 0.2633 - output_annoatations_layer_loss: 1.9240e-04\n",
      "Epoch 109/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2794 - output_seq_layer_loss: 0.2613 - output_annoatations_layer_loss: 1.8157e-04\n",
      "Epoch 110/100000\n",
      "100/100 [==============================] - 57s 570ms/step - loss: 0.2806 - output_seq_layer_loss: 0.2608 - output_annoatations_layer_loss: 1.9799e-04\n",
      "Epoch 111/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2828 - output_seq_layer_loss: 0.2621 - output_annoatations_layer_loss: 2.0688e-04\n",
      "Epoch 112/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2846 - output_seq_layer_loss: 0.2647 - output_annoatations_layer_loss: 1.9860e-04\n",
      "Epoch 113/100000\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.2843 - output_seq_layer_loss: 0.2640 - output_annoatations_layer_loss: 2.0239e-04\n",
      "Epoch 114/100000\n",
      "100/100 [==============================] - 42s 425ms/step - loss: 0.2844 - output_seq_layer_loss: 0.2639 - output_annoatations_layer_loss: 2.0503e-04\n",
      "Epoch 115/100000\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.2867 - output_seq_layer_loss: 0.2673 - output_annoatations_layer_loss: 1.9445e-04\n",
      "Epoch 116/100000\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.2836 - output_seq_layer_loss: 0.2637 - output_annoatations_layer_loss: 1.9931e-04\n",
      "Epoch 117/100000\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.2816 - output_seq_layer_loss: 0.2618 - output_annoatations_layer_loss: 1.9827e-04\n",
      "Epoch 118/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2814 - output_seq_layer_loss: 0.2619 - output_annoatations_layer_loss: 1.9520e-04\n",
      "Epoch 119/100000\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.2811 - output_seq_layer_loss: 0.2616 - output_annoatations_layer_loss: 1.9504e-04\n",
      "Epoch 120/100000\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 0.2851 - output_seq_layer_loss: 0.2648 - output_annoatations_layer_loss: 2.0205e-04\n",
      "Epoch 121/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2832 - output_seq_layer_loss: 0.2634 - output_annoatations_layer_loss: 1.9774e-04\n",
      "Epoch 122/100000\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.2812 - output_seq_layer_loss: 0.2618 - output_annoatations_layer_loss: 1.9378e-04\n",
      "Epoch 123/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2845 - output_seq_layer_loss: 0.2637 - output_annoatations_layer_loss: 2.0755e-04\n",
      "Epoch 124/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2857 - output_seq_layer_loss: 0.2657 - output_annoatations_layer_loss: 2.0060e-04\n",
      "Epoch 125/100000\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.2911 - output_seq_layer_loss: 0.2706 - output_annoatations_layer_loss: 2.0527e-04\n",
      "Epoch 126/100000\n",
      "100/100 [==============================] - 42s 425ms/step - loss: 0.2820 - output_seq_layer_loss: 0.2616 - output_annoatations_layer_loss: 2.0386e-04\n",
      "Epoch 127/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2787 - output_seq_layer_loss: 0.2575 - output_annoatations_layer_loss: 2.1245e-04\n",
      "Epoch 128/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2810 - output_seq_layer_loss: 0.2624 - output_annoatations_layer_loss: 1.8538e-04\n",
      "Epoch 129/100000\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.2794 - output_seq_layer_loss: 0.2596 - output_annoatations_layer_loss: 1.9825e-04\n",
      "Epoch 130/100000\n",
      "100/100 [==============================] - 53s 535ms/step - loss: 0.2811 - output_seq_layer_loss: 0.2606 - output_annoatations_layer_loss: 2.0409e-04\n",
      "Epoch 131/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2855 - output_seq_layer_loss: 0.2650 - output_annoatations_layer_loss: 2.0525e-04\n",
      "Epoch 132/100000\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.2870 - output_seq_layer_loss: 0.2669 - output_annoatations_layer_loss: 2.0051e-04\n",
      "Epoch 133/100000\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.2817 - output_seq_layer_loss: 0.2610 - output_annoatations_layer_loss: 2.0708e-04\n",
      "Epoch 134/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2774 - output_seq_layer_loss: 0.2584 - output_annoatations_layer_loss: 1.8954e-04\n",
      "Epoch 135/100000\n",
      "100/100 [==============================] - 49s 493ms/step - loss: 0.2872 - output_seq_layer_loss: 0.2654 - output_annoatations_layer_loss: 2.1778e-04\n",
      "Epoch 136/100000\n",
      "100/100 [==============================] - 42s 425ms/step - loss: 0.2852 - output_seq_layer_loss: 0.2648 - output_annoatations_layer_loss: 2.0465e-04\n",
      "Epoch 137/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2820 - output_seq_layer_loss: 0.2624 - output_annoatations_layer_loss: 1.9516e-04\n",
      "Epoch 138/100000\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.2834 - output_seq_layer_loss: 0.2629 - output_annoatations_layer_loss: 2.0556e-04\n",
      "Epoch 139/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2819 - output_seq_layer_loss: 0.2631 - output_annoatations_layer_loss: 1.8779e-04\n",
      "Epoch 140/100000\n",
      "100/100 [==============================] - 53s 531ms/step - loss: 0.2855 - output_seq_layer_loss: 0.2659 - output_annoatations_layer_loss: 1.9613e-04\n",
      "Epoch 141/100000\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.2854 - output_seq_layer_loss: 0.2653 - output_annoatations_layer_loss: 2.0135e-04\n",
      "Epoch 142/100000\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.2777 - output_seq_layer_loss: 0.2596 - output_annoatations_layer_loss: 1.8096e-04\n",
      "Epoch 143/100000\n",
      "100/100 [==============================] - 42s 425ms/step - loss: 0.2823 - output_seq_layer_loss: 0.2620 - output_annoatations_layer_loss: 2.0299e-04\n",
      "Epoch 144/100000\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.2815 - output_seq_layer_loss: 0.2595 - output_annoatations_layer_loss: 2.1982e-04\n",
      "Epoch 145/100000\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.2847 - output_seq_layer_loss: 0.2635 - output_annoatations_layer_loss: 2.1225e-04\n",
      "Epoch 146/100000\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.2818 - output_seq_layer_loss: 0.2621 - output_annoatations_layer_loss: 1.9678e-04\n",
      "Epoch 147/100000\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.2825 - output_seq_layer_loss: 0.2626 - output_annoatations_layer_loss: 1.9856e-04\n",
      "Epoch 148/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2791 - output_seq_layer_loss: 0.2602 - output_annoatations_layer_loss: 1.8933e-04\n",
      "Epoch 149/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2835 - output_seq_layer_loss: 0.2632 - output_annoatations_layer_loss: 2.0296e-04\n",
      "Epoch 150/100000\n",
      "100/100 [==============================] - 56s 557ms/step - loss: 0.2818 - output_seq_layer_loss: 0.2619 - output_annoatations_layer_loss: 1.9925e-04\n",
      "Epoch 151/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2820 - output_seq_layer_loss: 0.2610 - output_annoatations_layer_loss: 2.1002e-04\n",
      "Epoch 152/100000\n",
      "100/100 [==============================] - 42s 425ms/step - loss: 0.2795 - output_seq_layer_loss: 0.2590 - output_annoatations_layer_loss: 2.0484e-04\n",
      "Epoch 153/100000\n",
      "100/100 [==============================] - 43s 431ms/step - loss: 0.2857 - output_seq_layer_loss: 0.2661 - output_annoatations_layer_loss: 1.9570e-04\n",
      "Epoch 154/100000\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.2810 - output_seq_layer_loss: 0.2620 - output_annoatations_layer_loss: 1.8951e-04\n",
      "Epoch 155/100000\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.2845 - output_seq_layer_loss: 0.2642 - output_annoatations_layer_loss: 2.0301e-04\n",
      "Epoch 156/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2846 - output_seq_layer_loss: 0.2635 - output_annoatations_layer_loss: 2.1039e-04\n",
      "Epoch 157/100000\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.2846 - output_seq_layer_loss: 0.2646 - output_annoatations_layer_loss: 1.9978e-04\n",
      "Epoch 158/100000\n",
      "100/100 [==============================] - 42s 425ms/step - loss: 0.2799 - output_seq_layer_loss: 0.2610 - output_annoatations_layer_loss: 1.8878e-04\n",
      "Epoch 159/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2816 - output_seq_layer_loss: 0.2615 - output_annoatations_layer_loss: 2.0152e-04\n",
      "Epoch 160/100000\n",
      "100/100 [==============================] - 60s 595ms/step - loss: 0.2770 - output_seq_layer_loss: 0.2562 - output_annoatations_layer_loss: 2.0814e-04\n",
      "Epoch 161/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2782 - output_seq_layer_loss: 0.2592 - output_annoatations_layer_loss: 1.9013e-04\n",
      "Epoch 162/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2850 - output_seq_layer_loss: 0.2642 - output_annoatations_layer_loss: 2.0786e-04\n",
      "Epoch 163/100000\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.2808 - output_seq_layer_loss: 0.2597 - output_annoatations_layer_loss: 2.1080e-04\n",
      "Epoch 164/100000\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.2824 - output_seq_layer_loss: 0.2623 - output_annoatations_layer_loss: 2.0146e-04\n",
      "Epoch 165/100000\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.2831 - output_seq_layer_loss: 0.2626 - output_annoatations_layer_loss: 2.0549e-04\n",
      "Epoch 166/100000\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.2827 - output_seq_layer_loss: 0.2619 - output_annoatations_layer_loss: 2.0795e-04\n",
      "Epoch 167/100000\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.2836 - output_seq_layer_loss: 0.2643 - output_annoatations_layer_loss: 1.9318e-04\n",
      "Epoch 168/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2830 - output_seq_layer_loss: 0.2634 - output_annoatations_layer_loss: 1.9664e-04\n",
      "Epoch 169/100000\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.2852 - output_seq_layer_loss: 0.2647 - output_annoatations_layer_loss: 2.0435e-04\n",
      "Epoch 170/100000\n",
      "100/100 [==============================] - 51s 506ms/step - loss: 0.2818 - output_seq_layer_loss: 0.2631 - output_annoatations_layer_loss: 1.8684e-04\n",
      "Epoch 171/100000\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.2818 - output_seq_layer_loss: 0.2617 - output_annoatations_layer_loss: 2.0068e-04\n",
      "Epoch 172/100000\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.2852 - output_seq_layer_loss: 0.2640 - output_annoatations_layer_loss: 2.1234e-04\n",
      "Epoch 173/100000\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.2809 - output_seq_layer_loss: 0.2620 - output_annoatations_layer_loss: 1.8976e-04\n",
      "Epoch 174/100000\n",
      " 34/100 [=========>....................] - ETA: 28s - loss: 0.2816 - output_seq_layer_loss: 0.2593 - output_annoatations_layer_loss: 2.2259e-04"
     ]
    }
   ],
   "source": [
    "# RESUME_FROM_EPOCH_SAVE = None\n",
    "RESUME_FROM_EPOCH_SAVE = 5240\n",
    "\n",
    "wandb.config.batch_size = 32\n",
    "wandb.config.steps_per_epoch = 100\n",
    "\n",
    "if RESUME_FROM_EPOCH_SAVE is None:\n",
    "    wandb.config.lr = 2e-04\n",
    "    wandb.config.annots_loss_weight = 1e02\n",
    "    print('Compiling model...')\n",
    "    model.compile(optimizer = keras.optimizers.Adam(lr = wandb.config.lr), loss = ['sparse_categorical_crossentropy', 'binary_crossentropy'], \\\n",
    "            loss_weights = [1, wandb.config.annots_loss_weight])\n",
    "    epochs_offset = 0\n",
    "else:\n",
    "    save_path = os.path.join(run_weights_dir, 'epoch-%d.pkl' % RESUME_FROM_EPOCH_SAVE)\n",
    "    print('Loading model %s...' % save_path)\n",
    "    load_model(model, save_path)\n",
    "    epochs_offset = RESUME_FROM_EPOCH_SAVE\n",
    "\n",
    "record_offset = epochs_offset * wandb.config.steps_per_epoch * wandb.config.batch_size\n",
    "np.random.seed(0)\n",
    "model.fit_generator(generate_batches(training_set_indices, batch_size = wandb.config.batch_size, record_offset = record_offset), \\\n",
    "        steps_per_epoch = wandb.config.steps_per_epoch, epochs = 100000, callbacks = [WandbCallback(), \\\n",
    "        AutoSaveCallback(auto_save_weights_dir, epochs_offset = epochs_offset, every_epochs_to_save = 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniProt ID: A0A365T8E7_9EURY (https://www.uniprot.org/uniprot/A0A365T8E7_9EURY)\n",
      "[2020_06_11-09:02:19] Iteration #1 over the datasdet...\n",
      "Sequence results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>3</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>22</th>\n",
       "      <th>51</th>\n",
       "      <th>67</th>\n",
       "      <th>69</th>\n",
       "      <th>72</th>\n",
       "      <th>77</th>\n",
       "      <th>82</th>\n",
       "      <th>84</th>\n",
       "      <th>88</th>\n",
       "      <th>90</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>101</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>127</th>\n",
       "      <th>129</th>\n",
       "      <th>134</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>140</th>\n",
       "      <th>142</th>\n",
       "      <th>153</th>\n",
       "      <th>168</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>174</th>\n",
       "      <th>176</th>\n",
       "      <th>180</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>190</th>\n",
       "      <th>193</th>\n",
       "      <th>195</th>\n",
       "      <th>197</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>210</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>216</th>\n",
       "      <th>218</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>233</th>\n",
       "      <th>237</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>257</th>\n",
       "      <th>259</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>272</th>\n",
       "      <th>275</th>\n",
       "      <th>279</th>\n",
       "      <th>281</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>296</th>\n",
       "      <th>299</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>311</th>\n",
       "      <th>321</th>\n",
       "      <th>324</th>\n",
       "      <th>344</th>\n",
       "      <th>347</th>\n",
       "      <th>349</th>\n",
       "      <th>351</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true</th>\n",
       "      <td>M</td>\n",
       "      <td>A</td>\n",
       "      <td>L</td>\n",
       "      <td>H</td>\n",
       "      <td>G</td>\n",
       "      <td>S</td>\n",
       "      <td>D</td>\n",
       "      <td>Q</td>\n",
       "      <td>M</td>\n",
       "      <td>S</td>\n",
       "      <td>W</td>\n",
       "      <td>S</td>\n",
       "      <td>I</td>\n",
       "      <td>F</td>\n",
       "      <td>G</td>\n",
       "      <td>L</td>\n",
       "      <td>A</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>Q</td>\n",
       "      <td>Q</td>\n",
       "      <td>Y</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>K</td>\n",
       "      <td>N</td>\n",
       "      <td>P</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>L</td>\n",
       "      <td>A</td>\n",
       "      <td>Q</td>\n",
       "      <td>Q</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>S</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>R</td>\n",
       "      <td>E</td>\n",
       "      <td>R</td>\n",
       "      <td>D</td>\n",
       "      <td>E</td>\n",
       "      <td>M</td>\n",
       "      <td>E</td>\n",
       "      <td>H</td>\n",
       "      <td>L</td>\n",
       "      <td>D</td>\n",
       "      <td>Y</td>\n",
       "      <td>G</td>\n",
       "      <td>N</td>\n",
       "      <td>G</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>P</td>\n",
       "      <td>S</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>D</td>\n",
       "      <td>Q</td>\n",
       "      <td>Y</td>\n",
       "      <td>C</td>\n",
       "      <td>H</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>V</td>\n",
       "      <td>M</td>\n",
       "      <td>Q</td>\n",
       "      <td>K</td>\n",
       "      <td>M</td>\n",
       "      <td>Q</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>P</td>\n",
       "      <td>V</td>\n",
       "      <td>Q</td>\n",
       "      <td>D</td>\n",
       "      <td>G</td>\n",
       "      <td>&lt;PAD&gt;</td>\n",
       "      <td>&lt;PAD&gt;</td>\n",
       "      <td>&lt;PAD&gt;</td>\n",
       "      <td>&lt;PAD&gt;</td>\n",
       "      <td>&lt;PAD&gt;</td>\n",
       "      <td>&lt;PAD&gt;</td>\n",
       "      <td>&lt;PAD&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>input</th>\n",
       "      <td>D</td>\n",
       "      <td>C</td>\n",
       "      <td>I</td>\n",
       "      <td>H</td>\n",
       "      <td>G</td>\n",
       "      <td>V</td>\n",
       "      <td>D</td>\n",
       "      <td>Q</td>\n",
       "      <td>M</td>\n",
       "      <td>S</td>\n",
       "      <td>W</td>\n",
       "      <td>I</td>\n",
       "      <td>U</td>\n",
       "      <td>F</td>\n",
       "      <td>V</td>\n",
       "      <td>G</td>\n",
       "      <td>&lt;OTHER&gt;</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>Q</td>\n",
       "      <td>H</td>\n",
       "      <td>X</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>K</td>\n",
       "      <td>N</td>\n",
       "      <td>P</td>\n",
       "      <td>G</td>\n",
       "      <td>N</td>\n",
       "      <td>L</td>\n",
       "      <td>M</td>\n",
       "      <td>Q</td>\n",
       "      <td>Q</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>M</td>\n",
       "      <td>W</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>S</td>\n",
       "      <td>W</td>\n",
       "      <td>G</td>\n",
       "      <td>I</td>\n",
       "      <td>E</td>\n",
       "      <td>R</td>\n",
       "      <td>W</td>\n",
       "      <td>E</td>\n",
       "      <td>M</td>\n",
       "      <td>E</td>\n",
       "      <td>H</td>\n",
       "      <td>X</td>\n",
       "      <td>H</td>\n",
       "      <td>Y</td>\n",
       "      <td>G</td>\n",
       "      <td>N</td>\n",
       "      <td>G</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "      <td>X</td>\n",
       "      <td>N</td>\n",
       "      <td>D</td>\n",
       "      <td>Q</td>\n",
       "      <td>Y</td>\n",
       "      <td>P</td>\n",
       "      <td>H</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>P</td>\n",
       "      <td>D</td>\n",
       "      <td>M</td>\n",
       "      <td>Q</td>\n",
       "      <td>K</td>\n",
       "      <td>M</td>\n",
       "      <td>Q</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>E</td>\n",
       "      <td>K</td>\n",
       "      <td>Q</td>\n",
       "      <td>D</td>\n",
       "      <td>G</td>\n",
       "      <td>Y</td>\n",
       "      <td>E</td>\n",
       "      <td>I</td>\n",
       "      <td>L</td>\n",
       "      <td>N</td>\n",
       "      <td>L</td>\n",
       "      <td>K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>M</td>\n",
       "      <td>C</td>\n",
       "      <td>I</td>\n",
       "      <td>H</td>\n",
       "      <td>G</td>\n",
       "      <td>V</td>\n",
       "      <td>D</td>\n",
       "      <td>Q</td>\n",
       "      <td>M</td>\n",
       "      <td>S</td>\n",
       "      <td>W</td>\n",
       "      <td>I</td>\n",
       "      <td>L</td>\n",
       "      <td>F</td>\n",
       "      <td>V</td>\n",
       "      <td>G</td>\n",
       "      <td>L</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>Q</td>\n",
       "      <td>H</td>\n",
       "      <td>L</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>K</td>\n",
       "      <td>N</td>\n",
       "      <td>P</td>\n",
       "      <td>G</td>\n",
       "      <td>N</td>\n",
       "      <td>L</td>\n",
       "      <td>D</td>\n",
       "      <td>Q</td>\n",
       "      <td>Q</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>M</td>\n",
       "      <td>W</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>S</td>\n",
       "      <td>W</td>\n",
       "      <td>G</td>\n",
       "      <td>I</td>\n",
       "      <td>E</td>\n",
       "      <td>R</td>\n",
       "      <td>W</td>\n",
       "      <td>E</td>\n",
       "      <td>M</td>\n",
       "      <td>E</td>\n",
       "      <td>H</td>\n",
       "      <td>L</td>\n",
       "      <td>H</td>\n",
       "      <td>Y</td>\n",
       "      <td>G</td>\n",
       "      <td>N</td>\n",
       "      <td>G</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "      <td>E</td>\n",
       "      <td>N</td>\n",
       "      <td>D</td>\n",
       "      <td>Q</td>\n",
       "      <td>Y</td>\n",
       "      <td>P</td>\n",
       "      <td>H</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>P</td>\n",
       "      <td>D</td>\n",
       "      <td>M</td>\n",
       "      <td>Q</td>\n",
       "      <td>K</td>\n",
       "      <td>M</td>\n",
       "      <td>Q</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>E</td>\n",
       "      <td>K</td>\n",
       "      <td>Q</td>\n",
       "      <td>D</td>\n",
       "      <td>G</td>\n",
       "      <td>&lt;PAD&gt;</td>\n",
       "      <td>&lt;PAD&gt;</td>\n",
       "      <td>&lt;PAD&gt;</td>\n",
       "      <td>&lt;PAD&gt;</td>\n",
       "      <td>&lt;PAD&gt;</td>\n",
       "      <td>&lt;PAD&gt;</td>\n",
       "      <td>&lt;PAD&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p_true</th>\n",
       "      <td>0.932644</td>\n",
       "      <td>0.0487611</td>\n",
       "      <td>0.00862729</td>\n",
       "      <td>0.873806</td>\n",
       "      <td>0.852475</td>\n",
       "      <td>0.0146901</td>\n",
       "      <td>0.897791</td>\n",
       "      <td>0.896382</td>\n",
       "      <td>0.857139</td>\n",
       "      <td>0.892149</td>\n",
       "      <td>0.486972</td>\n",
       "      <td>0.00332719</td>\n",
       "      <td>0.0691604</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.00358824</td>\n",
       "      <td>0.00281589</td>\n",
       "      <td>0.137417</td>\n",
       "      <td>0.7842</td>\n",
       "      <td>0.791875</td>\n",
       "      <td>0.885027</td>\n",
       "      <td>0.00622065</td>\n",
       "      <td>0.0390215</td>\n",
       "      <td>0.871884</td>\n",
       "      <td>0.869986</td>\n",
       "      <td>0.881525</td>\n",
       "      <td>0.801306</td>\n",
       "      <td>0.83184</td>\n",
       "      <td>0.00255193</td>\n",
       "      <td>0.824488</td>\n",
       "      <td>0.866605</td>\n",
       "      <td>0.00216684</td>\n",
       "      <td>0.583395</td>\n",
       "      <td>0.862437</td>\n",
       "      <td>0.898946</td>\n",
       "      <td>0.886613</td>\n",
       "      <td>0.0184474</td>\n",
       "      <td>0.723021</td>\n",
       "      <td>0.89957</td>\n",
       "      <td>0.896812</td>\n",
       "      <td>0.899173</td>\n",
       "      <td>0.015542</td>\n",
       "      <td>0.0122734</td>\n",
       "      <td>0.00190007</td>\n",
       "      <td>0.855374</td>\n",
       "      <td>0.883319</td>\n",
       "      <td>0.0125678</td>\n",
       "      <td>0.794823</td>\n",
       "      <td>0.819927</td>\n",
       "      <td>0.756204</td>\n",
       "      <td>0.87644</td>\n",
       "      <td>0.156918</td>\n",
       "      <td>0.00444508</td>\n",
       "      <td>0.839795</td>\n",
       "      <td>0.895073</td>\n",
       "      <td>0.871421</td>\n",
       "      <td>0.8905</td>\n",
       "      <td>0.76859</td>\n",
       "      <td>0.893645</td>\n",
       "      <td>0.88894</td>\n",
       "      <td>0.00935091</td>\n",
       "      <td>0.0541376</td>\n",
       "      <td>0.847755</td>\n",
       "      <td>0.886054</td>\n",
       "      <td>0.897201</td>\n",
       "      <td>0.882212</td>\n",
       "      <td>0.00442891</td>\n",
       "      <td>0.851009</td>\n",
       "      <td>0.816804</td>\n",
       "      <td>0.823067</td>\n",
       "      <td>0.00425702</td>\n",
       "      <td>0.00470254</td>\n",
       "      <td>0.843416</td>\n",
       "      <td>0.877819</td>\n",
       "      <td>0.887906</td>\n",
       "      <td>0.879245</td>\n",
       "      <td>0.860611</td>\n",
       "      <td>0.874766</td>\n",
       "      <td>0.0291987</td>\n",
       "      <td>0.00211714</td>\n",
       "      <td>0.0122999</td>\n",
       "      <td>0.832863</td>\n",
       "      <td>0.832968</td>\n",
       "      <td>0.898422</td>\n",
       "      <td>0.999744</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p_input</th>\n",
       "      <td>0.062901</td>\n",
       "      <td>0.249568</td>\n",
       "      <td>0.959875</td>\n",
       "      <td>0.873806</td>\n",
       "      <td>0.852475</td>\n",
       "      <td>0.871916</td>\n",
       "      <td>0.897791</td>\n",
       "      <td>0.896382</td>\n",
       "      <td>0.857139</td>\n",
       "      <td>0.892149</td>\n",
       "      <td>0.486972</td>\n",
       "      <td>0.95086</td>\n",
       "      <td>2.50571e-05</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.963632</td>\n",
       "      <td>0.977931</td>\n",
       "      <td>2.0718e-06</td>\n",
       "      <td>0.7842</td>\n",
       "      <td>0.791875</td>\n",
       "      <td>0.885027</td>\n",
       "      <td>0.831567</td>\n",
       "      <td>0.00720158</td>\n",
       "      <td>0.871884</td>\n",
       "      <td>0.869986</td>\n",
       "      <td>0.881525</td>\n",
       "      <td>0.801306</td>\n",
       "      <td>0.83184</td>\n",
       "      <td>0.975113</td>\n",
       "      <td>0.824488</td>\n",
       "      <td>0.866605</td>\n",
       "      <td>0.448441</td>\n",
       "      <td>0.583395</td>\n",
       "      <td>0.862437</td>\n",
       "      <td>0.898946</td>\n",
       "      <td>0.886613</td>\n",
       "      <td>0.819643</td>\n",
       "      <td>0.723021</td>\n",
       "      <td>0.89957</td>\n",
       "      <td>0.896812</td>\n",
       "      <td>0.899173</td>\n",
       "      <td>0.66657</td>\n",
       "      <td>0.868406</td>\n",
       "      <td>0.973051</td>\n",
       "      <td>0.855374</td>\n",
       "      <td>0.883319</td>\n",
       "      <td>0.393833</td>\n",
       "      <td>0.794823</td>\n",
       "      <td>0.819927</td>\n",
       "      <td>0.756204</td>\n",
       "      <td>0.87644</td>\n",
       "      <td>0.0124558</td>\n",
       "      <td>0.808249</td>\n",
       "      <td>0.839795</td>\n",
       "      <td>0.895073</td>\n",
       "      <td>0.871421</td>\n",
       "      <td>0.8905</td>\n",
       "      <td>0.76859</td>\n",
       "      <td>0.893645</td>\n",
       "      <td>0.88894</td>\n",
       "      <td>0.860916</td>\n",
       "      <td>0.0126288</td>\n",
       "      <td>0.847755</td>\n",
       "      <td>0.886054</td>\n",
       "      <td>0.897201</td>\n",
       "      <td>0.882212</td>\n",
       "      <td>0.72591</td>\n",
       "      <td>0.851009</td>\n",
       "      <td>0.816804</td>\n",
       "      <td>0.823067</td>\n",
       "      <td>0.854694</td>\n",
       "      <td>0.935329</td>\n",
       "      <td>0.843416</td>\n",
       "      <td>0.877819</td>\n",
       "      <td>0.887906</td>\n",
       "      <td>0.879245</td>\n",
       "      <td>0.860611</td>\n",
       "      <td>0.874766</td>\n",
       "      <td>0.330974</td>\n",
       "      <td>0.938256</td>\n",
       "      <td>0.865413</td>\n",
       "      <td>0.832863</td>\n",
       "      <td>0.832968</td>\n",
       "      <td>0.898422</td>\n",
       "      <td>3.53456e-05</td>\n",
       "      <td>9.68169e-13</td>\n",
       "      <td>1.68533e-14</td>\n",
       "      <td>1.04748e-14</td>\n",
       "      <td>2.08496e-13</td>\n",
       "      <td>9.02285e-15</td>\n",
       "      <td>5.2488e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p_max</th>\n",
       "      <td>0.932644</td>\n",
       "      <td>0.249568</td>\n",
       "      <td>0.959875</td>\n",
       "      <td>0.873806</td>\n",
       "      <td>0.852475</td>\n",
       "      <td>0.871916</td>\n",
       "      <td>0.897791</td>\n",
       "      <td>0.896382</td>\n",
       "      <td>0.857139</td>\n",
       "      <td>0.892149</td>\n",
       "      <td>0.486972</td>\n",
       "      <td>0.95086</td>\n",
       "      <td>0.12316</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.963632</td>\n",
       "      <td>0.977931</td>\n",
       "      <td>0.160395</td>\n",
       "      <td>0.7842</td>\n",
       "      <td>0.791875</td>\n",
       "      <td>0.885027</td>\n",
       "      <td>0.831567</td>\n",
       "      <td>0.213972</td>\n",
       "      <td>0.871884</td>\n",
       "      <td>0.869986</td>\n",
       "      <td>0.881525</td>\n",
       "      <td>0.801306</td>\n",
       "      <td>0.83184</td>\n",
       "      <td>0.975113</td>\n",
       "      <td>0.824488</td>\n",
       "      <td>0.866605</td>\n",
       "      <td>0.51716</td>\n",
       "      <td>0.583395</td>\n",
       "      <td>0.862437</td>\n",
       "      <td>0.898946</td>\n",
       "      <td>0.886613</td>\n",
       "      <td>0.819643</td>\n",
       "      <td>0.723021</td>\n",
       "      <td>0.89957</td>\n",
       "      <td>0.896812</td>\n",
       "      <td>0.899173</td>\n",
       "      <td>0.66657</td>\n",
       "      <td>0.868406</td>\n",
       "      <td>0.973051</td>\n",
       "      <td>0.855374</td>\n",
       "      <td>0.883319</td>\n",
       "      <td>0.393833</td>\n",
       "      <td>0.794823</td>\n",
       "      <td>0.819927</td>\n",
       "      <td>0.756204</td>\n",
       "      <td>0.87644</td>\n",
       "      <td>0.156918</td>\n",
       "      <td>0.808249</td>\n",
       "      <td>0.839795</td>\n",
       "      <td>0.895073</td>\n",
       "      <td>0.871421</td>\n",
       "      <td>0.8905</td>\n",
       "      <td>0.76859</td>\n",
       "      <td>0.893645</td>\n",
       "      <td>0.88894</td>\n",
       "      <td>0.860916</td>\n",
       "      <td>0.0982833</td>\n",
       "      <td>0.847755</td>\n",
       "      <td>0.886054</td>\n",
       "      <td>0.897201</td>\n",
       "      <td>0.882212</td>\n",
       "      <td>0.72591</td>\n",
       "      <td>0.851009</td>\n",
       "      <td>0.816804</td>\n",
       "      <td>0.823067</td>\n",
       "      <td>0.854694</td>\n",
       "      <td>0.935329</td>\n",
       "      <td>0.843416</td>\n",
       "      <td>0.877819</td>\n",
       "      <td>0.887906</td>\n",
       "      <td>0.879245</td>\n",
       "      <td>0.860611</td>\n",
       "      <td>0.874766</td>\n",
       "      <td>0.330974</td>\n",
       "      <td>0.938256</td>\n",
       "      <td>0.865413</td>\n",
       "      <td>0.832863</td>\n",
       "      <td>0.832968</td>\n",
       "      <td>0.898422</td>\n",
       "      <td>0.999744</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          3           10        11        22         51   \\\n",
       "true            M          A           L         H         G          S   \n",
       "input           D          C           I         H         G          V   \n",
       "max             M          C           I         H         G          V   \n",
       "p_true   0.932644  0.0487611  0.00862729  0.873806  0.852475  0.0146901   \n",
       "p_input  0.062901   0.249568    0.959875  0.873806  0.852475   0.871916   \n",
       "p_max    0.932644   0.249568    0.959875  0.873806  0.852475   0.871916   \n",
       "\n",
       "              67        69        72        77        82          84   \\\n",
       "true            D         Q         M         S         W           S   \n",
       "input           D         Q         M         S         W           I   \n",
       "max             D         Q         M         S         W           I   \n",
       "p_true   0.897791  0.896382  0.857139  0.892149  0.486972  0.00332719   \n",
       "p_input  0.897791  0.896382  0.857139  0.892149  0.486972     0.95086   \n",
       "p_max    0.897791  0.896382  0.857139  0.892149  0.486972     0.95086   \n",
       "\n",
       "                 88     90          96          97          101     106  \\\n",
       "true               I      F           G           L           A       M   \n",
       "input              U      F           V           G     <OTHER>       M   \n",
       "max                L      F           V           G           L       M   \n",
       "p_true     0.0691604  0.898  0.00358824  0.00281589    0.137417  0.7842   \n",
       "p_input  2.50571e-05  0.898    0.963632    0.977931  2.0718e-06  0.7842   \n",
       "p_max        0.12316  0.898    0.963632    0.977931    0.160395  0.7842   \n",
       "\n",
       "              107       127         129         134       136       137  \\\n",
       "true            M         Q           Q           Y         D         D   \n",
       "input           M         Q           H           X         D         D   \n",
       "max             M         Q           H           L         D         D   \n",
       "p_true   0.791875  0.885027  0.00622065   0.0390215  0.871884  0.869986   \n",
       "p_input  0.791875  0.885027    0.831567  0.00720158  0.871884  0.869986   \n",
       "p_max    0.791875  0.885027    0.831567    0.213972  0.871884  0.869986   \n",
       "\n",
       "              138       140      142         153       168       171  \\\n",
       "true            K         N        P           T         N         L   \n",
       "input           K         N        P           G         N         L   \n",
       "max             K         N        P           G         N         L   \n",
       "p_true   0.881525  0.801306  0.83184  0.00255193  0.824488  0.866605   \n",
       "p_input  0.881525  0.801306  0.83184    0.975113  0.824488  0.866605   \n",
       "p_max    0.881525  0.801306  0.83184    0.975113  0.824488  0.866605   \n",
       "\n",
       "                172       174       176       180       183        184  \\\n",
       "true              A         Q         Q         N         N          A   \n",
       "input             M         Q         Q         N         N          M   \n",
       "max               D         Q         Q         N         N          M   \n",
       "p_true   0.00216684  0.583395  0.862437  0.898946  0.886613  0.0184474   \n",
       "p_input    0.448441  0.583395  0.862437  0.898946  0.886613   0.819643   \n",
       "p_max       0.51716  0.583395  0.862437  0.898946  0.886613   0.819643   \n",
       "\n",
       "              185      186       190       193       195        197  \\\n",
       "true            W        F         T         S         E          E   \n",
       "input           W        F         T         S         W          G   \n",
       "max             W        F         T         S         W          G   \n",
       "p_true   0.723021  0.89957  0.896812  0.899173  0.015542  0.0122734   \n",
       "p_input  0.723021  0.89957  0.896812  0.899173   0.66657   0.868406   \n",
       "p_max    0.723021  0.89957  0.896812  0.899173   0.66657   0.868406   \n",
       "\n",
       "                199       200       201        203       204       205  \\\n",
       "true              R         E         R          D         E         M   \n",
       "input             I         E         R          W         E         M   \n",
       "max               I         E         R          W         E         M   \n",
       "p_true   0.00190007  0.855374  0.883319  0.0125678  0.794823  0.819927   \n",
       "p_input    0.973051  0.855374  0.883319   0.393833  0.794823  0.819927   \n",
       "p_max      0.973051  0.855374  0.883319   0.393833  0.794823  0.819927   \n",
       "\n",
       "              210      213        214         216       218       220  \\\n",
       "true            E        H          L           D         Y         G   \n",
       "input           E        H          X           H         Y         G   \n",
       "max             E        H          L           H         Y         G   \n",
       "p_true   0.756204  0.87644   0.156918  0.00444508  0.839795  0.895073   \n",
       "p_input  0.756204  0.87644  0.0124558    0.808249  0.839795  0.895073   \n",
       "p_max    0.756204  0.87644   0.156918    0.808249  0.839795  0.895073   \n",
       "\n",
       "              221     233      237       253      254         255        257  \\\n",
       "true            N       G        M         N        P           S          T   \n",
       "input           N       G        M         N        P           P          X   \n",
       "max             N       G        M         N        P           P          E   \n",
       "p_true   0.871421  0.8905  0.76859  0.893645  0.88894  0.00935091  0.0541376   \n",
       "p_input  0.871421  0.8905  0.76859  0.893645  0.88894    0.860916  0.0126288   \n",
       "p_max    0.871421  0.8905  0.76859  0.893645  0.88894    0.860916  0.0982833   \n",
       "\n",
       "              259       269       270       272         275       279  \\\n",
       "true            N         D         Q         Y           C         H   \n",
       "input           N         D         Q         Y           P         H   \n",
       "max             N         D         Q         Y           P         H   \n",
       "p_true   0.847755  0.886054  0.897201  0.882212  0.00442891  0.851009   \n",
       "p_input  0.847755  0.886054  0.897201  0.882212     0.72591  0.851009   \n",
       "p_max    0.847755  0.886054  0.897201  0.882212     0.72591  0.851009   \n",
       "\n",
       "              281       285         286         293       294       296  \\\n",
       "true            M         M           N           V         M         Q   \n",
       "input           M         M           P           D         M         Q   \n",
       "max             M         M           P           D         M         Q   \n",
       "p_true   0.816804  0.823067  0.00425702  0.00470254  0.843416  0.877819   \n",
       "p_input  0.816804  0.823067    0.854694    0.935329  0.843416  0.877819   \n",
       "p_max    0.816804  0.823067    0.854694    0.935329  0.843416  0.877819   \n",
       "\n",
       "              299       301       302       303        304         305  \\\n",
       "true            K         M         Q         G          T           P   \n",
       "input           K         M         Q         G          C           E   \n",
       "max             K         M         Q         G          C           E   \n",
       "p_true   0.887906  0.879245  0.860611  0.874766  0.0291987  0.00211714   \n",
       "p_input  0.887906  0.879245  0.860611  0.874766   0.330974    0.938256   \n",
       "p_max    0.887906  0.879245  0.860611  0.874766   0.330974    0.938256   \n",
       "\n",
       "               308       309       311       321          324          344  \\\n",
       "true             V         Q         D         G        <PAD>        <PAD>   \n",
       "input            K         Q         D         G            Y            E   \n",
       "max              K         Q         D         G        <PAD>        <PAD>   \n",
       "p_true   0.0122999  0.832863  0.832968  0.898422     0.999744            1   \n",
       "p_input   0.865413  0.832863  0.832968  0.898422  3.53456e-05  9.68169e-13   \n",
       "p_max     0.865413  0.832863  0.832968  0.898422     0.999744            1   \n",
       "\n",
       "                 347          349          351          393         394  \n",
       "true           <PAD>        <PAD>        <PAD>        <PAD>       <PAD>  \n",
       "input              I            L            N            L           K  \n",
       "max            <PAD>        <PAD>        <PAD>        <PAD>       <PAD>  \n",
       "p_true             1            1            1            1           1  \n",
       "p_input  1.68533e-14  1.04748e-14  2.08496e-13  9.02285e-15  5.2488e-13  \n",
       "p_max              1            1            1            1           1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation results:\n",
      "True annotations: [1427 3945]\n",
      "Input annotations: [1427]\n",
      "Predicted annotations: 1427 (0.99), 3945 (0.66)\n"
     ]
    }
   ],
   "source": [
    "record_index = test_set_indices[1]\n",
    "\n",
    "with h5py.File(H5_FILE_PATH, 'r') as h5f:\n",
    "    uniprot_id = h5f['uniprot_ids'][record_index]\n",
    "    \n",
    "print('UniProt ID: %s (https://www.uniprot.org/uniprot/%s)' % (uniprot_id, uniprot_id))\n",
    "\n",
    "(X_seqs, X_annots), (Y_true_seqs, Y_true_annots) = next(generate_batches([record_index], batch_size = 1, chunk_size = 1))\n",
    "Y_pred_seqs, Y_pred_annots = model.predict([X_seqs, X_annots])\n",
    "\n",
    "X_seqs = X_seqs.flatten()\n",
    "X_annots = X_annots.flatten()\n",
    "Y_true_seqs = Y_true_seqs.flatten()\n",
    "Y_true_annots = Y_true_annots.flatten()\n",
    "Y_pred_seqs = Y_pred_seqs[0, :, :]\n",
    "Y_pred_annots = Y_pred_annots.flatten()\n",
    "Y_pred_seqs_max = Y_pred_seqs.argmax(axis = -1)\n",
    "\n",
    "seq_result = pd.DataFrame()\n",
    "seq_result['true'] = list(map(token_to_str.get, Y_true_seqs))\n",
    "seq_result['input'] = list(map(token_to_str.get, X_seqs))\n",
    "seq_result['max'] = list(map(token_to_str.get, Y_pred_seqs_max))\n",
    "seq_result['p_true'] = Y_pred_seqs[np.arange(MAX_SEQ_LENGTH), Y_true_seqs]\n",
    "seq_result['p_input'] = Y_pred_seqs[np.arange(MAX_SEQ_LENGTH), X_seqs]\n",
    "seq_result['p_max'] = Y_pred_seqs[np.arange(MAX_SEQ_LENGTH), Y_pred_seqs_max]\n",
    "\n",
    "print('Sequence results:')\n",
    "\n",
    "with pd.option_context('display.max_columns', MAX_SEQ_LENGTH):\n",
    "    display(seq_result[(seq_result['true'] != seq_result['input']) | (seq_result['p_true'] < 0.9)].transpose())\n",
    "    \n",
    "true_annots = np.where(Y_true_annots)[0]\n",
    "input_annots = np.where(X_annots)[0]\n",
    "relevant_annots = sorted(set(true_annots) | set(input_annots) | set(np.where(Y_pred_annots >= 0.05)[0]))\n",
    "    \n",
    "print('Annotation results:')\n",
    "print('True annotations: %s' % true_annots)\n",
    "print('Input annotations: %s' % input_annots)\n",
    "print('Predicted annotations: %s' % ', '.join('%d (%.2g)' % (annot, Y_pred_annots[annot]) for annot in relevant_annots))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_python",
   "language": "python",
   "name": "my_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
