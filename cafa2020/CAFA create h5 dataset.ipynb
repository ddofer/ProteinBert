{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the .h5 dataset from .db annotations and .fasta sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating log file: cafa_create_h5_dataset__28708__2020_06_04-11:14:45.txt\n",
      "[2020_06_04-11:14:45] Will encode the 8943 most common annotations.\n",
      "[2020_06_04-11:14:45] Loading all GO annotations...\n",
      "[2020_06_04-11:20:21] Loaded 106555993 GO annotations (8 columns: index, tax_id, uniprot_name, go_annotations, flat_go_annotations, n_go_annotations, complete_go_annotation_indices, n_complete_go_annotations)\n",
      "[2020_06_04-11:20:21] Loading Faidx (/cs/phd/nadavb/cafa_project/data/uniref90.fasta)...\n",
      "[2020_06_04-11:24:47] Finished loading Faidx.\n",
      "[2020_06_04-17:22:44] Finished. 349008 of 106555993 sequences failed.\n",
      "[2020_06_04-17:40:16] Will create a dataset of 106206985 final sequences.\n",
      "[2020_06_04-17:40:18] Loading all GO annotations...\n",
      "[2020_06_04-17:46:15] Loaded 106555993 GO annotations (8 columns: index, tax_id, uniprot_name, go_annotations, flat_go_annotations, n_go_annotations, complete_go_annotation_indices, n_complete_go_annotations)\n",
      "[2020_06_04-17:48:16] Loading Faidx (/cs/phd/nadavb/cafa_project/data/uniref90.fasta)...\n",
      "[2020_06_04-17:52:38] Finished loading Faidx.\n",
      "[2020_06_04-17:59:30] 80440000/1065559933\r"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "import sqlite3\n",
    "from pyfaidx import Faidx\n",
    "\n",
    "from pwas.shared_utils.util import start_log, log\n",
    "\n",
    "ANNOTATION_COUNTS_CSV_FILE_PATH = '/cs/phd/nadavb/cafa_project/data/unique_annotations_counts.csv'\n",
    "ANNOTATIONS_SQLITE_FILE_PATH = '/cs/phd/nadavb/cafa_project/data/protein_annotations.db'\n",
    "FASTA_FILE_PATH = '/cs/phd/nadavb/cafa_project/data/uniref90.fasta'\n",
    "\n",
    "OUTPUT_H5_FILE_PATH = '/cs/phd/nadavb/cafa_project/data/dataset.h5'\n",
    "\n",
    "start_log('/cs/phd/nadavb/logs', 'cafa_create_h5_dataset')\n",
    "\n",
    "def format_quantity(quantity):\n",
    "    if quantity is None:\n",
    "        return 'all'\n",
    "    else:\n",
    "        return str(quantity)\n",
    "\n",
    "def load_seqs_and_annotations(annotations_limit = None, shuffle = False, print_progress_interval = 10000):\n",
    "\n",
    "    log('Loading %s GO annotations...' % format_quantity(annotations_limit))\n",
    "    cnx = sqlite3.connect(ANNOTATIONS_SQLITE_FILE_PATH)\n",
    "    raw_go_annotations = pd.read_sql_query('SELECT * FROM protein_annotations' + ('' if annotations_limit is None else \\\n",
    "            (' LIMIT %d' % annotations_limit)), cnx)\n",
    "    log('Loaded %d GO annotations (%d columns: %s)' % (raw_go_annotations.shape + (', '.join(raw_go_annotations.columns),)))\n",
    "    \n",
    "    if shuffle:\n",
    "        raw_go_annotations = raw_go_annotations.sample(frac = 1, random_state = 0)\n",
    "\n",
    "    log('Loading Faidx (%s)...' % FASTA_FILE_PATH)\n",
    "    seqs_faidx = Faidx(FASTA_FILE_PATH)\n",
    "    log('Finished loading Faidx.')\n",
    "    \n",
    "    n_failed = 0\n",
    "\n",
    "    for i, (uniprot_id, raw_annotations) in raw_go_annotations[['uniprot_name', 'complete_go_annotation_indices']].iterrows():\n",
    "        \n",
    "        if i % print_progress_interval == 0:\n",
    "            log('%d/%d' % (i, len(raw_go_annotations)), end = '\\r')\n",
    "        \n",
    "        seq_fasta_id = 'UniRef90_%s' % uniprot_id.split('_')[0]\n",
    "        \n",
    "        try:\n",
    "            seq = str(seqs_faidx.fetch(seq_fasta_id, 1, seqs_faidx.index[seq_fasta_id].rlen))\n",
    "            yield uniprot_id, seq, json.loads(raw_annotations)\n",
    "        except KeyError:\n",
    "            n_failed += 1\n",
    "            \n",
    "    log('Finished. %d of %d sequences failed.' % (n_failed, len(raw_go_annotations)))\n",
    "\n",
    "annotation_counts = pd.read_csv(ANNOTATION_COUNTS_CSV_FILE_PATH, index_col = 0, squeeze = True, names = ['count'])\n",
    "common_annotations = np.array(sorted(annotation_counts[annotation_counts >= 100].index))\n",
    "common_annotation_to_index = {annotation: i for i, annotation in enumerate(common_annotations)}\n",
    "n_common_annotations = len(common_annotations)\n",
    "log('Will encode the %d most common annotations.' % n_common_annotations)\n",
    "\n",
    "def encode_annotations(annotations):\n",
    "    \n",
    "    annotation_mask = np.zeros(n_common_annotations, dtype = bool)\n",
    "    \n",
    "    for annotation in annotations:\n",
    "        if annotation in common_annotation_to_index:\n",
    "            annotation_mask[common_annotation_to_index[annotation]] = True\n",
    "            \n",
    "    return annotation_mask\n",
    "\n",
    "n_seqs = sum(1 for _ in load_seqs_and_annotations())\n",
    "log('Will create a dataset of %d final sequences.' % n_seqs)\n",
    "\n",
    "with h5py.File(OUTPUT_H5_FILE_PATH, 'w') as h5f:\n",
    "    \n",
    "    h5f.create_dataset('included_annotation_indices', data = common_annotations, dtype = np.int32)\n",
    "    uniprot_ids = h5f.create_dataset('uniprot_ids', shape = (n_seqs,), dtype = h5py.string_dtype())\n",
    "    seqs = h5f.create_dataset('seqs', shape = (n_seqs,), dtype = h5py.string_dtype())\n",
    "    seq_lengths = h5f.create_dataset('seq_lengths', shape = (n_seqs,), dtype = np.int32)\n",
    "    annotation_masks = h5f.create_dataset('annotation_masks', shape = (n_seqs, n_common_annotations), dtype = bool)\n",
    "    \n",
    "    for i, (uniprot_id, seq, annotations) in enumerate(load_seqs_and_annotations(shuffle = True)):\n",
    "        uniprot_ids[i] = uniprot_id\n",
    "        seqs[i] = seq\n",
    "        seq_lengths[i] = len(seq)\n",
    "        annotation_masks[i, :] = encode_annotations(annotations)\n",
    "                \n",
    "log('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle the dataset\n",
    "\n",
    "Apparently it is not that useful (creating the dataset from scratch appears to be faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020_06_03-14:38:56] Setting 0..100000/106206985...\n",
      "[2020_06_03-15:32:49] Setting 100000..200000/106206985...\n",
      "[2020_06_03-16:52:42] Setting 200000..300000/106206985...\n",
      "[2020_06_03-18:10:26] Setting 300000..400000/106206985...\n",
      "[2020_06_03-19:28:26] Setting 400000..500000/106206985...\n",
      "[2020_06_03-20:44:16] Setting 500000..600000/106206985...\n",
      "[2020_06_03-21:59:43] Setting 600000..700000/106206985...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "from pwas.shared_utils.util import start_log, log, get_chunk_intervals\n",
    "\n",
    "INPUT_H5_FILE_PATH = '/cs/phd/nadavb/cafa_project/data/dataset.h5'\n",
    "OUTPUT_SHUFFLED_H5_FILE_PATH = '/cs/phd/nadavb/cafa_project/data/shuffled_dataset.h5'\n",
    "\n",
    "CHUNK_SIZE = 100000\n",
    "\n",
    "start_log('/cs/phd/nadavb/logs', 'cafa_create_h5_dataset')\n",
    "\n",
    "def slice_h5_data_in_arbitrary_order(data, indices):\n",
    "    '''\n",
    "    This is required because h5py forces you to slice in increasing order.\n",
    "    '''\n",
    "    sorted_indices = pd.Series(np.array(indices)).sort_values()\n",
    "    sliced_data_in_increasing_order = data[sorted_indices.values]\n",
    "    sliced_data_in_desired_order = np.empty(sliced_data_in_increasing_order.shape, dtype = data.dtype)\n",
    "    sliced_data_in_desired_order[sorted_indices.index] = sliced_data_in_increasing_order\n",
    "    return sliced_data_in_desired_order\n",
    "\n",
    "with h5py.File(INPUT_H5_FILE_PATH, 'r') as h5f_input, h5py.File(OUTPUT_SHUFFLED_H5_FILE_PATH, 'w') as h5f_output:\n",
    "    \n",
    "    included_annotation_indices = h5f_input['included_annotation_indices'][:]\n",
    "    n_seqs = len(h5f_input['seq_lengths'])\n",
    "    \n",
    "    h5f_output.create_dataset('included_annotation_indices', data = included_annotation_indices)\n",
    "    seq_lengths = h5f_output.create_dataset('seq_lengths', shape = (n_seqs,), dtype = np.int32)\n",
    "    seqs = h5f_output.create_dataset('seqs', shape = (n_seqs,), dtype = h5py.string_dtype())\n",
    "    annotation_masks = h5f_output.create_dataset('annotation_masks', shape = (n_seqs, len(included_annotation_indices)), dtype = bool)\n",
    "    \n",
    "    shuffled_indices = np.arange(n_seqs)\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    \n",
    "    for i1, i2 in get_chunk_intervals(n_seqs, CHUNK_SIZE):\n",
    "        \n",
    "        log('Setting %d..%d/%d...' % (i1, i2, n_seqs))\n",
    "        chunk_indices = shuffled_indices[i1:i2]\n",
    "        \n",
    "        seq_lengths[i1:i2] = slice_h5_data_in_arbitrary_order(h5f_input['seq_lengths'], chunk_indices)\n",
    "        seqs[i1:i2] = slice_h5_data_in_arbitrary_order(h5f_input['seqs'], chunk_indices)\n",
    "        annotation_masks[i1:i2] = slice_h5_data_in_arbitrary_order(h5f_input['annotation_masks'], chunk_indices)\n",
    "\n",
    "log('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_python",
   "language": "python",
   "name": "my_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
