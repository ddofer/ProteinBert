{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data\n",
    "* Download sample data from uniclust. Full model should use uniref (but same process). \n",
    "* sample for toy experiemnt. Run bert on that data (including setence piece). \n",
    "\n",
    "* Future: toy labelled data, check that data already downloaded.. \n",
    "\n",
    "* biopython is useful for processing fasta files \n",
    "\n",
    "\n",
    "* For final model :\n",
    "    * uniref 50 : ftp://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref50/uniref50.fasta.gz\n",
    "    * uniref 90 : ftp://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref90/uniref90.fasta.gz\n",
    "    * url = http://gwdu111.gwdg.de/~compbiol/uniclust/2018_08/uniclust30_2018_08.tar.gz   -  30% similarity from uniclust. \n",
    "\n",
    "\n",
    "\n",
    "* Exampe of building sentence piece : https://github.com/google/sentencepiece/blob/master/python/README.md\n",
    "\n",
    "* toy BERT (pytorch) :  https://github.com/YuvalPeleg/transformers-workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "# import keras\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
      "\u001b[K     |████████████████████████████████| 184kB 5.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/danofer/anaconda3/lib/python3.7/site-packages (from pytorch_transformers) (1.17.2)\n",
      "Requirement already satisfied: tqdm in /Users/danofer/anaconda3/lib/python3.7/site-packages (from pytorch_transformers) (4.38.0)\n",
      "Requirement already satisfied: requests in /Users/danofer/anaconda3/lib/python3.7/site-packages (from pytorch_transformers) (2.22.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/danofer/anaconda3/lib/python3.7/site-packages (from pytorch_transformers) (0.1.83)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
      "\u001b[K     |████████████████████████████████| 860kB 7.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /Users/danofer/anaconda3/lib/python3.7/site-packages (from pytorch_transformers) (1.1.0)\n",
      "Requirement already satisfied: regex in /Users/danofer/anaconda3/lib/python3.7/site-packages (from pytorch_transformers) (2018.8.29)\n",
      "Requirement already satisfied: boto3 in /Users/danofer/anaconda3/lib/python3.7/site-packages (from pytorch_transformers) (1.10.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/danofer/anaconda3/lib/python3.7/site-packages (from requests->pytorch_transformers) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/danofer/anaconda3/lib/python3.7/site-packages (from requests->pytorch_transformers) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/danofer/anaconda3/lib/python3.7/site-packages (from requests->pytorch_transformers) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/danofer/anaconda3/lib/python3.7/site-packages (from requests->pytorch_transformers) (3.0.4)\n",
      "Requirement already satisfied: six in /Users/danofer/anaconda3/lib/python3.7/site-packages (from sacremoses->pytorch_transformers) (1.13.0)\n",
      "Requirement already satisfied: click in /Users/danofer/anaconda3/lib/python3.7/site-packages (from sacremoses->pytorch_transformers) (7.0)\n",
      "Collecting joblib\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/42/155696f85f344c066e17af287359c9786b436b1bf86029bb3411283274f3/joblib-0.14.0-py2.py3-none-any.whl (294kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 14.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: botocore<1.14.0,>=1.13.19 in /Users/danofer/anaconda3/lib/python3.7/site-packages (from boto3->pytorch_transformers) (1.13.19)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/danofer/anaconda3/lib/python3.7/site-packages (from boto3->pytorch_transformers) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /Users/danofer/anaconda3/lib/python3.7/site-packages (from boto3->pytorch_transformers) (0.2.1)\n",
      "Requirement already satisfied: python-dateutil<2.8.1,>=2.1 in /Users/danofer/anaconda3/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.19->boto3->pytorch_transformers) (2.8.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /Users/danofer/anaconda3/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.19->boto3->pytorch_transformers) (0.15.2)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp37-none-any.whl size=883999 sha256=4c240979c598821a458282c2f4796f44fb526efca23b54081d9db0959267aa71\n",
      "  Stored in directory: /Users/danofer/Library/Caches/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: joblib, sacremoses, pytorch-transformers\n",
      "Successfully installed joblib-0.14.0 pytorch-transformers-1.2.0 sacremoses-0.0.35\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentencepiece\n",
    "# ! pip install pytorch_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "## https://github.com/YuvalPeleg/transformers-workshop/blob/master/MLM.ipynb\n",
    "import pytorch_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Code for downloading the raw data\n",
    "* About 4-6 GB zipped. \n",
    "* commented for now - as I manually extracted the files from the zip & directory strucutre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['wget', '-r', '-nc', '-P', './data/uniclust', 'http://wwwuser.gwdg.de/~compbiol/uniclust/2016_03/uniclust30_2016_03.tar.gz'], returncode=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# url = \"http://wwwuser.gwdg.de/~compbiol/uniclust/2016_03/uniclust30_2016_03.tar.gz\"  ## Older version of above, 30% smaller.\n",
    "\n",
    "# path = \"./data/uniclust\"\n",
    "# subprocess.run([\"wget\", \"-r\", \"-nc\", \"-P\", path, url])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I mv the downloaded file from it's subdir to a higher one. \n",
    "* I unzip it (from gz and tar). \n",
    "* Within it, I access one of the files from the cmd and sample from the command line - getting the first K rows and outputting only their sequences. \n",
    "* We could also remove/merge the rare letters down to 3-4  (We have 25 letters in theory, 20 common and 5 very rare ones). https://wiki.thegpm.org/wiki/Amino_acid_symbols \n",
    "\n",
    "\n",
    "* We may want to split too long sequences (either in half or overlapping +- multiple times) \n",
    "* Our sample sequences is : `uniclust30_2016_1M_sampledSeq.fasta` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! head -323456   ./data/uniclust/uniclust30_2016_03/uniclust30_2016_03_consensus.fasta | grep -v \">\" > uniclust30_2016_1M_sampledSeq.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We know run sentencePiece to construct  a vocab\n",
    "* output is viewable on the terminal\n",
    "* https://github.com/google/sentencepiece/blob/master/python/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spm.SentencePieceTrainer.Train('--input=uniclust30_2016_1M_sampledSeq.fasta --model_prefix=m --vocab_size=500 --input_sentence_size=22345')\n",
    "spm.SentencePieceTrainer.Train('--input=uniclust30_2016_1M_sampledSeq.fasta --model_prefix=m --vocab_size=1500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "</s>\n",
      "LA\n",
      "0\n",
      "0\n",
      "<unk> False\n",
      "<s> True\n",
      "</s> True\n"
     ]
    }
   ],
   "source": [
    "# returns vocab size\n",
    "print(sp.get_piece_size())\n",
    "\n",
    "# id <=> piece conversion\n",
    "print(sp.id_to_piece(2))\n",
    "print(sp.id_to_piece(29))\n",
    "print(sp.piece_to_id('KR'))\n",
    "\n",
    "# returns 0 for unknown tokens (we can change the id for UNK)\n",
    "print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))\n",
    "\n",
    "# <unk>, <s>, </s> are defined by default. Their ids are (0, 1, 2)\n",
    "# <s> and </s> are defined as 'control' symbol.\n",
    "for i in range(3):\n",
    "  print(sp.id_to_piece(i), sp.is_control(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁M', 'R', 'Y', 'T', 'VL', 'I', 'AL', 'Q', 'G', 'A', 'LL', 'LL', 'LL', 'I', 'D', 'D', 'G', 'Q', 'G', 'Q', 'S', 'P', 'Y', 'P', 'Y', 'P', 'G', 'M', 'P', 'C', 'N', 'S', 'S', 'R', 'Q', 'C', 'G', 'LG', 'T', 'C', 'V', 'H', 'S', 'R', 'C', 'A', 'H', 'C', 'S', 'S', 'D', 'G', 'T', 'L', 'C', 'S', 'P', 'E', 'D', 'P', 'T', 'M', 'V', 'W', 'P', 'C', 'C', 'P', 'E', 'S', 'S', 'C', 'Q', 'LV', 'V', 'GL', 'P', 'S', 'LV', 'N', 'H', 'Y', 'N', 'C', 'L', 'P', 'N', 'Q', 'C', 'T', 'D', 'S', 'S', 'Q', 'C', 'P', 'GG', 'F', 'G', 'C', 'M', 'T', 'R', 'R', 'S', 'K', 'C', 'EL', 'C', 'K', 'A', 'D', 'G', 'E', 'A', 'C', 'N', 'S', 'P', 'Y', 'L', 'D', 'W', 'R', 'K', 'D', 'K', 'E', 'C', 'C', 'S', 'G', 'Y', 'C', 'H', 'T', 'E', 'A', 'R', 'G', 'LE', 'G', 'V', 'C', 'I', 'D', 'P', 'K', 'K', 'I', 'F', 'C', 'T', 'P', 'K', 'N', 'P', 'W', 'Q', 'LA', 'P', 'Y', 'P', 'P', 'S', 'Y', 'H', 'Q', 'P', 'T', 'T', 'L', 'R', 'P', 'P', 'T', 'S', 'L', 'Y', 'D', 'S', 'W', 'L', 'M', 'S', 'G', 'F', 'LV', 'K', 'S', 'T', 'T', 'A', 'P', 'S', 'T', 'Q', 'E', 'EE', 'D', 'D', 'Y']\n",
      "[24, 4, 17, 8, 37, 3, 31, 18, 12, 6, 23, 23, 23, 3, 9, 9, 12, 18, 12, 18, 5, 10, 17, 10, 17, 10, 12, 20, 10, 21, 14, 5, 5, 4, 18, 21, 12, 36, 8, 21, 13, 19, 5, 4, 21, 6, 19, 21, 5, 5, 9, 12, 8, 11, 21, 5, 10, 15, 9, 10, 8, 20, 13, 22, 10, 21, 21, 10, 15, 5, 5, 21, 18, 34, 13, 44, 10, 5, 34, 14, 19, 17, 14, 21, 11, 10, 14, 18, 21, 8, 9, 5, 5, 18, 21, 10, 33, 16, 12, 21, 20, 8, 4, 4, 5, 7, 21, 32, 21, 7, 6, 9, 12, 15, 6, 21, 14, 5, 10, 17, 11, 9, 22, 4, 7, 9, 7, 15, 21, 21, 5, 12, 17, 21, 19, 8, 15, 6, 4, 12, 28, 12, 13, 21, 3, 9, 10, 7, 7, 3, 16, 21, 8, 10, 7, 14, 10, 22, 18, 29, 10, 17, 10, 10, 5, 17, 19, 18, 10, 8, 8, 11, 4, 10, 10, 8, 5, 11, 17, 9, 5, 22, 11, 20, 5, 12, 16, 34, 7, 5, 8, 8, 6, 10, 5, 8, 18, 15, 25, 9, 9, 17]\n"
     ]
    }
   ],
   "source": [
    "eg_seq = \"MRYTVLIALQGALLLLLLIDDGQGQSPYPYPGMPCNSSRQCGLGTCVHSRCAHCSSDGTLCSPEDPTMVWPCCPESSCQLVVGLPSLVNHYNCLPNQCTDSSQCPGGFGCMTRRSKCELCKADGEACNSPYLDWRKDKECCSGYCHTEARGLEGVCIDPKKIFCTPKNPWQLAPYPPSYHQPTTLRPPTSLYDSWLMSGFLVKSTTAPSTQEEEDDY\"\n",
    "print(sp.encode_as_pieces(eg_seq))\n",
    "print(sp.encode_as_ids(eg_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build language model\n",
    "* For proteins - we can legitimately look from both directions (no need to mask!) \n",
    "* Need to tokenize using sentence piece. \n",
    "\n",
    "* Evaluate model.. once it's done, can try to do on more data or/and fine tuning on supervised task / classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
